# INTERSPEECH-2023-Papers

[![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)
![Version](https://img.shields.io/badge/version-v1.0.0-rc4)
![GitHub repo size](https://img.shields.io/github/repo-size/DmitryRyumin/INTERSPEECH-2023-Papers)
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](https://github.com/DmitryRyumin/INTERSPEECH-2023-Papers/blob/main/LICENSE)
[![Contributions welcome](https://img.shields.io/badge/contributions-welcome-brightgreen.svg?style=flat)](https://github.com/DmitryRyumin/INTERSPEECH-2023-Papers/blob/main/README.md)
![GitHub contributors](https://img.shields.io/github/contributors/dmitryryumin/INTERSPEECH-2023-Papers)
![GitHub commit activity (branch)](https://img.shields.io/github/commit-activity/t/dmitryryumin/INTERSPEECH-2023-Papers)
![GitHub closed issues](https://img.shields.io/github/issues-closed/DmitryRyumin/INTERSPEECH-2023-Papers)
![GitHub issues](https://img.shields.io/github/issues/DmitryRyumin/INTERSPEECH-2023-Papers)
![GitHub closed pull requests](https://img.shields.io/github/issues-pr-closed/DmitryRyumin/INTERSPEECH-2023-Papers)
![GitHub pull requests](https://img.shields.io/github/issues-pr/dmitryryumin/INTERSPEECH-2023-Papers)
![GitHub last commit](https://img.shields.io/github/last-commit/DmitryRyumin/INTERSPEECH-2023-Papers)
![GitHub watchers](https://img.shields.io/github/watchers/dmitryryumin/INTERSPEECH-2023-Papers)
![GitHub forks](https://img.shields.io/github/forks/dmitryryumin/INTERSPEECH-2023-Papers)
![GitHub Repo stars](https://img.shields.io/github/stars/dmitryryumin/INTERSPEECH-2023-Papers)
<!-- ![Papers Implemented](https://badgen.net/badge/Papers%20implemented/0) -->

---

INTERSPEECH 2023 Papers: A complete collection of influential and exciting research papers from the [*INTERSPEECH 2023*](https://interspeech2023.org/) conference. Explore the latest advances in speech and language processing. Code included. :star: the repository to support the advancement of speech technology!

<p align="center">
    <a href="https://interspeech2023.org/" target="_blank">
        <img width="600" src="https://cdn.jsdelivr.net/gh/DmitryRyumin/INTERSPEECH-2023-Papers@main/images/Interspeech2023-Stacked-Colour_v2.png" alt="Interspeech 2023">
    </a>
<p>

---

[*The PDF version of the INTERSPEECH 2023 Conference Programme*](https://drive.google.com/file/d/1xnYB2tQdhSNQwa3txhxFJ3OyUnLpuOCT/view), comprises a list of all accepted full papers, their presentation order, as well as the designated presentation times.

---

<a href="https://github.com/DmitryRyumin/NewEraAI-Papers" style="float:left;">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/arrow_click_cursor_pointer.png" width="25" />
  Other collections of the best AI conferences
</a>

<br />
<br />

> :exclamation: Conference table will be up to date all the time.

<table>
    <tr>
        <td><strong>Conference</strong></td>
        <td colspan="1" align="center"><strong>Year</strong></td>
    </tr>
    <tr>
      <td colspan="2" align="center"><i>Computer Vision (CV)</i></td>
    </tr>
    <tr>
        <td>CVPR</td>
        <td><a href="https://github.com/DmitryRyumin/CVPR-2023-Papers" target="_blank">2023</a></td>
    </tr>
    <tr>
      <td colspan="2" align="center"><i>Speech (SP)</i></td>
    </tr>
    <tr>
        <td>ICASSP</td>
        <td><a href="https://github.com/DmitryRyumin/ICASSP-2023-Papers" target="_blank">2023</a></td>
    </tr>
</table>

---

## Contributors

<a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-Papers/graphs/contributors">
  <img src="http://contributors.nn.ci/api?repo=DmitryRyumin/INTERSPEECH-2023-Papers" />
</a>

<br />
<br />

Contributions to improve the completeness of this list are greatly appreciated. If you come across any overlooked papers, please **feel free to [*create pull requests*](https://github.com/DmitryRyumin/INTERSPEECH-2023-Papers/pulls), [*open issues*](https://github.com/DmitryRyumin/INTERSPEECH-2023-Papers/issues) or contact me via [*email*](mailto:neweraairesearch@gmail.com)**. Your participation is crucial to making this repository even better.

---

## [Papers](https://www.isca-speech.org/archive/interspeech_2023/)

> :exclamation: Final paper links will be added post-conference.

<details open>
<summary>List of sections<a id="sections"></a></summary>

- [Resources for Spoken Language Processing](#resources-for-spoken-language-processing)
- [Speech Synthesis: Prosody and Emotion](#speech-synthesis-prosody-and-emotion)
- [Statistical Machine Translation](#statistical-machine-translation)
- [Self-Supervised Learning in ASR](#self-supervised-learning-in-asr)
- [Prosody](#prosody)
- [Speech Production](#speech-production)
- [Dysarthric Speech Assessment](#dysarthric-speech-assessment)
- [Speech Coding: Transmission](#speech-coding-transmission)
- [Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation](#speech-recognition-signal-processing-acoustic-modeling-robustness-adaptation)
- [Analysis of Speech and Audio Signals](#analysis-of-speech-and-audio-signals)
- [Speech Recognition: Architecture, Search, and Linguistic Components](#speech-recognition-architecture-search-and-linguistic-components)
- [Speech Recognition: Technologies and Systems for New Applications](#speech-recognition-technologies-and-systems-for-new-applications)
- [Lexical and Language Modeling for ASR](#lexical-and-language-modeling-for-asr)
- [Language Identification and Diarization](#language-identification-and-diarization)
- [Speech Quality Assessment](#speech-quality-assessment)
- [Feature Modeling for ASR](#feature-modeling-for-asr)
- [Interfacing Speech Technology and Phonetics](#interfacing-speech-technology-and-phonetics)
- [Speech Synthesis: Multilinguality](#speech-synthesis-multilinguality)
- [Speech Emotion Recognition](#speech-emotion-recognition)
- [Spoken Dialog Systems and Conversational Analysis](#spoken-dialog-systems-and-conversational-analysis)
- [Speech Coding and Enhancement](#speech-coding-and-enhancement)
- [Paralinguistics](#paralinguistics)
- [Speech Enhancement and Denoising](#speech-enhancement-and-denoising)
- [Speech Synthesis: Evaluation](#speech-synthesis-evaluation)
- [End-to-End Spoken Dialog Systems](#end-to-end-spoken-dialog-systems)
- [Biosignal-enabled Spoken Communication](#biosignal-enabled-spoken-communication)
- [Neural-based Speech and Acoustic Analysis](#neural-based-speech-and-acoustic-analysis)
- [DiGo - Dialog for Good: Speech and Language Technology for Social Good](#digo---dialog-for-good-speech-and-language-technology-for-social-good)
- [Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources, and Evaluation](#spoken-language-processing-translation-information-retrieval-summarization-resources-and-evaluation)
- [Speech, Voice, and Hearing Disorders](#speech-voice-and-hearing-disorders)
- [Spoken Term Detection and Voice Search](#spoken-term-detection-and-voice-search)
- [Models for Streaming ASR](#models-for-streaming-asr)
- [Source Separation](#source-separation)
- [Speech Perception](#speech-perception)
- [Phonetics and Phonology: Languages and Varieties](#phonetics-and-phonology-languages-and-varieties)
- [Speaker and Language Identification](#speaker-and-language-identification)
- [Speech Synthesis and Voice Conversion](#speech-synthesis-and-voice-conversion)
- [Speech and Language in Health: from Remote Monitoring to Medical Conversations](#speech-and-language-in-health-from-remote-monitoring-to-medical-conversations)
- [Novel Transformer Models for ASR](#novel-transformer-models-for-asr)
- [Speaker Recognition](#speaker-recognition)
- [Cross-lingual and Multilingual ASR](#cross-lingual-and-multilingual-asr)
- [Voice Conversion](#voice-conversion)
- [Pathological Speech Analysis](#pathological-speech-analysis)
- [Multimodal Speech Emotion Recognition](#multimodal-speech-emotion-recognition)
- [Phonetics, Phonology, and Prosody](#phonetics-phonology-and-prosody)
- [Speech Coding: Privacy](#speech-coding-privacy)
- [Analysis of Neural Speech Representations](#analysis-of-neural-speech-representations)
- [End-to-end ASR](#end-to-end-asr)
- [Spoken Language Understanding, Summarization, and Information Retrieval](#spoken-language-understanding-summarization-and-information-retrieval)
- [Invariant and Robust Pre-trained Acoustic Models](#invariant-and-robust-pre-trained-acoustic-models)
- [Speech Synthesis: Representation Learning](#speech-synthesis-representation-learning)
- [Speech Perception, Production, and Acquisition](#speech-perception-production-and-acquisition)
- [Acoustic Model Adaptation for ASR](#acoustic-model-adaptation-for-asr)
- [Speech Synthesis: Expressivity](#speech-synthesis-expressivity)
- [Multi-modal Systems](#multi-modal-systems)
- [Question Answering from Speech](#question-answering-from-speech)
- [Multi-talker Methods in Speech Processing](#multi-talker-methods-in-speech-processing)
- [Sociophonetics](#sociophonetics)
- [Speaker and Language Diarization](#speaker-and-language-diarization)
- [Anti-Spoofing for Speaker Verification](#anti-spoofing-for-speaker-verification)
- [Speech Coding: Intelligibility](#speech-coding-intelligibility)
- [New Computational Strategies for ASR Training and Inference](#new-computational-strategies-for-asr-training-and-inference)
- [MERLIon CCS Challenge: Multilingual Everyday Recordings - Language Identification On Code-Switched Child-Directed Speech](#merlion-ccs-challenge-multilingual-everyday-recordings---language-identification-on-code-switched-child-directed-speech)
- [Health-Related Speech Analysis](#health-related-speech-analysis)
- [Automatic Audio Classification and Audio Captioning](#automatic-audio-classification-and-audio-captioning)
- [Speech Synthesis](#speech-synthesis)
- [Speech Synthesis: Controllability and Adaptation](#speech-synthesis-controllability-and-adaptation)
- [Search Methods and Decoding Algorithms for ASR](#search-methods-and-decoding-algorithms-for-asr)
- [Speech Signal Analysis](#speech-signal-analysis)
- [Connecting Speech-science and Speech-technology for Children's Speech](#connecting-speech-science-and-speech-technology-for-childrens-speech)
- [Dialog Management](#dialog-management)
- [Speech Activity Detection and Modeling](#speech-activity-detection-and-modeling)
- [Multilingual Models for ASR](#multilingual-models-for-asr)
- [Speech Enhancement and Bandwidth Expansion](#speech-enhancement-and-bandwidth-expansion)
- [Articulation](#articulation)
- [Neural Processing of Speech and Language: Encoding and Decoding the Diverse Auditory Brain](#neural-processing-of-speech-and-language-encoding-and-decoding-the-diverse-auditory-brain)
- [Perception of Paralinguistics](#perception-of-paralinguistics)
- [Technologies for Child Speech Processing](#technologies-for-child-speech-processing)
- [Speech Synthesis: Multilinguality; Evaluation](#speech-synthesis-multilinguality-evaluation)
- [Show and Tell: Health Applications and Emotion Recognition](#show-and-tell-health-applications-and-emotion-recognition)
- [Show and Tell: Speech Tools, Speech Enhancement, Speech Synthesis](#show-and-tell-speech-tools-speech-enhancement-speech-synthesis)
- [Show and Tell: Language Learning and Educational Resources](#show-and-tell-language-learning-and-educational-resources)
- [Show and Tell: Media and Commercial Applications](#show-and-tell-media-and-commercial-applications)

</details>

### Resources for Spoken Language Processing

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

| :id: | **Title** | **Repo** | **Paper** |
|------|-----------|:--------:|:---------:|
| 1686 | Multimodal Personality Traits Assessment (MuPTA) Corpus: The Impact of Spontaneous and Read Speech | [![GitHub](https://img.shields.io/github/stars/DmitryRyumin/OCEANAI)](https://github.com/DmitryRyumin/OCEANAI) <br /> [![Documentation Status](https://readthedocs.org/projects/oceanai/badge/?version=latest)](https://oceanai.readthedocs.io/en/latest/?badge=latest) <br /> [![PyPI](https://img.shields.io/pypi/v/oceanai)](https://pypi.org/project/oceanai/) <br /> [![MuPTA](https://img.shields.io/badge/MuPTA-dataset-20BEFF.svg)](https://hci.nw.ru/en/pages/mupta-corpus) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/ryumina23_interspeech.pdf) |
| 1049 | MOCKS 1.0: Multilingual Open Custom Keyword Spotting Testset | [![Hugging Face](https://img.shields.io/badge/HuggingFace-MOCKS-yellow)](https://huggingface.co/datasets/voiceintelligenceresearch/MOCKS) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/pudo23_interspeech.pdf) |
| 2150 | MD3: The Multi-Dialect Dataset of Dialogues | [![Kaggle](https://img.shields.io/badge/kaggle-dataset-20BEFF.svg)](https://www.kaggle.com/datasets/jacobeis99/md3en) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/eisenstein23_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.11355-b31b1b.svg)](https://arxiv.org/abs/2305.11355) |
| 2279 | MuAViC: A Multilingual Audio-Visual Corpus for Robust Speech Recognition and Robust Speech-to-Text Translation | [![GitHub](https://img.shields.io/github/stars/facebookresearch/muavic)](https://github.com/facebookresearch/muavic) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/anwar23_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.00628-b31b1b.svg)](https://arxiv.org/abs/2303.00628) |
| 1828 | Thai Dialect Corpus and Transfer-based Curriculum Learning Investigation for Dialect Automatic Speech Recognition | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/suwanbandit23_interspeech.pdf) |
| 2351 | HK-LegiCoST: Leveraging Non-Verbatim Transcripts for Speech Translation | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/xiao23d_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.11252-b31b1b.svg)](https://arxiv.org/abs/2306.11252) |

### Speech Synthesis: Prosody and Emotion

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

| :id: | **Title** | **Repo** | **Paper** |
|------|-----------|:--------:|:---------:|
| 749 | Emotional Talking Head Generation based on Memory-Sharing and Attention-Augmented Networks | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/wang23m_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.03594-b31b1b.svg)](https://arxiv.org/abs/2306.03594) |
| 1292 | Speech Synthesis with Self-Supervisedly Learnt Prosodic Representations | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://ttsbylzc.github.io/ttsdemo202303/) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/liu23n_interspeech.pdf) |
| 1317 | EmoMix: Emotion Mixing via Diffusion Models for Emotional Speech Synthesis | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/tang23_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.00648-b31b1b.svg)](https://arxiv.org/abs/2306.00648) |
| 806 | Laughter Synthesis using Pseudo Phonetic Tokens with a Large-Scale In-the-Wild Laughter Corpus | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://aria-k-alethia.github.io/2023laughter-demo/) <br /> [![GitHub](https://img.shields.io/github/stars/Aria-K-Alethia/laughter-synthesis)](https://github.com/Aria-K-Alethia/laughter-synthesis) <br /> [![Laughterscape](https://img.shields.io/badge/Laughterscape-corpus-20BEFF.svg)](https://sites.google.com/site/shinnosuketakamichi/research-topics/laughter_corpus) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/xin23b_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.12442-b31b1b.svg)](https://arxiv.org/abs/2305.12442) |
| 2270 | Explicit Intensity Control for Accented Text-to-Speech | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://ttslr.github.io/Ai-TTS/) <br /> [![GitHub](https://img.shields.io/github/stars/ttslr/Ai-TTS)](https://github.com/ttslr/Ai-TTS) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/liu23u_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2210.15364-b31b1b.svg)](https://arxiv.org/abs/2210.15364) |
| 834 | Comparing Normalizing Flows and Diffusion Models for Prosody and Acoustic Modelling in Text-to-Speech | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/zhang23o_interspeech.pdf) |

### Statistical Machine Translation

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

| :id: | **Title** | **Repo** | **Paper** |
|------|-----------|:--------:|:---------:|
| 2484 | Modular Speech-to-Text Translation for Zero-Shot Cross-Modal Transfer | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/duquenne23_interspeech.pdf) |
| 1063 | Improving Isochronous Machine Translation with Target Factors and Auxiliary Counters | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/pal23_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.13204-b31b1b.svg)](https://arxiv.org/abs/2305.13204) |
| 648 | StyleS2ST: Zero-Shot Style Transfer for Direct Speech-to-Speech Translation | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://styles2st.github.io/StyleS2ST/) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/song23_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.17732-b31b1b.svg)](https://arxiv.org/abs/2305.17732) |
| 1767 | Joint Speech Translation and Named Entity Recognition | [![GitHub](https://img.shields.io/github/stars/hlt-mt/FBK-fairseq)](https://github.com/hlt-mt/FBK-fairseq/blob/master/fbk_works/JOINT_ST_NER2023.md) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/gaido23_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2210.11987-b31b1b.svg)](https://arxiv.org/abs/2210.11987) |
| 2050 | Analysis of Acoustic Information in End-to-End Spoken Language Translation | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/sant23_interspeech.pdf) |
| 2004 | LAMASSU: A Streaming Language-Agnostic Multilingual Speech Recognition and Translation Model using Neural Transducers | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/wang23oa_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.02809-b31b1b.svg)](https://arxiv.org/abs/2211.02809) |

### Self-Supervised Learning in ASR

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

| :id: | **Title** | **Repo** | **Paper** |
|------|-----------|:--------:|:---------:|
| 1213 | DPHuBERT: Joint Distillation and Pruning of Self-Supervised Speech Models | [![GitHub](https://img.shields.io/github/stars/pyf98/DPHuBERT)](https://github.com/pyf98/DPHuBERT) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/peng23c_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.17651-b31b1b.svg)](https://arxiv.org/abs/2305.17651) |
| 1040 | Automatic Data Augmentation for Domain Adapted Fine-Tuning of Self-Supervised Speech Representations | [![GitHub](https://img.shields.io/github/stars/salah-zaiem/augmentations_adaptation)](https://github.com/salah-zaiem/augmentations_adaptation) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/zaiem23_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.00481-b31b1b.svg)](https://arxiv.org/abs/2306.00481) |
| 387 | Dual Acoustic Linguistic Self-Supervised Representation Learning for Cross-Domain Speech Recognition | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/yang23e_interspeech.pdf) |
| 2166 | O-1: Self-Training with Oracle and 1-best Hypothesis | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/baskar23_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.07486-b31b1b.svg)](https://arxiv.org/abs/2308.07486) |
| 822 | MT4SSL: Boosting Self-Supervised Speech Representation Learning by Integrating Multiple Targets | [![GitHub](https://img.shields.io/github/stars/ddlBoJack/MT4SSL)](https://github.com/ddlBoJack/MT4SSL) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/ma23d_interspeech.pdf) <br />  [![arXiv](https://img.shields.io/badge/arXiv-2211.07321-b31b1b.svg)](https://arxiv.org/abs/2211.07321) |
| 1802 | Comparing Self-Supervised Pre-Training and Semi-Supervised Training for Speech Recognition in Languages with Weak Language Models | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/lamyeemui23_interspeech.pdf) |

### Prosody

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

| :id: | **Title** | **Repo** | **Paper** |
|------|-----------|:--------:|:---------:|
| 1781 | Chinese EFL Learners' Perception of English Prosodic Focus | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/zhang23da_interspeech.pdf) |
| 315 | Pitch Accent Variation and the Interpretation of Rising and Falling Intonation in American English | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/sostarics23_interspeech.pdf) |
| 1033 | Tonal Coarticulation as a Cue for Upcoming Prosodic Boundary | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/kuang23_interspeech.pdf) |
| 2116 | Alignment of Beat Gestures and Prosodic Prominence in German | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/repp23_interspeech.pdf) |
| 1454 | Creak Prevalence and Prosodic Context in Australian English | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/white23_interspeech.pdf) |
| 1651 | Speech Reduction: Position within French Prosodic Structure | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/bodur23_interspeech.pdf) |

### Speech Production

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

| :id: | **Title** | **Repo** | **Paper** |
|------|-----------|:--------:|:---------:|
| 637 | Transvelar Nasal Coupling Contributing to Speaker Characteristics in Non-nasal Vowels | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/zhu23d_interspeech.pdf) |
| 286 | Speech Synthesis from Articulatory Movements Recorded by Real-time MRI | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/otani23_interspeech.pdf) |
| 2283 |  The ART of Conversation: Measuring Phonetic Convergence and Deliberate Imitation in L2-Speech with a Siamese RNN | [![GitHub](https://img.shields.io/github/stars/byronthecoder/S-RNN-4-ART)](https://github.com/byronthecoder/S-RNN-4-ART) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/yuan23b_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.05088-b31b1b.svg)](https://arxiv.org/abs/2306.05088) |
| 1933 | Did You See that? Exploring the Role of Vision in the Development of Consonant Feature Contrasts in Children with Cochlear Implants | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/mahshie23_interspeech.pdf) |

### Dysarthric Speech Assessment

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

| :id: | **Title** | **Repo** | **Paper** |
|------|-----------|:--------:|:---------:|
| 2017 | Automatic Assessments of Dysarthric Speech: the Usability of Acoustic-Phonetic Features | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/vanbemmel23_interspeech.pdf) |
| 1455 | Classification of Multi-class Vowels and Fricatives from Patients Having Amyotrophic Lateral Sclerosis with Varied Levels of Dysarthria Severity | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/venkatathirumalakumar23_interspeech.pdf) |
| 1627 | Parameter-efficient Dysarthric Speech Recognition using Adapter Fusion and Householder Transformation | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/qi23b_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.07090-b31b1b.svg)](https://arxiv.org/abs/2306.07090) |
| 2481 | Few-Shot Dysarthric Speech Recognition with Text-to-Speech Data Augmentation | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/hermann23_interspeech.pdf) <br /> [![idiap](https://img.shields.io/badge/idiap.ch.5055-FF6A00.svg)](http://publications.idiap.ch/index.php/publications/show/5055) |
| 1921 | Latent Phrase Matching for Dysarthric Speech | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/yee23_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.05446-b31b1b.svg)](https://arxiv.org/abs/2306.05446) |
| 173 | Speech Intelligibility Assessment of Dysarthric Speech by using Goodness of Pronunciation with Uncertainty Quantification | [![GitHub](https://img.shields.io/github/stars/juice500ml/dysarthria-gop)](https://github.com/juice500ml/dysarthria-gop) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/yeo23_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.18392-b31b1b.svg)](https://arxiv.org/abs/2305.18392) |

### Speech Coding: Transmission

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

| :id: | **Title** | **Repo** | **Paper** |
|------|-----------|:--------:|:---------:|
| 1562 | CQNV: A Combination of Coarsely Quantized Bitstream and Neural Vocoder for Low Rate Speech Coding | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/zheng23c_interspeech.pdf) |
| 1234 | Target Speech Extraction with Conditional Diffusion Model | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/kamo23_interspeech.pdf) |
| 883 | Towards Fully Quantized Neural Networks For Speech Enhancement | [![GitHub](https://img.shields.io/github/stars/ssi-research/FQSE)](https://github.com/ssi-research/FQSE) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/cohen23_interspeech.pdf) |
| 980 | Complex Image Generation SwinTransformer Network for Audio Denoising | [![GitHub](https://img.shields.io/github/stars/YoushanZhang/CoxImgSwinTransformer)](https://github.com/YoushanZhang/CoxImgSwinTransformer) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/zhang23p_interspeech.pdf) |

### Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

| :id: | **Title** | **Repo** | **Paper** |
|------|-----------|:--------:|:---------:|
| 2118 | Using Text Injection to Improve Recognition of Personal Identifiers in Speech | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/blau23_interspeech.pdf) |
| 837 | Investigating Wav2Vec2 Context Representations and the Effects of Fine-Tuning, a Case-Study of a Finnish Model | [![GitHub](https://img.shields.io/github/stars/aalto-speech/Wav2vec2Interpretation)](https://github.com/aalto-speech/Wav2vec2Interpretation) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/grosz23_interspeech.pdf) |
| 872 | Transformer-based Speech Recognition Models for Oral History Archives in English, German, and Czech | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/lehecka23_interspeech.pdf) |
| 177 | Iteratively Improving Speech Recognition and Voice Conversion | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://demosamplesites.github.io/IterativeASR_VC/) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/singh23_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.15055-b31b1b.svg)](https://arxiv.org/abs/2305.15055) |
| 2001 | LABERT: A Combination of Local Aggregation and Self-Supervised Speech Representation Learning for Detecting Informative Hidden Units in Low-Resource ASR Systems | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/fatehi23_interspeech.pdf) <br /> [![nottingham-repo](https://img.shields.io/badge/nottingham-22183323-1A296B.svg)](https://nottingham-repository.worktribe.com/output/22183323) |
| 746 | TranUSR: Phoneme-to-Word Transcoder based Unified Speech Representation Learning for Cross-Lingual Speech Recognition | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/xue23_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.13629-b31b1b.svg)](https://arxiv.org/abs/2305.13629) |
| 1124 | Dual-Mode NAM: Effective Top-K Context Injection for End-to-End ASR | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/wu23e_interspeech.pdf) |
| 2417 | GhostRNN: Reducing State Redundancy in RNN with Cheap Operations | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/zhou23g_interspeech.pdf) |
| 1442 | Task-Agnostic Structured Pruning of Speech Representation Models | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/wang23da_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.01385-b31b1b.svg)](https://arxiv.org/abs/2306.01385) |
| 485 | Factual Consistency Oriented Speech Recognition | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/kanda23_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2302.12369-b31b1b.svg)](https://arxiv.org/abs/2302.12369) |
| 1036 | Multi-Head State Space Model for Speech Recognition | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/fathullah23_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.12498-b31b1b.svg)](https://arxiv.org/abs/2305.12498) |
| 341 | Cascaded Multi-task Adaptive Learning based on Neural Architecture Search | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/gao23_interspeech.pdf) |
| 2359 | Probing Self-Supervised Speech Models for Phonetic and Phonemic Information: A Case Study in Aspiration | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/martin23_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.06232-b31b1b.svg)](https://arxiv.org/abs/2306.06232) |
| 739 | Selective Biasing with Trie-based Contextual Adapters for Personalised Speech Recognition using Neural Transducers | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/harding23_interspeech.pdf) <br /> [![Amazon Science](https://img.shields.io/badge/amazon-science-FE9901.svg)](https://www.amazon.science/publications/selective-biasing-with-trie-based-contextual-adapters-for-personalised-speech-recognition-using-neural-transducers) |
| 213 | A More Accurate Internal Language Model Score Estimation for the Hybrid Autoregressive Transducer | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/lee23b_interspeech.pdf) |
| 106 | Attention Gate between Capsules in Fully Capsule-Network Speech Recognition | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/lee23_interspeech.pdf) |
|2585 | OOD-Speech: A Large Bengali Speech Recognition Dataset for Out-of-Distribution Benchmarking | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://bengaliai.github.io/asr) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/rakib23_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.09688-b31b1b.svg)](https://arxiv.org/abs/2305.09688) |
| 1316 | ML-SUPERB: Multilingual Speech Universal PERformance Benchmark | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://github.com/espnet/espnet/tree/master/egs2/ml_superb/asr1) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/shi23g_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.10615-b31b1b.svg)](https://arxiv.org/abs/2305.10615) |
| 2389 | General-purpose Adversarial Training for Enhanced Automatic Speech Recognition Model Generalization | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/kim23l_interspeech.pdf) |
| 275 | Joint Instance Reconstruction and Feature Sub-space Alignment for Cross-Domain Speech Emotion Recognition | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/zhao23_interspeech.pdf) |
| 2280 | Knowledge Distillation for Neural Transducer-based Target-Speaker ASR: Exploiting Parallel Mixture/Single-Talker Speech Data | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/moriya23_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.15971-b31b1b.svg)](https://arxiv.org/abs/2305.15971) |
| 1272 | Random Utterance Concatenation based Data Augmentation for Improving Short-Video Speech Recognition | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/lin23i_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2210.15876-b31b1b.svg)](https://arxiv.org/abs/2210.15876) |
| 1189 | Adapter Incremental Continual Learning of Efficient Audio Spectrogram Transformers | [![GitHub](https://img.shields.io/github/stars/NMS05/Adapter-Incremental-Continual-Learning-AST)](https://github.com/NMS05/Adapter-Incremental-Continual-Learning-AST) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/muthuchamyselvaraj23_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2302.14314-b31b1b.svg)](https://arxiv.org/abs/2302.14314) |
| 223 | Rethinking Speech Recognition with a Multimodal Perspective via Acoustic and Semantic Cooperative Decoding | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/zhang23_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.14049-b31b1b.svg)](https://arxiv.org/abs/2305.14049) |
| 923 | Improving Code-Switching and Name Entity Recognition in ASR with Speech Editing based Data Augmentation | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://liangzheng-zl.github.io/bedit-web/) <br /> [![GitHub](https://img.shields.io/github/stars/Liangzheng-ZL/BEdit-TTS)](https://github.com/Liangzheng-ZL/BEdit-TTS) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/liang23b_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.08588-b31b1b.svg)](https://arxiv.org/abs/2306.08588) |
| 2258 | Bypass Temporal Classification: Weakly Supervised Automatic Speech Recognition with Imperfect Transcripts | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/gao23h_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.01031-b31b1b.svg)](https://arxiv.org/abs/2306.01031) |
| 1184 | DCCRN-KWS: An Audio Bias based Model for Noise Robust Small-Footprint Keyword Spotting | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/lv23_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.12331-b31b1b.svg)](https://arxiv.org/abs/2305.12331) |
| 1609 | OTF: Optimal Transport based Fusion of Supervised and Self-Supervised Learning Models for Automatic Speech Recognition | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/fu23b_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.02541-b31b1b.svg)](https://arxiv.org/abs/2306.02541) |
| 2136 | Approximate Nearest Neighbour Phrase Mining for Contextual Speech Recognition | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/bleeker23_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.08862-b31b1b.svg)](https://arxiv.org/abs/2304.08862) |
| 788 | Rehearsal-Free Online Continual Learning for Automatic Speech Recognition | [![GitHub](https://img.shields.io/github/stars/StevenVdEeckt/online-cl-for-asr)](https://github.com/StevenVdEeckt/online-cl-for-asr) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/vandereeckt23_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.10860-b31b1b.svg)](https://arxiv.org/abs/2306.10860) |
| 496 | ASR Data Augmentation in Low-Resource Settings using Cross-Lingual Multi-Speaker TTS and Cross-Lingual Voice Conversion | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://github.com/Edresson/Wav2Vec-Wrapper/tree/main/Papers/TTS-Augmentation) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/casanova23_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2204.00618-b31b1b.svg)](https://arxiv.org/abs/2204.00618) |
| 642 | Personality-aware Training based Speaker Adaptation for End-to-End Speech Recognition | [![GitHub](https://img.shields.io/github/stars/shibeiing/Personality-aware-Training-PAT)](https://github.com/shibeiing/Personality-aware-Training-PAT) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/gu23_interspeech.pdf) |
| 2257 | Target Vocabulary Recognition based on Multi-task Learning with Decomposed Teacher Sequences | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/ito23b_interspeech.pdf) |
| 679 | Wave to Syntax: Probing Spoken Language Models for Syntax | [![GitHub](https://img.shields.io/github/stars/techsword/wave-to-syntax)](https://github.com/techsword/wave-to-syntax) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/shen23_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.18957-b31b1b.svg)](https://arxiv.org/abs/2305.18957) |
| 720 | Effective Training of Attention-based Contextual Biasing Adapters with Synthetic Audio for Personalised ASR | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/naowarat23_interspeech.pdf) <br /> [![Amazon Science](https://img.shields.io/badge/amazon-science-FE9901.svg)](https://www.amazon.science/publications/effective-training-of-attention-based-contextual-biasing-adapters-with-synthetic-audio-for-personalised-asr) |
| 630 | Pushing the Limits of Unsupervised Unit Discovery for SSL Speech Representation | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/ma23b_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.08920-b31b1b.svg)](https://arxiv.org/abs/2306.08920) |
| 1118 | SlothSpeech: Denial-of-Service Attack Against Speech Recognition Models | [![GitHub](https://img.shields.io/github/stars/0xrutvij/SlothSpeech)](https://github.com/0xrutvij/SlothSpeech) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/haque23_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.00794-b31b1b.svg)](https://arxiv.org/abs/2306.00794) |
| 503 | CLRL-Tuning: A Novel Continual Learning Approach for Automatic Speech Recognition | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/wang23d_interspeech.pdf) |
| 159 | Exploring Sources of Racial Bias in Automatic Speech Recognition through the Lens of Rhythmic Variation | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/lai23_interspeech.pdf) |
| 1440 | Can Contextual Biasing Remain Effective with Whisper and GPT-2? | [![GitHub](https://img.shields.io/github/stars/BriansIDP/WhisperBiasing)](https://github.com/BriansIDP/WhisperBiasing) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/sun23e_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.01942-b31b1b.svg)](https://arxiv.org/abs/2306.01942) |
| 221 | Masked Modeling Duo for Speech: Specializing General-Purpose Audio Representation to Speech using Denoising Distillation | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://github.com/nttcslab/m2d/tree/master/speech) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/niizumi23_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.14079-b31b1b.svg)](https://arxiv.org/abs/2305.14079) |
| 2207 | Improving RNN Transducer Acoustic Models for English Conversational Speech Recognition | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/cui23c_interspeech.pdf) |
| 1216 | MixRep: Hidden Representation Mixup for Low-Resource Speech Recognition | [![GitHub](https://img.shields.io/github/stars/jiamin1013/mixrep-espnet)](https://github.com/jiamin1013/mixrep-espnet) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/xie23_interspeech.pdf) |
| 1192 | Improving Chinese Mandarin Speech Recognition using Graph Embedding Regularization | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/lin23h_interspeech.pdf) |
| 1276 | Adapting Multi-Lingual ASR Models for Handling Multiple Talkers | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/li23o_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.18747-b31b1b.svg)](https://arxiv.org/abs/2305.18747) |
| 1221 | Adapter-Tuning with Effective Token-Dependent Representation Shift for Automatic Speech Recognition | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/ng23c_interspeech.pdf) |
| 1010 | Model-Internal Slot-Triggered Biasing for Domain Expansion in Neural Transducer ASR Models | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/lu23c_interspeech.pdf) <br /> [![Amazon Science](https://img.shields.io/badge/amazon-science-FE9901.svg)](https://www.amazon.science/publications/model-internal-slot-triggered-biasing-for-domain-expansion-in-neural-transducer-asr-models) |
| 2508 | Delay-Penalized CTC Implemented based on Finite State Transducer | [![GitHub](https://img.shields.io/github/stars/k2-fsa/k2)](https://github.com/k2-fsa/k2) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/yao23b_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.11539-b31b1b.svg)](https://arxiv.org/abs/2305.11539) |
| 2589 | Vistaar: Diverse Benchmarks and Training Sets for Indian Language ASR | [![GitHub](https://img.shields.io/github/stars/AI4Bharat/vistaar)](https://github.com/AI4Bharat/vistaar) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/bhogale23_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.15386-b31b1b.svg)](https://arxiv.org/abs/2305.15386) |
| 1091 | Domain Adaptive Self-Supervised Training of Automatic Speech Recognition | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/do23_interspeech.pdf) |
| 1105 | There is more than One Kind of Robustness: Fooling Whisper with Adversarial Examples | [![GitHub](https://img.shields.io/github/stars/RaphaelOlivier/whisper_attack)](https://github.com/RaphaelOlivier/whisper_attack) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/olivier23_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2210.17316-b31b1b.svg)](https://arxiv.org/abs/2210.17316) |
| 1064 | MT-SLVR: Multi-Task Self-Supervised Learning for Transformation In(Variant) Representations | [![GitHub](https://img.shields.io/github/stars/CHeggan/MT-SLVR)](https://github.com/CHeggan/MT-SLVR) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/heggan23_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.17191-b31b1b.svg)](https://arxiv.org/abs/2305.17191) |
| 1176 | Reducing Barriers to Self-Supervised Learning: HuBERT Pre-training with Academic Compute | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/chen23l_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.06672-b31b1b.svg)](https://arxiv.org/abs/2306.06672) |
| 759 | Blank-Regularized CTC for Frame Skipping in Neural Transducer | [![GitHub](https://img.shields.io/github/stars/k2-fsa/k2)](https://github.com/k2-fsa/k2) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/yang23l_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.11558-b31b1b.svg)](https://arxiv.org/abs/2305.11558) |
| 2406 | The Tag-Team Approach: Leveraging CLS and Language Tagging for Enhancing Multilingual ASR | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/jayakumar23_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.19584-b31b1b.svg)](https://arxiv.org/abs/2305.19584) |
| 2354 | Improving RNN-Transducers with Acoustic LookAhead | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/unni23_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.05006-b31b1b.svg)](https://arxiv.org/abs/2307.05006) |
| 1847 | Everyone has an Accent | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/markl23_interspeech.pdf) |
| 2124 | Some Voices are too Common: Building Fair Speech Recognition Systems using the Common-Voice Dataset | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/maison23_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.03773-b31b1b.svg)](https://arxiv.org/abs/2306.03773) |
| 1168 | Information Magnitude based Dynamic Sub-Sampling for Speech-to-Text | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/zhang23u_interspeech.pdf) |
| 353 | Towards Multi-task Learning of Speech and Speaker Recognition | [![GitHub](https://img.shields.io/github/stars/nikvaessen/disjoint-mtl)](https://github.com/nikvaessen/disjoint-mtl) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/vaessen23_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2302.12773-b31b1b.svg)](https://arxiv.org/abs/2302.12773) |
| 2186 | Regarding Topology and Variant Frame Rates for Differentiable WFST-based End-to-End ASR | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/zhao23f_interspeech.pdf) |
| 1012 | 2-bit Conformer Quantization for Automatic Speech Recognition | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/rybakov23b_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.16619-b31b1b.svg)](https://arxiv.org/abs/2305.16619) |
| 167 | Time-Domain Speech Enhancement for Robust Automatic Speech Recognition | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/yang23_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2210.13318-b31b1b.svg)](https://arxiv.org/abs/2210.13318) |
| 257 | Multi-Channel Multi-Speaker Transformer for Speech Recognition | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/yifan23_interspeech.pdf) |
| 733 | Fake the Real: Backdoor Attack on Deep Speech Classification via Voice Conversion | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/ye23_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.15875-b31b1b.svg)](https://arxiv.org/abs/2306.15875) |
| 2463 | Dialect Speech Recognition Modeling using Corpus of Japanese Dialects and Self-Supervised Learning-based Model XLSR | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/miwa23_interspeech.pdf) |
| 767 | Contextualized End-to-End Speech Recognition with Contextual Phrase Prediction Network | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/huang23d_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.12493-b31b1b.svg)](https://arxiv.org/abs/2305.12493) |
| 970 | Competitive and Resource Efficient Factored Hybrid HMM Systems are Simpler Than You Think | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/raissi23_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.12493-b31b1b.svg)](https://arxiv.org/abs/2305.12493) |
| 791 | MMSpeech: Multi-Modal Multi-Task Encoder-Decoder Pre-training for Speech Recognition | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/zhou23d_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.00500-b31b1b.svg)](https://arxiv.org/abs/2212.00500) |
| 2499 | Biased Self-Supervised Learning for ASR | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/kreyssig23_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.02536-b31b1b.svg)](https://arxiv.org/abs/2211.02536) |
| 1300 | A Unified Recognition and Correction Model under Noisy and Accent Speech Conditions | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/yang23q_interspeech.pdf) |
| 2470 | Wav2Vec 2.0 ASR for Cantonese-Speaking Older Adults in a Clinical Setting | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/huang23h_interspeech.pdf) |
| 770 | BAT: Boundary aware Transducer for Memory-Efficient and Low-Latency ASR | [![GitHub](https://img.shields.io/github/stars/alibaba-damo-academy/FunASR)](https://github.com/alibaba-damo-academy/FunASR) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/an23_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.11571-b31b1b.svg)](https://arxiv.org/abs/2305.11571) |
| 1342 | Bayes Risk Transducer: Transducer with Controllable Alignment Prediction | [![GitHub](https://img.shields.io/github/stars/espnet/espnet)](https://github.com/espnet/espnet) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/tian23_interspeech.pdf) |
| 783 | Multi-View Frequency-Attention Alternative to CNN Frontends for Automatic Speech Recognition | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/alastruey23_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.06954-b31b1b.svg)](https://arxiv.org/abs/2306.06954) |

### Analysis of Speech and Audio Signals

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

| :id: | **Title** | **Repo** | **Paper** |
|------|-----------|:--------:|:---------:|
| 1173 | Robust Prototype Learning for Anomalous Sound Detection | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/zeng23b_interspeech.pdf) |
| 982 | A Multimodal Prototypical Approach for Unsupervised Sound Classification | [![GitHub](https://img.shields.io/github/stars/sakshamsingh1/audio_text_proto)](https://github.com/sakshamsingh1/audio_text_proto) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/kushwaha23_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.12300-b31b1b.svg)](https://arxiv.org/abs/2306.12300) |
| 563 | Robust Audio Anti-Spoofing with Fusion-Reconstruction Learning on Multi-Order Spectrograms | [![GitHub](https://img.shields.io/github/stars/ph-w2000/S2pecNet)](https://github.com/ph-w2000/S2pecNet) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/wen23_interspeech.pdf) |
| 1082 | Adapting Language-Audio Models as Few-Shot Audio Learners | [![GitHub](https://img.shields.io/github/stars/JinhuaLiang/lam4fsl)](https://github.com/JinhuaLiang/lam4fsl) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/liang23c_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.17719-b31b1b.svg)](https://arxiv.org/abs/2305.17719) |
| 734 | TFECN: Time-Frequency Enhanced ConvNet for Audio Classification | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/wang23l_interspeech.pdf) |
| 350 | Resolution Consistency Training on Time-Frequency Domain for Semi-Supervised Sound Event Detection | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/choi23b_interspeech.pdf) |
| 1174 | Fine-Tuning Audio Spectrogram Transformer with Task-Aware Adapters for Sound Event Detection | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/li23n_interspeech.pdf) |
| 1210 | Small Footprint Multi-Channel Network for Keyword Spotting with Centroid based Awareness | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/ng23b_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2204.05445-b31b1b.svg)](https://arxiv.org/abs/2204.05445) |
| 1380 | Few-Shot Class-Incremental Audio Classification using Adaptively-Refined Prototypes | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/xie23b_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.18045-b31b1b.svg)](https://arxiv.org/abs/2305.18045) |
| 1549 | Interpretable Latent Space using Space-Filling Curves for Phonetic Analysis in Voice Conversion | [![GitLab](https://img.shields.io/gitlab/stars/speech-interaction-technology-aalto-university/sfvq)](https://gitlab.com/speech-interaction-technology-aalto-university/sfvq) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/vali23_interspeech.pdf) <br /> [![Aalto](https://img.shields.io/badge/aalto-fi-005EB8.svg)](https://research.aalto.fi/en/publications/interpretable-latent-space-using-space-filling-curves-for-phoneti) |
| 1861 | Topological Data Analysis for Speech Processing | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://topohubert.github.io/speech-topology-webpages/) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/tulchinskii23_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.17223-b31b1b.svg)](https://arxiv.org/abs/2211.17223) |
| 1329 | Recycle-and-Distill: Universal Compression Strategy for Transformer-based Speech SSL Models with Attention Map Reusing and Masking Distillation | [![GitHub](https://img.shields.io/github/stars/sungnyun/ARMHuBERT)](https://github.com/sungnyun/ARMHuBERT) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/jang23_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.11685-b31b1b.svg)](https://arxiv.org/abs/2305.11685) |
| 932 | Personalized Acoustic Scene Classification in Ultra-Low Power Embedded Devices using Privacy-Preserving Data Augmentation | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/koppelmann23_interspeech.pdf) |
| 176 | Background Domain Switch: A Novel Data Augmentation Technique for Robust Sound Event Detection | [![GitHub](https://img.shields.io/github/stars/boschresearch/soundsee-background-domain-switch)](https://github.com/boschresearch/soundsee-background-domain-switch) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/lin23_interspeech.pdf) |
| 1021 | Joint Prediction of Audio Event and Annoyance Rating in an Urban Soundscape by Hierarchical Graph Representation Learning | [![GitHub](https://img.shields.io/github/stars/Yuanbo2020/HGRL)](https://github.com/Yuanbo2020/HGRL) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/hou23_interspeech.pdf) <br /> [![Pdf](https://img.shields.io/badge/pdf-version-003B10.svg)](http://personal.ee.surrey.ac.uk/Personal/W.Wang/papers/Hou%20etal_INTERSPEECH_2023.pdf) |
| 2416 | Anomalous Sound Detection using Self-Attention-based Frequency Pattern Analysis of Machine Sounds | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/zhang23fa_interspeech.pdf) |
| 1478 | Improving Audio-Text Retrieval via Hierarchical Cross-Modal Interaction and Auxiliary Captions | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/xin23c_interspeech.pdf) |
| 575 | Differential Privacy enabled Dementia Classification: An Exploration of the Privacy-Accuracy Trade-off in Speech Signal Data | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/bn23_interspeech.pdf) |
| 1595 | Learning Emotional Representations from Imbalanced Speech Data for Speech Emotion Recognition and Emotional Text-to-Speech | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://anonymous.4open.science/w/INTERSPEECH2023-F8C4/) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/wang23ka_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.05709-b31b1b.svg)](https://arxiv.org/abs/2306.05709) |
| 1816 | Towards Multi-Lingual Audio Question Answering | [![GitHub](https://img.shields.io/github/stars/swarupbehera/mAQA)](https://github.com/swarupbehera/mAQA) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/behera23_interspeech.pdf) |
| 1344 | Blind Estimation of Room Impulse Response from Monaural Reverberant Speech with Segmental Generative Neural Network | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/liao23_interspeech.pdf) <br /> [![ResearchGate](https://img.shields.io/badge/Research-Gate-D7E7F5.svg)](https://www.researchgate.net/publication/370843606_Blind_Estimation_of_Room_Impulse_Response_from_Monaural_Reverberant_Speech_with_Segmental_Generative_Neural_Network) |
| 358 | Emotion-aware Audio-Driven Face Animation via Contrastive Feature Disentanglement | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/ren23_interspeech.pdf) |
| 591 | Anomalous Sound Detection based on Sound Separation | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/shimonishi23_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.15859-b31b1b.svg)](https://arxiv.org/abs/2305.15859) |
| 2089 | Random Forest Classification of Breathing Phases from Audio Signals Recorded using Mobile Devices | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/fahed23_interspeech.pdf) |
| 1581 | GRAVO: Learning to Generate Relevant Audio from Visual Features with Noisy Online Videos | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/ahn23b_interspeech.pdf) |
| 477 | Wav2ToBI: A New Approach to Automatic ToBI Transcription | [![GitHub](https://img.shields.io/github/stars/reginazhai/Wav2ToBI)](https://github.com/reginazhai/Wav2ToBI) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/zhai23_interspeech.pdf) |
| 344 | Joint-Former: Jointly Regularized and Locally Down-Sampled Conformer for Semi-Supervised Sound Event Detection | [![GitHub](https://img.shields.io/github/stars/mastergofujs/Joint-Former)](https://github.com/mastergofujs/Joint-Former) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/gao23b_interspeech.pdf) |
| 245 | Towards Attention-based Contrastive Learning for Audio Spoof Detection | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/goel23_interspeech.pdf) |
| 2488 | Masked Audio Modeling with CLAP and Multi-Objective Learning | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/xin23d_interspeech.pdf) |
| 1904 | Few-Shot Open-Set Learning for On-Device Customization of KeyWord Spotting Systems | [![GitHub](https://img.shields.io/github/stars/mrusci/ondevice-fewshot-kws)](https://github.com/mrusci/ondevice-fewshot-kws) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/rusci23_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.02161-b31b1b.svg)](https://arxiv.org/abs/2306.02161) |
| 481 | Self-Supervised Dataset Pruning for Efficient Training in Audio Anti-Spoofing | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/azeemi23_interspeech.pdf) |
| 491 | Semantic Segmentation with Bidirectional Language Models Improves Long-Form ASR | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/huang23b_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.18419-b31b1b.svg)](https://arxiv.org/abs/2305.18419) |
| 684 | Multi-Microphone Automatic Speech Segmentation in Meetings based on Circular Harmonics Features | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/mariotte23_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.04268-b31b1b.svg)](https://arxiv.org/abs/2306.04268) |
| 542 | Advanced RawNet2 with Attention-based Channel Masking for Synthetic Speech Detection | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/li23h_interspeech.pdf) |
| 88 | Insights Into End-to-End Audio-to-Score Transcription with Real Recordings: A Case Study with Saxophone Works | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/martinezsevilla23_interspeech.pdf) |
| 2193 | Whisper-AT: Noise-Robust Automatic Speech Recognizers are also Strong Audio Event Taggers | [![GitHub](https://img.shields.io/github/stars/YuanGongND/whisper-at)](https://github.com/YuanGongND/whisper-at) <br /> [![PyPI](https://img.shields.io/pypi/v/whisper-at)](https://pypi.org/project/whisper-at/) <br /> [![Whisper-AT](https://img.shields.io/badge/🤗-demo-FFD21F.svg)](https://huggingface.co/spaces/yuangongfdu/whisper-at) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/gong23d_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.03183-b31b1b.svg)](https://arxiv.org/abs/2307.03183) |
| 1621 | Synthetic Voice Spoofing Detection based on Feature Pyramid Conformer | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/gong23b_interspeech.pdf) |
| 1383 | Learning a Self-Supervised Domain-Invariant Feature Representation for Generalized Audio Deepfake Detection | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/xie23c_interspeech.pdf) |
| 2011 | Application of Knowledge Distillation to Multi-Task Speech Representation Learning | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/kerpicci23_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2210.16611-b31b1b.svg)](https://arxiv.org/abs/2210.16611) |
| 2297 | DeCoR: Defy Knowledge Forgetting by Predicting Earlier Audio Codes | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/jiang23g_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.18441-b31b1b.svg)](https://arxiv.org/abs/2305.18441) |
| 1965 | Variational Classifier for Unsupervised Anomalous Sound Detection under Domain Generalization | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/almudevar23_interspeech.pdf) |
| 745 | FlexiAST: Flexibility is What AST Needs | [![GitHub](https://img.shields.io/github/stars/JiuFengSC/FlexiAST_INTERSPEECH23)](https://github.com/JiuFengSC/FlexiAST_INTERSPEECH23) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/feng23c_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.09286-b31b1b.svg)](https://arxiv.org/abs/2307.09286) |
| 1579 | MCR-Data2vec 2.0: Improving Self-Supervised Speech Pre-training via Model-Level Consistency Regularization | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/yoon23c_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.08463-b31b1b.svg)](https://arxiv.org/abs/2306.08463) |
| 914 | Visually-Aware Audio Captioning With Adaptive Audio-Visual Attention | [![GitHub](https://img.shields.io/github/stars/liuxubo717/V-ACT)](https://github.com/liuxubo717/V-ACT) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/liu23l_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2210.16428-b31b1b.svg)](https://arxiv.org/abs/2210.16428) |
| 165 | Time-Frequency Domain Filter-and-Sum Network for Multi-Channel Speech Separation | [![GitHub](https://img.shields.io/github/stars/JonathanDZ/TF-FaSNet)](https://github.com/JonathanDZ/TF-FaSNet) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/deng23_interspeech.pdf) |
| 801 | Audio-Visual Fusion using Multiscale Temporal Convolutional Attention for Time-Domain Speech Separation | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/liu23h_interspeech.pdf) |
| 1431 | An Efficient Speech Separation Network based on Recurrent Fusion Dilated Convolution and Channel Attention | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/wang23ca_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.05887-b31b1b.svg)](https://arxiv.org/abs/2306.05887) |
| 2015 | Binaural Sound Localization in Noisy Environments using Frequency-based Audio Vision Transformer (FAViT) | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/phokhinanan23_interspeech.pdf) |
| 1723 | Contrastive Learning based Deep Latent Masking for Music Source Separation | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/kim23i_interspeech.pdf) |
| 655 | Speaker Extraction with Detection of Presence and Absence of Target Speakers | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/zhang23k_interspeech.pdf) |
| 889 | PIAVE: A Pose-Invariant Audio-Visual Speaker Extraction Network | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/liu23k_interspeech.pdf) |
| 2117 | Spatial LibriSpeech: An Augmented Dataset for Spatial Audio Learning | [![GitHub](https://img.shields.io/github/stars/apple/ml-spatial-librispeech)](https://github.com/apple/ml-spatial-librispeech) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/sarabia23_interspeech.pdf) <br /> [![Apple](https://img.shields.io/badge/apple-ml-FE9901.svg)](https://machinelearning.apple.com/research/spatial-librispeech) |
| 1309 | Image-Driven Audio-Visual Universal Source Separation | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/li23q_interspeech.pdf) |
| 2520 | Joint Blind Source Separation and Dereverberation for Automatic Speech Recognition using Delayed-Subsource | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/fras23_interspeech.pdf) |
| 1766 | SDNet: Stream-Attention and Dual-Feature Learning Network for Ad-hoc Array Speech Separation | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/wang23na_interspeech.pdf) |
| 2451 | Deeply Supervised Curriculum Learning for Deep Neural Network-based Sound Source Localization | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/baek23_interspeech.pdf) |
| 164 | Multi-Channel Separation of Dynamic Speech and Sound Events | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](http://www.robinscheibler.org/interspeech2023-moving-iva-samples/) <br /> [![GitHub](https://img.shields.io/github/stars/fakufaku/interspeech2023-moving-iva-samples)](https://github.com/fakufaku/interspeech2023-moving-iva-samples) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/fujimura23_interspeech.pdf) |
| 2545 | Rethinking the Visual Cues in Audio-Visual Speaker Extraction | [![GitHub](https://img.shields.io/github/stars/mrjunjieli/DAVSE)](https://github.com/mrjunjieli/DAVSE) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/li23ja_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.02625-b31b1b.svg)](https://arxiv.org/abs/2306.02625) |
| 85 | Using Semi-Supervised Learning for Monaural Time-Domain Speech Separation with a Self-Supervised Learning-based SI-SNR Estimator | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/dang23_interspeech.pdf) |
| 1158 | Investigation of Training Mute-Expressive End-to-End Speech Separation Networks for an Unknown Number of Speakers | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/kim23d_interspeech.pdf) |
| 2369 | SR-SRP: Super-Resolution based SRP-PHAT for Sound Source Localization and Tracking | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/cho23_interspeech.pdf) |
| 613 | Dual-Memory Multi-Modal Learning for Continual Spoken Keyword Spotting with Confidence Selection and Diversity Enhancement | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/yang23g_interspeech.pdf) |
| 714 | FN-SSL: Full-Band and Narrow-Band Fusion for Sound Source Localization | [![GitHub](https://img.shields.io/github/stars/Audio-WestlakeU/FN-SSL)](https://github.com/Audio-WestlakeU/FN-SSL) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/wang23j_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.19610-b31b1b.svg)](https://arxiv.org/abs/2305.19610) |
| 696 | A Neural State-Space Modeling Approach to Efficient Speech Separation | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/chen23g_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.16932-b31b1b.svg)](https://arxiv.org/abs/2305.16932) |
| 1777 | Locate and Beamform: Two-Dimensional Locating All-Neural Beamformer for Multi-Channel Speech Separation | [![GitHub](https://img.shields.io/github/stars/FYJNEVERFOLLOWS/LaBNet)](https://github.com/FYJNEVERFOLLOWS/LaBNet) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/fu23c_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.10821-b31b1b.svg)](https://arxiv.org/abs/2305.10821) |
| 518 | Monaural Speech Separation Method based on Recurrent Attention with Parallel Branches | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/yang23f_interspeech.pdf) |
| 979 | Ontology-Aware Learning and Evaluation for Audio Tagging | [![GitHub](https://img.shields.io/github/stars/haoheliu/ontology-aware-audio-tagging)](https://github.com/haoheliu/ontology-aware-audio-tagging) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/liu23m_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.12195-b31b1b.svg)](https://arxiv.org/abs/2211.12195) |
| 951 | What do Self-Supervised Speech Representations Encode? An Analysis of Languages, Varieties, Speaking Styles and Speakers | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://gitlab.tugraz.at/speech/speechcodebookanalysis) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/linke23_interspeech.pdf) |
| 1696 | A Compressed Synthetic Speech Detection Method with Compression Feature Embedding | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/zhang23ca_interspeech.pdf) |
| 572 | Outlier-aware Inlier Modeling and Multi-Scale Scoring for Anomalous Sound Detection via Multitask Learning | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/zhang23j_interspeech.pdf) |
| 263 | MOSLight: A Lightweight Data-Efficient System for Non-Intrusive Speech Quality Assessment | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/li23c_interspeech.pdf) |
| 1626 | A Multi-Scale Attentive Transformer for Multi-Instrument Symbolic Music Generation | [![GitHub](https://img.shields.io/github/stars/HaRry-qaq/MSAT)](https://github.com/HaRry-qaq/MSAT) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/wei23c_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.16592-b31b1b.svg)](https://arxiv.org/abs/2305.16592) |
| 2494 | MTANet: Multi-band Time-Frequency Attention Network for Singing Melody Extraction from Polyphonic Music | [![GitHub](https://img.shields.io/github/stars/Annmixiu/MTANet)](https://github.com/Annmixiu/MTANet) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/gao23i_interspeech.pdf) |
| 119 | Xiaoicesing 2: A High-Fidelity Singing Voice Synthesizer based on Generative Adversarial Network | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://wavelandspeech.github.io/xiaoice2/) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/chunhui23_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2210.14666-b31b1b.svg)](https://arxiv.org/abs/2210.14666) |
| 2190 | Do Vocal Breath Sounds Encode Gender cues for Automatic Gender Classification? | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/solanki23_interspeech.pdf) |
| 202 | Automatic Exploration of Optimal Data Processing Operations for Sound Data Augmentation using Improved Differentiable Automatic Data Augmentation | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/sugiura23_interspeech.pdf) |
| 1430 | A Snoring Sound Dataset for Body Position Recognition: Collection, Annotation, and Analysis | [![GitHub](https://img.shields.io/github/stars/xiaoli1996/SSBPR)](https://github.com/xiaoli1996/SSBPR) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/xiao23b_interspeech.pdf) |
| 528 | RMVPE: A Robust Model for Vocal Pitch Estimation in Polyphonic Music | [![GitHub](https://img.shields.io/github/stars/Dream-High/RMVPE)](https://github.com/Dream-High/RMVPE) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/wei23b_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.15412-b31b1b.svg)](https://arxiv.org/abs/2306.15412) |
| 832 | Spatialization Quality Metric for Binaural Speech | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/manocha23_interspeech.pdf) |
| 428 | AsthmaSCELNet: A Lightweight Supervised Contrastive Embedding Learning Framework for Asthma Classification using Lung Sounds | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/roy23_interspeech.pdf) |
| 1426 | Patch-Mix Contrastive Learning with Audio Spectrogram Transformer on Respiratory Sound Classification | [![GitHub](https://img.shields.io/github/stars/raymin0223/patch-mix_contrastive_learning)](https://github.com/raymin0223/patch-mix_contrastive_learning) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/bae23b_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.14032-b31b1b.svg)](https://arxiv.org/abs/2305.14032) |
| 2115 | Remote Assessment for ALS using Multimodal Dialog Agents: Data Quality, Feasibility and Task Compliance | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/richter23_interspeech.pdf) <br /> [![Pdf](https://img.shields.io/badge/pdf-version-003B10.svg)](https://drive.google.com/file/d/1ydqKLgO18TFrMzFC2Bz_6y3Uml0bUaaN/view) |
| 852 | AudioToken: Adaptation of Text-Conditioned Diffusion Models for Audio-to-Image Generation | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://pages.cs.huji.ac.il/adiyoss-lab/AudioToken/) <br /> [![GitHub](https://img.shields.io/github/stars/guyyariv/AudioToken)](https://github.com/guyyariv/AudioToken) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/yariv23_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.13050-b31b1b.svg)](https://arxiv.org/abs/2305.13050) |
| 209 | Obstructive Sleep Apnea Screening with Breathing Sounds and Respiratory Effort: A Multimodal Deep Learning Approach | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/romero23_interspeech.pdf) |
| 2275 | Investigation of Music Emotion Recognition based on Segmented Semi-Supervised Learning | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/sun23f_interspeech.pdf) |

### Speech Recognition: Architecture, Search, and Linguistic Components

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

| :id: | **Title** | **Repo** | **Paper** |
|------|-----------|:--------:|:---------:|
| 2344 | Diacritic Recognition Performance in Arabic ASR | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/aldarmaki23_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2302.14022-b31b1b.svg)](https://arxiv.org/abs/2302.14022) |
| 990 | Personalization for BERT-based Discriminative Speech Recognition Rescoring | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/kolehmainen23_interspeech.pdf) <br /> [![Amazon Science](https://img.shields.io/badge/amazon-science-FE9901.svg)](https://www.amazon.science/publications/personalization-for-bert-based-discriminative-speech-recognition-rescoring) |
| 2182 | On the N-gram Approximation of Pre-trained Language Models | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/krishnan23_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.06892-b31b1b.svg)](https://arxiv.org/abs/2306.06892) |
| 2147 | Record Deduplication for Entity Distribution Modeling in ASR Transcripts | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/huang23g_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.06246-b31b1b.svg)](https://arxiv.org/abs/2306.06246) |
| 2205 | Learning When to Trust Which Teacher for Weakly Supervised ASR | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/agrawal23_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.12012-b31b1b.svg)](https://arxiv.org/abs/2306.12012) |
| 1313 | Text-Only Domain Adaptation using Unified Speech-Text Representation in Transducer | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/huang23f_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.04076-b31b1b.svg)](https://arxiv.org/abs/2306.04076) |
| 1378 | Text-Only Domain Adaptation for End-to-End Speech Recognition through Down-Sampling Acoustic Representation | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/zhu23f_interspeech.pdf) |
| 2479 | Knowledge Distillation Approach for Efficient Internal Language Model Estimation | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/chen23u_interspeech.pdf) |
| 276 | Language Model Personalization for Improved Touchscreen Typing | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/adhikary23_interspeech.pdf) |
| 1223 | Blank Collapse: Compressing CTC Emission for the Faster Decoding | [![GitHub](https://img.shields.io/github/stars/minkjung/blankcollapse)](https://github.com/minkjung/blankcollapse) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/jung23b_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2210.17017-b31b1b.svg)](https://arxiv.org/abs/2210.17017) |
| 403 | Improving Joint Speech-Text Representations without Alignment | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/peyser23_interspeech.pdf) |
| 1941 | Leveraging Cross-Utterance Context for ASR Decoding | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/flynn23_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.16903-b31b1b.svg)](https://arxiv.org/abs/2306.16903) |
| 423 | Knowledge Transfer from Pre-trained Language Models to Cif-based Speech Recognizers via Hierarchical Distillation | [![GitHub](https://img.shields.io/github/stars/MingLunHan/CIF-HieraDist)](https://github.com/MingLunHan/CIF-HieraDist) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/han23_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2301.13003-b31b1b.svg)](https://arxiv.org/abs/2301.13003) |
| 1517 | Integration of Frame- and Label-Synchronous Beam Search for Streaming Encoder-Decoder Speech Recognition | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/tsunoo23_interspeech.pdf) |
| 1071 | A Neural Time Alignment Module for End-to-End Automatic Speech Recognition | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/jiang23e_interspeech.pdf) |
| 599 | Accelerating Transducers through Adjacent Token Merging | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/li23i_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.16009-b31b1b.svg)](https://arxiv.org/abs/2306.16009) |
| 617 | Language-Universal Phonetic Representation in Multilingual Speech Pretraining for Low-Resource Speech Recognition | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/feng23_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.11569-b31b1b.svg)](https://arxiv.org/abs/2305.11569) |
| 2292 | Language-Routing Mixture of Experts for Multi-Lingual and Code-Switching Speech Recognition | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/wang23sa_interspeech.pdf) |
| 1437 | Embedding Articulatory Constraints for Low-Resource Speech Recognition based on Large Pre-trained Model | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/lee23h_interspeech.pdf) |
| 2051 | Exploration of Efficient End-to-End ASR using Discretized Input from Self-Supervised Learning | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/chang23b_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.18108-b31b1b.svg)](https://arxiv.org/abs/2305.18108) |
| 768 | SpellMapper: A Non-Autoregressive Neural Spellchecker for ASR Customization with Candidate Retrieval based on N-Gram Mappings | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://github.com/NVIDIA/NeMo/blob/main/tutorials/nlp/SpellMapper_English_ASR_Customization.ipynb) <br /> [![GitHub](https://img.shields.io/github/stars/NVIDIA/NeMo)](https://github.com/NVIDIA/NeMo) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/antonova23_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.02317-b31b1b.svg)](https://arxiv.org/abs/2306.02317) |
| 2037 | Text Injection for Capitalization and Turn-Taking Prediction in Speech Models | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/bijwadia23_interspeech.pdf) |
| 1281 | Confidence-based Ensembles of End-to-End Speech Recognition Models | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://github.com/NVIDIA/NeMo/blob/main/tutorials/asr/Confidence_Ensembles.ipynb) <br /> [![GitHub](https://img.shields.io/github/stars/NVIDIA/NeMo)](https://github.com/NVIDIA/NeMo) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/gitman23_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.15824-b31b1b.svg)](https://arxiv.org/abs/2306.15824) |
| 1050 | Unsupervised Code-Switched Text Generation from Parallel Text | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/chi23_interspeech.pdf) |
| 258 | A Binary Keyword Spotting System With Error-Diffusion Speech Feature Binarization | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/wang23b_interspeech.pdf) |
| 621 | Language-Universal Phonetic Encoder for Low-Resource Speech Recognition | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/feng23b_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.11576-b31b1b.svg)](https://arxiv.org/abs/2305.11576) |
| 863 | A Lexical-aware Non-Autoregressive Transformer-based ASR Model | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/lin23g_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.10839-b31b1b.svg)](https://arxiv.org/abs/2305.10839) |
| 1841 | Improving Under-Resourced Code-Switched Speech Recognition: Large Pre-trained Models or Architectural Interventions | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/vanvuren23_interspeech.pdf) |
| 61 | A Model for Every User and Budget: Label-Free and Personalized Mixed-Precision Quantization | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/fish23_interspeech.pdf) |
| 137 | Modeling Dependent Structure for Utterances in ASR Evaluation | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/liu23_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2209.05281-b31b1b.svg)](https://arxiv.org/abs/2209.05281) |
| 757 | ASR for Low Resource and Multilingual Noisy Code-Mixed Speech | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/verma23_interspeech.pdf) |
| 390 | Accurate and Reliable Confidence Estimation based on Non-Autoregressive End-to-End Speech Recognition System | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/shi23b_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.10680-b31b1b.svg)](https://arxiv.org/abs/2305.10680) |
| 737 | Combining Multilingual Resources and Models to Develop State-of-the-Art E2E ASR for Swedish | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/mateju23_interspeech.pdf) |
| 1171 | Two Stage Contextual Word Filtering for Context bias in Unified Streaming and Non-Streaming Transducer | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/yang23o_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2301.06735-b31b1b.svg)](https://arxiv.org/abs/2301.06735) |
| 1867 | Towards Continually Learning New Languages | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/pham23_interspeech.pdf) |
| 1616 | N-best T5: Robust ASR Error Correction using Multiple Input Hypotheses and Constrained Decoding Space | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/ma23e_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.00456-b31b1b.svg)](https://arxiv.org/abs/2303.00456) |
| 1432 | SememeASR: Boosting Performance of End-to-End Speech Recognition against Domain and Long-Tailed Data Shift with Sememe Semantic Knowledge | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/zhu23g_interspeech.pdf) |
| 1162 | miniStreamer: Enhancing Small Conformer with Chunked-Context Masking for Streaming ASR Applications on the Edge | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/gulzar23_interspeech.pdf) |
| 1469 | CoMFLP: Correlation Measure based Fast Search on ASR Layer Pruning | [![GitHub](https://img.shields.io/github/stars/louislau1129/CoMFLP)](https://github.com/louislau1129/CoMFLP) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/liu23p_interspeech.pdf) |
| 1337 | Exploration on HuBERT with Multiple Resolution | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/shi23h_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.01084-b31b1b.svg)](https://arxiv.org/abs/2306.01084) |
| 2045 | Quantization-aware and Tensor-compressed Training of Transformers for Natural Language Understanding | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/yang23s_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.01076-b31b1b.svg)](https://arxiv.org/abs/2306.01076) |
| 2355 | Word-Level Confidence Estimation for CTC Models | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/naowarat23b_interspeech.pdf) |
| 2235 | Multilingual Contextual Adapters to Improve Custom Word Recognition in Low-Resource Languages | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/kulshreshtha23_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.00759-b31b1b.svg)](https://arxiv.org/abs/2307.00759) |
| 614 | Unsupervised Active Learning: Optimizing Labeling Cost-Effectiveness for Automatic Speech Recognition | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/zheng23_interspeech.pdf) |
| 1303 | 4D ASR: Joint Modeling of CTC, Attention, Transducer, and Mask-Predict Decoders | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/sudo23_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.10818-b31b1b.svg)](https://arxiv.org/abs/2212.10818) |
| 1086 | Neural Model Reprogramming with Similarity based Mapping for Low-Resource Spoken Command Recognition | [![GitHub](https://img.shields.io/github/stars/dodohow1011/SpeechAdvReprogram)](https://github.com/dodohow1011/SpeechAdvReprogram) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/yen23_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2110.03894-b31b1b.svg)](https://arxiv.org/abs/2110.03894) |
| 262 | Language-Specific Boundary Learning for Improving Mandarin-English Code-Switching Speech Recognition | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/fan23_interspeech.pdf) |
| 480 | Mixture-of-Expert Conformer for Streaming Multilingual ASR | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/hu23c_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.15663-b31b1b.svg)](https://arxiv.org/abs/2305.15663) |
| 1665 | Lossless 4-bit Quantization of Architecture Compressed Conformer ASR Systems on the 300-hr Switch-board Corpus | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/li23x_interspeech.pdf) |
| 2544 | Compressed MoE ASR Model based on Knowledge Distillation and Quantization | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/yuan23c_interspeech.pdf) |

### Speech Recognition: Technologies and Systems for New Applications

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

| :id: | **Title** | **Repo** | **Paper** |
|------|-----------|:--------:|:---------:|
| 2044 | Syllable Discovery and Cross-Lingual Generalization in a Visually Grounded, Self-Supervised Speech Model | [![GitHub](https://img.shields.io/github/stars/jasonppy/syllable-discovery)](https://github.com/jasonppy/syllable-discovery) | [![arXiv](https://img.shields.io/badge/arXiv-2305.11435-b31b1b.svg)](https://arxiv.org/abs/2305.11435) |
| 2032 | Prompting the Hidden Talent of Web-Scale Speech Models for Zero-Shot Task Generalization | [![GitHub](https://img.shields.io/github/stars/jasonppy/PromptingWhisper)](https://github.com/jasonppy/PromptingWhisper) | [![arXiv](https://img.shields.io/badge/arXiv-2305.11095-b31b1b.svg)](https://arxiv.org/abs/2305.11095) |
| 235 | Progress and Prospects for Spoken Language Technology: Results from Five Sexennial Surveys | :heavy_minus_sign: | :heavy_minus_sign: |
| 268 | Acoustic Word Embeddings for Untranscribed Target Languages with Continued Pretraining and Learned Pooling | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2306.02153-b31b1b.svg)](https://arxiv.org/abs/2306.02153) |
| 601 | CASA-ASR: Context-Aware Speaker-Attributed ASR | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2305.12459-b31b1b.svg)](https://arxiv.org/abs/2305.12459) |
| 1321 | Unsupervised Learning of Discrete Latent Representations with Data-Adaptive Dimensionality from Continuous Speech Streams | :heavy_minus_sign: | :heavy_minus_sign: |
| 1167 | AD-TUNING: An Adaptive CHILD-TUNING Approach to Efficient Hyperparameter Optimization of Child Networks for Speech Processing Tasks in the SUPERB Benchmark | [![GitHub](https://img.shields.io/github/stars/liyunlongaaa/AD-TUNING)](https://github.com/liyunlongaaa/AD-TUNING) | :heavy_minus_sign: |
| 190 | Distilling Knowledge from Gaussian Process Teacher to Neural Network Student | :heavy_minus_sign: | :heavy_minus_sign: |
| 135 | Segmental SpeechCLIP: Utilizing Pretrained Image-Text Models for Audio-Visual Learning | :heavy_minus_sign: | :heavy_minus_sign: |
| 421 | Towards Hate Speech Detection in Low-Resource Languages: Comparing ASR to Acoustic Word Embeddings on Wolof and Swahili | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2306.00410-b31b1b.svg)](https://arxiv.org/abs/2306.00410) |
| 385 | Mitigating Catastrophic Forgetting for Few-Shot Spoken Word Classification through Meta-Learning | [![GitHub](https://img.shields.io/github/stars/ByteFuse/MAMLCon)](https://github.com/ByteFuse/MAMLCon) | [![arXiv](https://img.shields.io/badge/arXiv-2305.13080-b31b1b.svg)](https://arxiv.org/abs/2305.13080) |
| 664 | Online Punctuation Restoration using ELECTRA Model for Streaming ASR Systems | :heavy_minus_sign: | :heavy_minus_sign: |
| 2066 | Language Agnostic Data-Driven Inverse Text Normalization | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2301.08506-b31b1b.svg)](https://arxiv.org/abs/2301.08506) |
| 1079 | How to Estimate Model Transferability of Pre-trained Speech Models? | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2306.01015-b31b1b.svg)](https://arxiv.org/abs/2306.01015) |
| 1655 | Transcribing Speech as Spoken and Written Dual Text using an Autoregressive Model | :heavy_minus_sign: | :heavy_minus_sign: |
| 587 | Phonetic and Prosody-aware Self-Supervised Learning Approach for Non-Native Fluency Scoring | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2305.11438-b31b1b.svg)](https://arxiv.org/abs/2305.11438) |
| 380 | Disentangling the Contribution of Non-Native Speech in Automated Pronunciation Assessment | :heavy_minus_sign: | :heavy_minus_sign: |
| 337 | A Joint Model for Pronunciation Assessment and Mispronunciation Detection and Diagnosis with Multi-task Learning | :heavy_minus_sign: | :heavy_minus_sign: |
| 1635 | Assessing Intelligibility in Non-Native Speech: Comparing Measures Obtained at Different Levels | :heavy_minus_sign: | :heavy_minus_sign: |
| 585 | End-to-End Word-Level Pronunciation Assessment with MASK Pre-training | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2306.02682-b31b1b.svg)](https://arxiv.org/abs/2306.02682) |
| 550 | A Hierarchical Context-aware Modeling Approach for Multi-Aspect and Multi-Granular Pronunciation Assessment | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2305.18146-b31b1b.svg)](https://arxiv.org/abs/2305.18146) |
| 2541 | Automatic Prediction of Language Learners' Listenability using Speech and Text Features Extracted from Listening Drills | :heavy_minus_sign: | :heavy_minus_sign: |
| 2371 | Assessment of Non-Native Speech Intelligibility using Wav2vec2-based Mispronunciation Detection and Multi-Level Goodness of Pronunciation Transformer | :heavy_minus_sign: | :heavy_minus_sign: |
| 1899 | Adapting an Unadaptable ASR System | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2306.01208-b31b1b.svg)](https://arxiv.org/abs/2306.01208) |
| 533 | Addressing Cold Start Problem for End-to-End Automatic Speech Scoring | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2306.14310-b31b1b.svg)](https://arxiv.org/abs/2306.14310) |
| 816 | Improving Grapheme-to-Phoneme Conversion by Learning Pronunciations from Speech Recordings | :heavy_minus_sign: | [![Amazon Science](https://img.shields.io/badge/amazon-science-FE9901.svg)](https://www.amazon.science/publications/improving-grapheme-to-phoneme-conversion-by-learning-pronunciations-from-speech-recordings) |
| 2577 | Orthography-based Pronunciation Scoring for Better CAPT Feedback | :heavy_minus_sign: | [![Pdf](https://img.shields.io/badge/pdf-version-003B10.svg)](https://catir.github.io/art/capt_phone_ctc_2023.pdf) |
| 1592 | Zero-Shot Automatic Pronunciation Assessment | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2305.19563-b31b1b.svg)](https://arxiv.org/abs/2305.19563) |
| 364 | Mispronunciation Detection and Diagnosis Model for Tonal Language, Applied to Vietnamese | :heavy_minus_sign: | :heavy_minus_sign: |
| 793 | An Efficient and Noise-Robust Audiovisual Encoder for Audiovisual Speech Recognition | :heavy_minus_sign: | :heavy_minus_sign: |
| 540 | A Novel Self-training Approach for Low-Resource Speech Recognition | :heavy_minus_sign: | :heavy_minus_sign: |
| 1428 | FunASR: A Fundamental End-to-End Speech Recognition Toolkit | [![GitHub](https://img.shields.io/github/stars/alibaba-damo-academy/FunASR)](https://github.com/alibaba-damo-academy/FunASR) | [![arXiv](https://img.shields.io/badge/arXiv-2305.11013-b31b1b.svg)](https://arxiv.org/abs/2305.11013) |
| 487 | Streaming Audio-Visual Speech Recognition with Alignment Regularization | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2211.02133-b31b1b.svg)](https://arxiv.org/abs/2211.02133) |
| 462 | SparseVSR: Lightweight and Noise Robust Visual Speech Recognition | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2307.04552-b31b1b.svg)](https://arxiv.org/abs/2307.04552) |
| 2262 | Multimodal Speech Recognition for Language-Guided Embodied Agents | [![GitHub](https://img.shields.io/github/stars/Cylumn/embodied-multimodal-asr)](https://github.com/Cylumn/embodied-multimodal-asr) | [![arXiv](https://img.shields.io/badge/arXiv-2302.14030-b31b1b.svg)](https://arxiv.org/abs/2302.14030) |

### Lexical and Language Modeling for ASR

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

| :id: | **Title** | **Repo** | **Paper** |
|------|-----------|:--------:|:---------:|
| 643 | NoRefER: A Referenceless Quality Metric for Automatic Speech Recognition via Semi-Supervised Language Model Fine-Tuning with Contrastive Learning | [![GitHub](https://img.shields.io/github/stars/aixplain/NoRefER)](https://github.com/aixplain/NoRefER) | [![arXiv](https://img.shields.io/badge/arXiv-2306.12577-b31b1b.svg)](https://arxiv.org/abs/2306.12577) |
| 2128 | Scaling Laws for Discriminative Speech Recognition Rescoring Models | :heavy_minus_sign: | [![Amazon Science](https://img.shields.io/badge/amazon-science-FE9901.svg)](https://www.amazon.science/publications/scaling-laws-for-discriminative-speech-recognition-rescoring-models) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.15815-b31b1b.svg)](https://arxiv.org/abs/2306.15815) |
| 2429 | Exploring Energy-based Language Models with Different Architectures and Training Methods for Speech Recognition | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://github.com/thu-spmi/CAT/blob/master/docs/energy-based_LM_training.md) | [![arXiv](https://img.shields.io/badge/arXiv-2305.12676-b31b1b.svg)](https://arxiv.org/abs/2305.12676) |
| 1362 | Memory Augmented Lookup Dictionary based Language Modeling for Automatic Speech Recognition | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2301.00066-b31b1b.svg)](https://arxiv.org/abs/2301.00066) |
| 1251 | Memory Network-based End-To-End Neural ES-KMeans for Improved Word Segmentation | :heavy_minus_sign: | :heavy_minus_sign: |
| 1320 | Retraining-free Customized ASR for Enharmonic Words based on a Named-Entity-Aware Model and Phoneme Similarity Estimation | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2305.17846-b31b1b.svg)](https://arxiv.org/abs/2305.17846) |

### Language Identification and Diarization

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

| :id: | **Title** | **Repo** | **Paper** |
|------|-----------|:--------:|:---------:|
| 304 | Lightweight and Efficient Spoken Language Identification of Long-form Audio | :heavy_minus_sign: | :heavy_minus_sign: |
| 1109 | End-to-End Spoken Language Diarization with Wav2vec Embeddings | :heavy_minus_sign: | :heavy_minus_sign: |
| 1986 | Efficient Spoken Language Recognition via Multilabel Classification | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2306.01945-b31b1b.svg)](https://arxiv.org/abs/2306.01945) |
| 1529 | Description and Analysis of ABC Submission to NIST LRE 2022 | :heavy_minus_sign: | :heavy_minus_sign: |
| 1790 | Exploring the Impact of Pretrained Models and Web-Scraped Data for the 2022 NIST Language Recognition Evaluation | :heavy_minus_sign: | :heavy_minus_sign: |
| 1094 | Advances in Language Recognition in Low Resource African Languages: The JHU-MIT Submission for NIST LRE22 | :heavy_minus_sign: | :heavy_minus_sign: |

### Speech Quality Assessment

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

| :id: | **Title** | **Repo** | **Paper** |
|------|-----------|:--------:|:---------:|
| 1436 | DeePMOS: Deep Posterior Mean-Opinion-Score of Speech | :heavy_minus_sign: | :heavy_minus_sign: |
| 1644 | The Role of Formant and Excitation Source Features in Perceived Naturalness of Low Resource Tribal Language TTS: An Empirical Study | :heavy_minus_sign: | :heavy_minus_sign: |
| 811 | A No-Reference Speech Quality Assessment Method based on Neural Network with Densely Connected Convolutional Architecture | :heavy_minus_sign: | :heavy_minus_sign: |
| 2507 | Probing Speech Quality Information in ASR Systems | :heavy_minus_sign: | :heavy_minus_sign: |
| 589 | Preference-based Training Framework for Automatic Speech Quality Assessment using Deep Neural Network | :heavy_minus_sign: | :heavy_minus_sign: |
| 389 | Crowdsourced Data Validation for ASR Training | :heavy_minus_sign: | :heavy_minus_sign: |

### Feature Modeling for ASR

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

| :id: | **Title** | **Repo** | **Paper** |
|------|-----------|:--------:|:---------:|
| 2296 | Re-Investigating the Efficient Transfer Learning of Speech Foundation Model using Feature Fusion Methods | :heavy_minus_sign: | :heavy_minus_sign: |
| 1556 | Robust Automatic Speech Recognition via WavAugment Guided Phoneme Adversarial Training | :heavy_minus_sign: | :heavy_minus_sign: |
| 509 | InterFormer: Interactive Local and Global Features Fusion for Automatic Speech Recognition | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2305.16342-b31b1b.svg)](https://arxiv.org/abs/2305.16342) |
| 579 | Transductive Feature Space Regularization for Few-Shot Bioacoustic Event Detection | :heavy_minus_sign: | :heavy_minus_sign: |
| 615 | Incorporating L2 Phonemes using Articulatory Features for Robust Speech Recognition | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2306.02534-b31b1b.svg)](https://arxiv.org/abs/2306.02534) |
| 1510 | On the (In)Efficiency of Acoustic Feature Extractors for Self-Supervised Speech Representation Learning | :heavy_minus_sign: | [![HAL Science](https://img.shields.io/badge/hal-science-040060.svg)](https://hal.science/hal-04116371) |

### Interfacing Speech Technology and Phonetics

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

| :id: | **Title** | **Repo** | **Paper** |
|------|-----------|:--------:|:---------:|
| 1846 | Phonemic Competition in End-to-End ASR models | :heavy_minus_sign: | :heavy_minus_sign: |
| 443 | Automatic Speaker Recognition with Variation Across Vocal Conditions: A Controlled Experiment with Implications for Forensics | :heavy_minus_sign: | :heavy_minus_sign: |
| 1398 | Exploring Graph Theory Methods for the Analysis of Pronunciation Variation in Spontaneous Speech | :heavy_minus_sign: | :heavy_minus_sign: |
| 680 | Automatic Speaker Recognition Performance with Matched and Mismatched Female Bilingual Speech Data | :heavy_minus_sign: | :heavy_minus_sign: |

### Speech Synthesis: Multilinguality

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

| :id: | **Title** | **Repo** | **Paper** |
|------|-----------|:--------:|:---------:|
| 2303 | FACTSpeech: Speaking a Foreign Language Pronunciation using Only Your Native Characters | :heavy_minus_sign: | :heavy_minus_sign: |
| 934 | Cross-Lingual Transfer Learning for Phrase Break Prediction with Multilingual Language Model | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2306.02579-b31b1b.svg)](https://arxiv.org/abs/2306.02579) |
| 363 | DSE-TTS: Dual Speaker Embedding for Cross-Lingual Text-to-Speech | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://goarsenal.github.io/DSE-TTS/) | [![arXiv](https://img.shields.io/badge/arXiv-2306.14145-b31b1b.svg)](https://arxiv.org/abs/2306.14145) |
| 1467 | Generating Multilingual Gender-Ambiguous Text-to-Speech Voices | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://innoetics.github.io/publications/gender-ambiguous/index.html) | [![arXiv](https://img.shields.io/badge/arXiv-2211.00375-b31b1b.svg)](https://arxiv.org/abs/2211.00375) |
| 2330 | RADMMM: Multilingual Multiaccented Multispeaker Text-to-Speech | :heavy_minus_sign: | [![NVidia AI](https://img.shields.io/badge/NVidia-AI-78B900.svg)](https://research.nvidia.com/labs/adlr/projects/radmmm/) |
| 861 | Multilingual Context-based Pronunciation Learning for Text-to-Speech | :heavy_minus_sign: | [![Amazon Science](https://img.shields.io/badge/amazon-science-FE9901.svg)](https://www.amazon.science/publications/multilingual-context-based-pronunciation-learning-for-text-to-speech) |

### Speech Emotion Recognition

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

| :id: | **Title** | **Repo** | **Paper** |
|------|-----------|:--------:|:---------:|
| 2170 | Personalized Adaptation with Pre-trained Speech Encoders for Continuous Emotion Recognition | :heavy_minus_sign: | :heavy_minus_sign: |
| 1113 | The Importance of Calibration: Rethinking Confidence and Performance of Speech Multi-label Emotion Classifiers | :heavy_minus_sign: | [![BIIC](https://img.shields.io/badge/biic-research-F7C552.svg)](https://biic.ee.nthu.edu.tw/research.php?id=166) |
| 1080 | A Preliminary Study on Augmenting Speech Emotion Recognition using a Diffusion Model | [![Emulation AI](https://img.shields.io/badge/Emulation-AI-161B1F.svg)](https://emulationai.com/research/diffusion-ser/) | [![arXiv](https://img.shields.io/badge/arXiv-2305.11413-b31b1b.svg)](https://arxiv.org/abs/2305.11413) |
| 454 | Privacy Risks in Speech Emotion Recognition: A Systematic Study on Gender Inference Attack | :heavy_minus_sign: | :heavy_minus_sign: |
| 2111 | Episodic Memory For Domain-Adaptable, Robust Speech Emotion Recognition | :heavy_minus_sign: | :heavy_minus_sign: |
| 80 | Stable Speech Emotion Recognition with Head-k-Pooling Loss | :heavy_minus_sign: | :heavy_minus_sign: |
| 1923 | Node-weighted Graph Convolutional Network for Depression Detection in Transcribed Clinical Interviews | [![GitHub](https://img.shields.io/github/stars/idiap/Node_weighted_GCN_for_depression_detection)](https://github.com/idiap/Node_weighted_GCN_for_depression_detection) | [![arXiv](https://img.shields.io/badge/arXiv-2307.00920-b31b1b.svg)](https://arxiv.org/abs/2307.00920) |
| 756 | Two-Stage Finetuning of Wav2vec 2.0 for Speech Emotion Recognition with ASR and Gender Pretraining | :heavy_minus_sign: | :heavy_minus_sign: |
| 240 | The Co-use of Laughter and Head Gestures Across Speech Styles | :heavy_minus_sign: | :heavy_minus_sign: |
| 1351 | EmotionNAS: Two-Stream Neural Architecture Search for Speech Emotion Recognition | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2203.13617-b31b1b.svg)](https://arxiv.org/abs/2203.13617) |
| 136 | Pre-Finetuning for Few-Shot Emotional Speech Recognition | [![GitHub](https://img.shields.io/github/stars/maxlchen/Speech-PreFinetuning)](https://github.com/maxlchen/Speech-PreFinetuning) | [![arXiv](https://img.shields.io/badge/arXiv-2302.12921-b31b1b.svg)](https://arxiv.org/abs/2302.12921) |
| 293 | Integrating Emotion Recognition with Speech Recognition and Speaker Diarization for Conversations | :heavy_minus_sign: | :heavy_minus_sign: |
| 1075 | Utility-Preserving Privacy-Enabled Speech Embeddings for Emotion Detection | :heavy_minus_sign: | :heavy_minus_sign: |
| 890 | A Context-Constrained Sentence Modeling for Deception Detection in Real Interrogation | :heavy_minus_sign: | :heavy_minus_sign: |
| 1914 | Laughter in Task-based Settings: Whom We Talk to Affects How, When, and How Often We Laugh | :heavy_minus_sign: | :heavy_minus_sign: |
| 653 | Exploring Downstream Transfer of Self-Supervised Features for Speech Emotion Recognition | :heavy_minus_sign: | :heavy_minus_sign: |
| 1758 | Leveraging Semantic Information for Efficient Self-Supervised Emotion Recognition with Audio-Textual Distilled Models | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2305.19184-b31b1b.svg)](https://arxiv.org/abs/2305.19184) |
| 819 | MetricAug: A Distortion Metric-Lead Augmentation Strategy for Training Noise-Robust Speech Emotion Recognizer | [![GitHub](https://img.shields.io/github/stars/crowpeter/MetricAug)](https://github.com/crowpeter/MetricAug) | :heavy_minus_sign: |
| 1311 | Investigating Acoustic Cues for Multilingual Abuse Detection | :heavy_minus_sign: | :heavy_minus_sign: |
| 1600 | A Novel Frequency Warping Scale for Speech Emotion Recognition | :heavy_minus_sign: | :heavy_minus_sign: |
| 1170 | Multi-Scale Temporal Transformer for Speech Emotion Recognition | :heavy_minus_sign: | :heavy_minus_sign: |
| 1169 | Distant Speech Emotion Recognition in an Indoor Human-Robot Interaction Scenario | :heavy_minus_sign: | :heavy_minus_sign: |
| 2498 | A Study on Prosodic Entrainment in Relation to Therapist Empathy in Counseling Conversation | :heavy_minus_sign: | :heavy_minus_sign: |
| 2375 | Improving Joint Speech and Emotion Recognition using Global Style Tokens | :heavy_minus_sign: | :heavy_minus_sign: |
| 1163 | Speech Emotion Recognition by Estimating Emotional Label Sequences with Phoneme Class Attribute | :heavy_minus_sign: | :heavy_minus_sign: |
| 274 | Unsupervised Transfer Components Learning for Cross-Domain Speech Emotion Recognition | :heavy_minus_sign: | :heavy_minus_sign: |
| 1090 | Dual Memory Fusion for Multimodal Speech Emotion Recognition | :heavy_minus_sign: | :heavy_minus_sign: |
| 311 | Hybrid Dataset for Speech Emotion Recognition in Russian Language | :heavy_minus_sign: | :heavy_minus_sign: |
| 396 | Speech Emotion Recognition using Decomposed Speech via Multi-Task Learning | :heavy_minus_sign: | :heavy_minus_sign: |

### Spoken Dialog Systems and Conversational Analysis

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

| :id: | **Title** | **Repo** | **Paper** |
|------|-----------|:--------:|:---------:|
| 46 | FC-MTLF: A Fine- and Coarse-grained Multi-task Learning Framework for Cross-Lingual Spoken Language Understanding | :heavy_minus_sign: | :heavy_minus_sign: |
| 93 | Cˆ2A-SLU: Cross and Contrastive Attention for Improving ASR Robustness in Spoken Language Understanding | :heavy_minus_sign: | :heavy_minus_sign: |
| 2300 | Tri-level Joint Natural Language Understanding for Multi-turn Conversational Datasets | [![GitHub](https://img.shields.io/github/stars/adlnlp/Tri-NLU)](https://github.com/adlnlp/Tri-NLU) | [![arXiv](https://img.shields.io/badge/arXiv-2305.17729-b31b1b.svg)](https://arxiv.org/abs/2305.17729) |
| 2234 | Semantic Enrichment Towards Efficient Speech Representations | :heavy_minus_sign: | :heavy_minus_sign: |
| 1299 | Tensor Decomposition for Minimization of E2E SLU Model Toward On-Device Processing | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2306.01247-b31b1b.svg)](https://arxiv.org/abs/2306.01247) |
| 699 | DiffSLU: Knowledge Distillation based Diffusion Model for Cross-Lingual Spoken Language Understanding | :heavy_minus_sign: | :heavy_minus_sign: |
| 1962 | Integrating Pretrained ASR and LM to perform Sequence Generation for Spoken Language Understanding | :heavy_minus_sign: | :heavy_minus_sign: |
| 644 | Contrastive Learning based ASR Robust Knowledge Selection for Spoken Dialogue System | :heavy_minus_sign: | :heavy_minus_sign: |
| 1859 | Unsupervised Dialogue Topic Segmentation in Hyperdimensional Space | :heavy_minus_sign: | :heavy_minus_sign: |
| 198 | An Investigation of the Combination of Rehearsal and Knowledge Distillation in Continual Learning for Spoken Language Understanding | [![GitHub](https://img.shields.io/github/stars/umbertocappellazzo/CL_SLU)](https://github.com/umbertocappellazzo/CL_SLU) | [![arXiv](https://img.shields.io/badge/arXiv-2211.08161-b31b1b.svg)](https://arxiv.org/abs/2211.08161) |
| 1740 | Enhancing New Intent Discovery via Robust Neighbor-based Contrastive Learning | :heavy_minus_sign: | [![Pdf](https://img.shields.io/badge/pdf-version-003B10.svg)](http://chenmengdx.github.io/papers/IS23-NID.pdf) |
| 211 | Personalized Predictive ASR for Latency Reduction in Voice Assistants | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2305.13794-b31b1b.svg)](https://arxiv.org/abs/2305.13794) |
| 1419 | Compositional Generalization in Spoken Language Understanding | :heavy_minus_sign: | :heavy_minus_sign: |
| 2314 | Sampling Bias in NLU Models: Impact and Mitigation | :heavy_minus_sign: | [![Amazon Science](https://img.shields.io/badge/amazon-science-FE9901.svg)](https://www.amazon.science/publications/sampling-bias-in-nlu-models-impact-and-mitigation) |
| 1038 | 5IDER: Unified Query Rewriting for Steering, Intent Carryover, Disfluencies, Entity Carryover and Repair | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2306.01855-b31b1b.svg)](https://arxiv.org/abs/2306.01855) |
| 1236 | Emotion Awareness in Multi-utterance Turn for Improving Emotion Prediction in Multi-Speaker Conversation | :heavy_minus_sign: | :heavy_minus_sign: |
| 1505 | WhiSLU: End-to-End Spoken Language Understanding with Whisper | :heavy_minus_sign: | :heavy_minus_sign: |
| 1947 | Relationship between Auditory and Semantic Entrainment using Deep Neural Networks (DNN) | :heavy_minus_sign: | :heavy_minus_sign: |
| 1929 | Unsupervised Auditory and Semantic Entrainment Models with Deep Neural Networks | :heavy_minus_sign: | :heavy_minus_sign: |
| 952 | Parsing Dialog Turns with Prosodic Features in English | :heavy_minus_sign: | :heavy_minus_sign: |
| 320 | Estimation of Listening Response Timing by Generative Model and Parameter Control of Response Substantialness using Dynamic-Prompt-Tune | :heavy_minus_sign: | :heavy_minus_sign: |
| 1885 | Parameter Selection for Analyzing Conversations with Autism Spectrum Disorder | :heavy_minus_sign: | :heavy_minus_sign: |
| 2341 | Efficient Multimodal Neural Networks for Trigger-Less Voice Assistants | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2305.12063-b31b1b.svg)](https://arxiv.org/abs/2305.12063) |
| 2332 | Rapid Lexical Alignment to a Conversational Agent | :heavy_minus_sign: | :heavy_minus_sign: |
| 578 | Multimodal Turn-Taking Model using Visual cues for End-of-Utterance Prediction in Spoken Dialogue Systems | :heavy_minus_sign: | :heavy_minus_sign: |
| 1464 | Audio-Visual Praise Estimation for Conversational Video based on Synchronization-Guided Multimodal Transformer | :heavy_minus_sign: | :heavy_minus_sign: |
| 1618 | Improving the Response Timing Estimation for Spoken Dialogue Systems by Reducing the Effect of Speech Recognition Delay | :heavy_minus_sign: | :heavy_minus_sign: |
| 555 | Focus-Attention-Enhanced Cross-Modal Transformer with Metric Learning for Multimodal Speech Emotion Recognition | :heavy_minus_sign: | :heavy_minus_sign: |
| 1717 | A Multiple-Teacher Pruning based Self-Distillation (MT-PSD) Approach to Model Compression for Audio-Visual Wake Word Spotting | :heavy_minus_sign: | :heavy_minus_sign: |
| 789 | Abusive Speech Detection in Indic Languages using Acoustic Features | :heavy_minus_sign: | :heavy_minus_sign: |
| 1791 | Listening to Silences In Contact Center Conversations using Textual cues | :heavy_minus_sign: | :heavy_minus_sign: |
| 2475 | I Learned Error, I Can Fix It!: A Detector-Corrector Structure for ASR Error Calibration | :heavy_minus_sign: | :heavy_minus_sign: |
| 1074 | Verbal and Nonverbal Feedback Signals in Response to Increasing Levels of Miscommunication | :heavy_minus_sign: | :heavy_minus_sign: |
| 76 | Speech-based Classification of Defensive Communication: A Novel Dataset and Results | :heavy_minus_sign: | :heavy_minus_sign: |
| 1951 | Quantifying the Perceptual Value of Lexical and Non-Lexical Channels in Speech | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://sarenne.github.io/is-2023/) | :heavy_minus_sign: |
| 1267 | Relationships between Gender, Personality Traits and Features of Multi-Modal Data to Responses to Spoken Dialog Systems Breakdown | :heavy_minus_sign: | :heavy_minus_sign: |
| 1650 | Speaker-aware Cross-Modal Fusion Architecture for Conversational Emotion Recognition | :heavy_minus_sign: | :heavy_minus_sign: |

### Speech Coding and Enhancement

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

| :id: | **Title** | **Repo** | **Paper** |
|------|-----------|:--------:|:---------:|
| 936 | Biophysically-Inspired Single-Channel Speech Enhancement in the Time Domain | :heavy_minus_sign: | :heavy_minus_sign: |
| 1902 | On-Device Speaker Anonymization of Acoustic Embeddings for ASR based on Flexible Location Gradient Reversal Layer | :heavy_minus_sign: | :heavy_minus_sign: |
| 1901 | How to Construct Perfect and Worse-than-Coin-Flip Spoofing Countermeasures: A Word of Warning on Shortcut Learning | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2306.00044-b31b1b.svg)](https://arxiv.org/abs/2306.00044) |
| 1287 | CleanUNet 2: A Hybrid Speech Denoising Model on Waveform and Spectrogram | :heavy_minus_sign: | :heavy_minus_sign: |
| 521 | A Two-Stage Progressive Neural Network for Acoustic echo Cancellation | :heavy_minus_sign: | [![ResearchGate](https://img.shields.io/badge/Research-Gate-D7E7F5.svg)](https://www.researchgate.net/publication/371040399_A_Two-stage_Progressive_Neural_Network_for_Acoustic_Echo_Cancellation) |
| 537 | An Intra-BRNN and GB-RVQ based End-to-End Neural Audio Codec | :heavy_minus_sign: | :heavy_minus_sign: |
| 1066 | Real-Time Personalised Speech Enhancement Transformers with Dynamic Cross-Attended Speaker Representations | :heavy_minus_sign: | :heavy_minus_sign: |
| 280 | CFTNet: Complex-Valued Frequency Transformation Network for Speech Enhancement | :heavy_minus_sign: | :heavy_minus_sign: |
| 623 | Feature Normalization for Fine-Tuning Self-Supervised Models in Speech Enhancement | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2306.08406-b31b1b.svg)](https://arxiv.org/abs/2306.08406) |
| 1490 | Multi-Mode Neural Speech Coding based on Deep Generative Networks | :heavy_minus_sign: | :heavy_minus_sign: |
| 751 | Streaming Dual-Path Transformer for Speech Enhancement | :heavy_minus_sign: | :heavy_minus_sign: |
| 1848 | Sequence-to-Sequence Multi-Modal Speech In-Painting | :heavy_minus_sign: | :heavy_minus_sign: |
| 984 | Hybrid AHS: A Hybrid of Kalman Filter and Deep Learning for Acoustic Howling Suppression | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2305.02583-b31b1b.svg)](https://arxiv.org/abs/2305.02583) |
| 551 | Differentially Private Adapters for Parameter Efficient Acoustic Modeling | [![GitHub](https://img.shields.io/github/stars/Chun-wei-Ho/Private-Speech-Adapter)](https://github.com/Chun-wei-Ho/Private-Speech-Adapter) | [![arXiv](https://img.shields.io/badge/arXiv-2305.11360-b31b1b.svg)](https://arxiv.org/abs/2305.11360) |
| 780 | Incorporating Ultrasound Tongue Images for Audio-Visual Speech Enhancement through Knowledge Distillation | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://zhengrachel.github.io/UTIforAVSE-demo/) <br /> [![GitHub](https://img.shields.io/github/stars/ZhengRachel/UTIforAVSE-demo)](https://github.com/ZhengRachel/UTIforAVSE-demo) | [![arXiv](https://img.shields.io/badge/arXiv-2305.14933-b31b1b.svg)](https://arxiv.org/abs/2305.14933) |
| 2568 | Consonant-Emphasis Method Incorporating Robust Consonant-Section Detection to Improve Intelligibility of Bone-Conducted Speech | :heavy_minus_sign: | :heavy_minus_sign: |
| 1578 | Downstream Task-Agnostic Speech Enhancement with Self-Supervised Representation Loss | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2305.14723-b31b1b.svg)](https://arxiv.org/abs/2305.14723) |
| 2305 | Perceptual Improvement of Deep Neural Network (DNN) Speech Coder using Parametric and Nonparametric Density Models | :heavy_minus_sign: | :heavy_minus_sign: |
| 2437 | DeFT-AN RT: Real-Time Multichannel Speech Enhancement using Dense Frequency-Time Attentive Network and Non-overlapping Synthesis Window | :heavy_minus_sign: | :heavy_minus_sign: |
| 1376 | PCNN: A Lightweight Parallel Conformer Neural Network for Efficient Monaural Speech Enhancement | :heavy_minus_sign: | :heavy_minus_sign: |
| 1364 | Exploring the Interactions between Target Positive and Negative Information for Acoustic Echo Cancellation | :heavy_minus_sign: | :heavy_minus_sign: |
| 365 | Iterative Autoregression: A Novel Trick to Improve your Low-Latency Speech Enhancement Model | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2211.01751-b31b1b.svg)](https://arxiv.org/abs/2211.01751) |
| 1084 | A Multi-Dimensional Deep Structured State Space Approach to Speech Enhancement using Small-Footprint Models | [![GitHub](https://img.shields.io/github/stars/Kuray107/S4ND-U-Net_speech_enhancement)](https://github.com/Kuray107/S4ND-U-Net_speech_enhancement) | [![arXiv](https://img.shields.io/badge/arXiv-2306.00331-b31b1b.svg)](https://arxiv.org/abs/2306.00331) |
| 705 | Domain Adaptation for Speech Enhancement in a Large Domain Gap | :heavy_minus_sign: | :heavy_minus_sign: |
| 456 | SCP-GAN: Self-Correcting Discriminator Optimization for Training Consistency Preserving Metric GAN on Speech Enhancement Tasks | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2210.14474-b31b1b.svg)](https://arxiv.org/abs/2210.14474) |
| 339 | A Mask Free Neural Network for Monaural Speech Enhancement | [![GitHub](https://img.shields.io/github/stars/ioyy900205/MFNet)](https://github.com/ioyy900205/MFNet) | [![arXiv](https://img.shields.io/badge/arXiv-2306.04286-b31b1b.svg)](https://arxiv.org/abs/2306.04286) |
| 1548 | A Training and Inference Strategy using Noisy and Enhanced Speech as Target for Speech Enhancement without Clean Speech | [![GitHub](https://img.shields.io/github/stars/Sinica-SLAM/Ny-EnhTT)](https://github.com/Sinica-SLAM/Ny-EnhTT) | [![arXiv](https://img.shields.io/badge/arXiv-2210.15368-b31b1b.svg)](https://arxiv.org/abs/2210.15368) |
| 2418 | A Simple RNN Model for Lightweight, Low-Compute and Low-Latency Multichannel Speech Enhancement in the Time Domain | :heavy_minus_sign: | :heavy_minus_sign: |
| 1433 | High Fidelity Speech Enhancement with Band-Split RNN | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2212.00406-b31b1b.svg)](https://arxiv.org/abs/2212.00406) |
| 218 | Focus on the Sound around You: Monaural Target Speaker Extraction via Distance and Speaker Information | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://thuhcsi.github.io/interspeech2023-NS-Extractor/) <br /> [![GitHub](https://img.shields.io/github/stars/thuhcsi/interspeech2023-NS-Extractor)](https://github.com/thuhcsi/interspeech2023-NS-Extractor) | [![arXiv](https://img.shields.io/badge/arXiv-2306.16241-b31b1b.svg)](https://arxiv.org/abs/2306.16241) |
| 882 | DFSNet: A Steerable Neural Beamformer Invariant to Microphone Array Configuration for Real-Time, Low-Latency Speech Enhancement | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2302.13407-b31b1b.svg)](https://arxiv.org/abs/2302.13407) |
| 1323 | Speaker-Aware Anti-Spoofing | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2303.01126-b31b1b.svg)](https://arxiv.org/abs/2303.01126) |
| 1116 | Impact of Residual Noise and Artifacts in Speech Enhancement Errors on Intelligibility of Human and Machine | :heavy_minus_sign: | :heavy_minus_sign: |
| 799 | EffCRN: An Efficient Convolutional Recurrent Network for High-Performance Speech Enhancement | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2306.02778-b31b1b.svg)](https://arxiv.org/abs/2306.02778) |
| 1795 | HAD-ANC: A Hybrid System Comprising an Adaptive Filter and Deep Neural Networks for Active Noise Control | :heavy_minus_sign: | :heavy_minus_sign: |
| 886 | MSAF: A Multiple Self-Attention Field Method for Speech Enhancement | :heavy_minus_sign: | :heavy_minus_sign: |
| 2302 | Ultra Dual-Path Compression for Joint echo Cancellation and Noise Suppression | :heavy_minus_sign: | :heavy_minus_sign: |
| 971 | ABC-KD: Attention-based-Compression Knowledge Distillation for Deep Learning-based Noise Suppression | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2305.16665-b31b1b.svg)](https://arxiv.org/abs/2305.16665) |
| 1532 | PLCMOS – a Data-Driven Non-Intrusive Metric for the Evaluation of Packet Loss Concealment Algorithms | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://github.com/microsoft/PLC-Challenge/tree/main/PLCMOS) <br /> [![PyPI](https://img.shields.io/pypi/v/speechmos)](https://pypi.org/project/speechmos/) | [![arXiv](https://img.shields.io/badge/arXiv-2305.15127-b31b1b.svg)](https://arxiv.org/abs/2305.15127) |
| 1910 | Multi-Dataset Co-training with Sharpness-aware Optimization for Audio Anti-Spoofing | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2305.19953-b31b1b.svg)](https://arxiv.org/abs/2305.19953) |
| 1445 | Reducing the Prior Mismatch of Stochastic Differential Equations for Diffusion-based Speech Enhancement | [![GitHub](https://img.shields.io/github/stars/sp-uhh/sgmse-bbed)](https://github.com/sp-uhh/sgmse-bbed) | [![arXiv](https://img.shields.io/badge/arXiv-2302.14748-b31b1b.svg)](https://arxiv.org/abs/2302.14748) |
| 901 | Complex-valued Neural Networks for Voice Anti-Spoofing | :heavy_minus_sign: | :heavy_minus_sign: |
| 1028 | DeepVQE: Real Time Deep Voice Quality Enhancement for Joint Acoustic echo Cancellation, Noise Suppression and Dereverberation | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2306.03177-b31b1b.svg)](https://arxiv.org/abs/2306.03177) |
| 1547 | Diffiner: A Versatile Diffusion-based Generative Refiner for Speech Enhancement | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2210.17287-b31b1b.svg)](https://arxiv.org/abs/2210.17287) |
| 1642 | HD-DEMUCS: General Speech Restoration with Heterogeneous Decoders | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2306.01411-b31b1b.svg)](https://arxiv.org/abs/2306.01411) |
| 1441 | MP-SENet: A Speech Enhancement Model with Parallel Denoising of Magnitude and Phase Spectra | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://yxlu-0102.github.io/mpsenet-demo/) <br /> [![GitHub](https://img.shields.io/github/stars/yxlu-0102/MP-SENet)](https://github.com/yxlu-0102/MP-SENet) | [![arXiv](https://img.shields.io/badge/arXiv-2305.13686-b31b1b.svg)](https://arxiv.org/abs/2305.13686) |
| 565 | TRIDENTSE: Guiding Speech Enhancement with 32 Global Tokens | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2210.12995-b31b1b.svg)](https://arxiv.org/abs/2210.12995) |
| 1254 | Detection of Cross-Dataset Fake Audio based on Prosodic and Pronunciation Features | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2305.13700-b31b1b.svg)](https://arxiv.org/abs/2305.13700) |
| 1890 | Self-Supervised Learning with Diffusion based Multichannel Speech Enhancement for Speaker Verification under Noisy Conditions | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2307.02244-b31b1b.svg)](https://arxiv.org/abs/2307.02244) |
| 1341 | Two-Stage Voice Anonymization for Enhanced Privacy | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2306.16069-b31b1b.svg)](https://arxiv.org/abs/2306.16069) |
| 2055 | Personalized Dereverberation of Speech | :heavy_minus_sign: | :heavy_minus_sign: |
| 580 | Weighted Von Mises Distribution-based Loss Function for Real-Time STFT Phase Reconstruction using DNN | :heavy_minus_sign: | :heavy_minus_sign: |
| 272 | Deep Multi-Frame Filtering for Hearing Aids | [![GitHub](https://img.shields.io/github/stars/rikorose/deepfilternet)](https://github.com/rikorose/deepfilternet) | [![arXiv](https://img.shields.io/badge/arXiv-2305.08225-b31b1b.svg)](https://arxiv.org/abs/2305.08225) |
| 1232 | Aligning Speech Enhancement for Improving Downstream Classification Performance | :heavy_minus_sign: | :heavy_minus_sign: |
| 420 | DNN-based Parameter Estimation for MVDR Beamforming and Post-Filtering | :heavy_minus_sign: | :heavy_minus_sign: |
| 675 | FRA-RIR: Fast Random Approximation of the Image-Source | [![GitHub](https://img.shields.io/github/stars/tencent-ailab/FRA-RIR)](https://github.com/tencent-ailab/FRA-RIR) | [![arXiv](https://img.shields.io/badge/arXiv-2208.04101-b31b1b.svg)](https://arxiv.org/abs/2208.04101) |
| 686 | Rethinking Complex-Valued Deep Neural Networks for Monaural Speech Enhancement | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2301.04320-b31b1b.svg)](https://arxiv.org/abs/2301.04320) |
| 186 | Harmonic Enhancement using Learnable Comb Filter for Light-Weight Full-band Speech Enhancement Model | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2306.00812-b31b1b.svg)](https://arxiv.org/abs/2306.00812) |

### Paralinguistics

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

| :id: | **Title** | **Repo** | **Paper** |
|------|-----------|:--------:|:---------:|
| 1023 | Detection of Emotional Hotspots in Meetings using a Cross-Corpus Approach | :heavy_minus_sign: | :heavy_minus_sign: |
| 1412 | Detection of Laughter and Screaming using the Attention and CTC Models | :heavy_minus_sign: | :heavy_minus_sign: |
| 1852 | Capturing Formality in Speech Across Domains and Languages | :heavy_minus_sign: | [![Pdf](https://img.shields.io/badge/pdf-version-003B10.svg)](http://www.cs.columbia.edu/speech/PaperFiles/2023/interspeech23_formality.pdf) |
| 460 | Towards Robust Family-Infant Audio Analysis based on Unsupervised Pretraining of Wav2vec 2.0 on Large-Scale Unlabeled Family Audio | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://github.com/jialuli3/speechbrain/tree/infant-voc-classification/recipes/wav2vec_LittleBeats) | [![arXiv](https://img.shields.io/badge/arXiv-2305.12530-b31b1b.svg)](https://arxiv.org/abs/2305.12530) |
| 778 | Cues to Next-Speaker Projection in Conversational Swedish: Evidence from Reaction Times | :heavy_minus_sign: | [![psyArXiv](https://img.shields.io/badge/psyArXiv-Preprints-226B79.svg)](https://psyarxiv.com/qasge/) |
| 1200 | Multiple Instance Learning for Inference of Child Attachment from Paralinguistic Aspects of Speech | :heavy_minus_sign: | :heavy_minus_sign: |
| 2070 | Speaker Embeddings as Individuality Proxy for Voice Stress Detection | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2306.05915-b31b1b.svg)](https://arxiv.org/abs/2306.05915) |
| 2213 | From Interval to Ordinal: A HMM based Approach for Emotion Label Conversion | :heavy_minus_sign: | :heavy_minus_sign: |
| 661 | Turbo your Multi-Modal Classification with Contrastive Learning | :heavy_minus_sign: | :heavy_minus_sign: |
| 497 | Towards Paralinguistic-Only Speech Representations for End-to-End Speech Emotion Recognition | :heavy_minus_sign: | :heavy_minus_sign: |
| 1360 | SOT: Self-Supervised Learning-Assisted Optimal Transport for Unsupervised Adaptive Speech Emotion Recognition | :heavy_minus_sign: | :heavy_minus_sign: |
| 2464 | On the Efficacy and Noise-Robustness of Jointly Learned Speech Emotion and Automatic Speech Recognition | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2305.12540-b31b1b.svg)](https://arxiv.org/abs/2305.12540) |
| 830 | Speaking State Decoder with Transition Detection for Next Speaker Prediction | :heavy_minus_sign: | :heavy_minus_sign: |
| 1507 | What are Differences? Comparing DNN and Human by their Performance and Characteristics in Speaker Age Estimation | :heavy_minus_sign: | :heavy_minus_sign: |
| 846 | Effects of Perceived Gender on the Perceived Social Function of Laughter | :heavy_minus_sign: | :heavy_minus_sign: |
| 1999 | Implicit Phonetic Information Modeling for Speech Emotion Recognition | :heavy_minus_sign: | [![idiap](https://img.shields.io/badge/idiap.ch.5062-FF6A00.svg)](https://publications.idiap.ch/publications/show/5062) |
| 1034 | Computation and Memory Efficient Noise Adaptation of Wav2Vec2.0 for Noisy Speech Emotion Recognition with Skip Connection Adapters | :heavy_minus_sign: | :heavy_minus_sign: |
| 300 | Multi-Level Knowledge Distillation for Speech Emotion Recognition in Noisy Conditions | :heavy_minus_sign: | :heavy_minus_sign: |
| 1108 | Preference Learning Labels by Anchoring on Consecutive Annotations | :heavy_minus_sign: | :heavy_minus_sign: |
| 2561 | Transforming the Embeddings: A Lightweight Technique for Speech Emotion Recognition Tasks | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2305.18640-b31b1b.svg)](https://arxiv.org/abs/2305.18640) |
| 543 | Learning Local to Global Feature Aggregation for Speech Emotion Recognition | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2306.01491-b31b1b.svg)](https://arxiv.org/abs/2306.01491) |
| 842 | Supervised Contrastive Learning with Nearest Neighbor Search for Speech Emotion Recognition | :heavy_minus_sign: | :heavy_minus_sign: |

### Speech Enhancement and Denoising

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

| :id: | **Title** | **Repo** | **Paper** |
|------|-----------|:--------:|:---------:|
| 1088 | Real-Time Joint Personalized Speech Enhancement and Acoustic echo Cancellation | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2211.02773-b31b1b.svg)](https://arxiv.org/abs/2211.02773) |
| 514 | TaylorBeamixer: Learning Taylor-Inspired All-Neural Multi-Channel Speech Enhancement from Beam-Space Dictionary Perspective | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://andong-li-speech.github.io/TaylorBM-Demo/) | [![arXiv](https://img.shields.io/badge/arXiv-2211.12024-b31b1b.svg)](https://arxiv.org/abs/2211.12024) |
| 865 | MFT-CRN:Multi-Scale Fourier Transform for Monaural Speech Enhancement | :heavy_minus_sign: | :heavy_minus_sign: |
| 1265 | Variance-Preserving-based Interpolation Diffusion Models for Speech Enhancement | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2306.08527-b31b1b.svg)](https://arxiv.org/abs/2306.08527) |
| 318 | Multi-Input Multi-Output Complex Spectral Mapping for Speaker Separation | :heavy_minus_sign: | :heavy_minus_sign: |
| 992 | Short-Term Extrapolation of Speech Signals using Recursive Neural Networks in the STFT Domain | :heavy_minus_sign: | :heavy_minus_sign: |

### Speech Synthesis: Evaluation

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

| :id: | **Title** | **Repo** | **Paper** |
|------|-----------|:--------:|:---------:|
| 1843 | Listener Sensitivity to Deviating Obstruents in WaveNet | :heavy_minus_sign: | :heavy_minus_sign: |
| 981 | How Generative Spoken Language Modeling Encodes Noisy Speech: Investigation from Phonetics to Syntactics | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2306.00697-b31b1b.svg)](https://arxiv.org/abs/2306.00697) |
| 2014 | MOS vs. AB: Evaluating Text-to-Speech Systems Reliably using Clustered Standard Errors | :heavy_minus_sign: | :heavy_minus_sign: |
| 851 | RAMP: Retrieval-Augmented MOS Prediction via Confidence-based Dynamic Weighting | :heavy_minus_sign: | :heavy_minus_sign: |
| 2013 | Can Better Perception Become a Disadvantage? Synthetic Speech Perception in Congenitally Blind Users | :heavy_minus_sign: | :heavy_minus_sign: |
| 1076 | Investigating Range-Equalizing Bias in Mean Opinion Score Ratings of Synthesized Speech | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2305.10608-b31b1b.svg)](https://arxiv.org/abs/2305.10608) |

### End-to-End Spoken Dialog Systems

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

| :id: | **Title** | **Repo** | **Paper** |
|------|-----------|:--------:|:---------:|
| 1799 | Can ChatGPT Detect Intent? Evaluating Large Language Models for Spoken Language Understanding | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://mutiann.github.io/papers/ChatGPT_SLU/) | [![arXiv](https://img.shields.io/badge/arXiv-2305.13512-b31b1b.svg)](https://arxiv.org/abs/2305.13512) |
| 1760 | Improving End-to-End SLU performance with Prosodic Attention and Distillation | [![GitHub](https://img.shields.io/github/stars/skit-ai/slu-prosody)](https://github.com/skit-ai/slu-prosody) | [![arXiv](https://img.shields.io/badge/arXiv-2305.08067-b31b1b.svg)](https://arxiv.org/abs/2305.08067) |
| 2575 | Modality Confidence Aware Training for Robust End-to-End Spoken Language Understanding | :heavy_minus_sign: | :heavy_minus_sign: |
|758 | Cross-Modal Semantic Alignment before Fusion for Two-Pass End-to-End Spoken Language | :heavy_minus_sign: | :heavy_minus_sign: |
| 2018 | ConvKT: Conversation-Level Knowledge Transfer for Context Aware End-to-End Spoken Language Understanding | :heavy_minus_sign: | :heavy_minus_sign: |
| 41 | GhostT5: Generate More Features with Cheap Operations to Improve Textless Spoken Question Answering | :heavy_minus_sign: | :heavy_minus_sign: |

### Biosignal-enabled Spoken Communication

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

| :id: | **Title** | **Repo** | **Paper** |
|------|-----------|:--------:|:---------:|
| 278 | Obstructive Sleep Apnea Detection using Pretrained Speech Representations | :heavy_minus_sign: | :heavy_minus_sign: |
| 620 | EEG-based Auditory Attention Detection with Spatiotemporal Graph and Graph Convolutional Network | :heavy_minus_sign: | :heavy_minus_sign: |
| 1966 | Silent Speech Recognition with Articulator Positions Estimated from Tongue Ultrasound and Lip Video | :heavy_minus_sign: | :heavy_minus_sign: |
| 1377 | Auditory Attention Detection in Real-Life Scenarios using Common Spatial Patterns from EEG | :heavy_minus_sign: | :heavy_minus_sign: |
| 1381 | Diff-E: Diffusion-based Learning for Decoding Imagined Speech EEG | [![GitHub](https://img.shields.io/github/stars/yorgoon/DiffE)](https://github.com/yorgoon/DiffE) | :heavy_minus_sign: |
| 40 | Towards Ultrasound Tongue Image Prediction from EEG During Speech Production | [![GitHub](https://img.shields.io/github/stars/BME-SmartLab/EEG-to-UTI)](https://github.com/BME-SmartLab/EEG-to-UTI) | [![arXiv](https://img.shields.io/badge/arXiv-2306.05374-b31b1b.svg)](https://arxiv.org/abs/2306.05374) |
| 1607 | Adaptation of Tongue Ultrasound-based Silent Speech Interfaces using Spatial Transformer Networks | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2305.19130-b31b1b.svg)](https://arxiv.org/abs/2305.19130) |
| 174 | STE-GAN: Speech-to-Electromyography Signal Conversion using Generative Adversarial Networks | :heavy_minus_sign: | :heavy_minus_sign: |
| 1881 | Spanish Phone Confusion Analysis for EMG-based Silent Speech Interfaces | :heavy_minus_sign: | :heavy_minus_sign: |
| 805 | Hybrid Silent Speech Interface through Fusion of Electroencephalography and Electromyography | :heavy_minus_sign: | :heavy_minus_sign: |

### Neural-based Speech and Acoustic Analysis

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

| :id: | **Title** | **Repo** | **Paper** |
|------|-----------|:--------:|:---------:|
| 1968 | Can Self-Supervised Neural Representations Pre-trained on Human Speech Distinguish Animal Callers? | [![GitHub](https://img.shields.io/github/stars/idiap/ssl-caller-detection)](https://github.com/idiap/ssl-caller-detection) | [![arXiv](https://img.shields.io/badge/arXiv-2305.14035-b31b1b.svg)](https://arxiv.org/abs/2305.14035) |
| 2342 | Discovering COVID-19 Coughing and Breathing Patterns from Unlabeled Data using Contrastive Learning with Varying Pre-Training Domains | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2306.01864-b31b1b.svg)](https://arxiv.org/abs/2306.01864) |
| 330 | Background-aware Modeling for Weakly Supervised Sound Event Detection | :heavy_minus_sign: | :heavy_minus_sign: |
| 1065 | How to (Virtually) Train Your Speaker Localizer | [![GitHub](https://img.shields.io/github/stars/prerak23/Dir_SrcMic_DOA)](https://github.com/prerak23/Dir_SrcMic_DOA) | [![arXiv](https://img.shields.io/badge/arXiv-2211.16958-b31b1b.svg)](https://arxiv.org/abs/2211.16958) |
| 2271 | MMER: Multimodal Multi-task Learning for Speech Emotion Recognition | [![GitHub](https://img.shields.io/github/stars/Sreyan88/MMER)](https://github.com/Sreyan88/MMER) | [![arXiv](https://img.shields.io/badge/arXiv-2203.16794-b31b1b.svg)](https://arxiv.org/abs/2203.16794) |
| 909 | A Multi-task Learning Framework for Sound Event Detection using High-Level Acoustic Characteristics of Sounds | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2305.10729-b31b1b.svg)](https://arxiv.org/abs/2305.10729) |

### DiGo - Dialog for Good: Speech and Language Technology for Social Good

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

| :id: | **Title** | **Repo** | **Paper** |
|------|-----------|:--------:|:---------:|
| 2194 | A Multimodal Investigation of Speech, Text, Cognitive and Facial Video Features for Characterizing Depression with and without Medication | :heavy_minus_sign: | [![Pdf](https://img.shields.io/badge/pdf-version-003B10.svg)](https://drive.google.com/file/d/1n6ymnLKt21RDfawu9WHsd8tgmBPuz9SC/view) |
| 307 | Understanding Disrupted Sentences using Underspecified Abstract Meaning Representation | [![GitHub](https://img.shields.io/github/stars/amazon-science/disrupt-amr)](https://github.com/amazon-science/disrupt-amr) | [![Amazon Science](https://img.shields.io/badge/amazon-science-FE9901.svg)](https://www.amazon.science/publications/understanding-disrupted-sentences-using-underspecified-abstract-meaning-representation) |
| 2109 | Developing Speech Processing Pipelines for Police Accountability | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2306.06086-b31b1b.svg)](https://arxiv.org/abs/2306.06086) |
| 2086 | Prosody-Controllable Gender-Ambiguous Speech Synthesis: A Tool for Investigating Implicit Bias in Speech Perception | :heavy_minus_sign: | :heavy_minus_sign: |
| 848 | Affective Attributes of French Caregivers' Professional Speech | :heavy_minus_sign: | :heavy_minus_sign: |

### Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources, and Evaluation

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

| :id: | **Title** | **Repo** | **Paper** |
|------|-----------|:--------:|:---------:|
| 180 | Pragmatic Pertinence: A Learnable Confidence Metric to Assess the Subjective Quality of LM-Generated Text | :heavy_minus_sign: | :heavy_minus_sign: |
| 2078 | ASR and Emotional Speech: A Word-Level Investigation of the Mutual Impact of Speech and Emotion Recognition | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2305.16065-b31b1b.svg)](https://arxiv.org/abs/2305.16065) |
| 916 | BASS: Block-wise Adaptation for Speech Summarization | :heavy_minus_sign: | :heavy_minus_sign: |
| 1258 | Speaker Tracking using Graph Attention Networks with Varying Duration Utterances in Multi-Channel Naturalistic Data: Fearless Steps Apollo 11 Audio Corpus | :heavy_minus_sign: | :heavy_minus_sign: |
| 36 | Combining Language Corpora in a Japanese Electromagnetic Articulography Database for Acoustic-to-Articulatory Inversion | :heavy_minus_sign: | :heavy_minus_sign: |
| 523 | A Dual Attention-based Modality-Collaborative Fusion Network for Emotion Recognition | :heavy_minus_sign: | :heavy_minus_sign: |
| 2174 | Large Dataset Generation of Synchronized Music Audio and Lyrics at Scale using Teacher-Student Paradigm | :heavy_minus_sign: | :heavy_minus_sign: |
| 483 | Enc-Dec RNN Acoustic Word Embeddings Learned via Pairwise Prediction | [![GitHub](https://img.shields.io/github/stars/madhavlab/2023_adhiraj_encdecPairwisePred)](https://github.com/madhavlab/2023_adhiraj_encdecPairwisePred) | :heavy_minus_sign: |
| 864 | Query based Acoustic Summarization for Podcasts | :heavy_minus_sign: | :heavy_minus_sign: |
| 1242 | Spot Keywords from Very Noisy and Mixed Speech | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2305.17706-b31b1b.svg)](https://arxiv.org/abs/2305.17706) |
| 891 | Knowledge Distillation on Joint Task End-to-End Speech Translation | :heavy_minus_sign: | [![Amazon Science](https://img.shields.io/badge/amazon-science-FE9901.svg)](https://www.amazon.science/publications/knowledge-distillation-on-joint-task-end-to-end-speech-translation) |
| 343 | Investigating Pre-trained Audio Encoders in the Low-Resource Condition | [![GitHub](https://img.shields.io/github/stars/YangHao97/investigateAudioEncoders)](https://github.com/YangHao97/investigateAudioEncoders) | [![arXiv](https://img.shields.io/badge/arXiv-2305.17733-b31b1b.svg)](https://arxiv.org/abs/2305.17733) |
| 1718 | Improving Textless Spoken Language Understanding with Discrete Units as Intermediate Target | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2305.18096-b31b1b.svg)](https://arxiv.org/abs/2305.18096) |
| 823 | MAVD: The First Open Large-Scale Mandarin Audio-Visual Dataset with Depth Information | [![GitHub](https://img.shields.io/github/stars/SpringHuo/MAVD)](https://github.com/YangHao97/investigateAudioEncoders) | [![arXiv](https://img.shields.io/badge/arXiv-2306.02263-b31b1b.svg)](https://arxiv.org/abs/2306.02263) |
| 1674 | CN-Celeb-AV: A Multi-Genre Audio-Visual Dataset for Person Recognition | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](http://cnceleb.org/) | [![arXiv](https://img.shields.io/badge/arXiv-2305.16049-b31b1b.svg)](https://arxiv.org/abs/2305.16049) |
| 1762 | Improving Zero-Shot Cross-Domain Slot Filling via Transformer-based Slot Semantics Fusion | :heavy_minus_sign: | :heavy_minus_sign: |
| 619 | Rethinking Transfer and Auxiliary Learning for Improving Audio Captioning Transformer | :heavy_minus_sign: | :heavy_minus_sign: |
| 1468 | Boosting Punctuation Restoration with Data Generation and Reinforcement Learning | :heavy_minus_sign: | :heavy_minus_sign: |
| 695 | J-ToneNet: A Transformer-based Encoding Network for Improving Tone Classification in Continuous Speech via F0 Sequences | :heavy_minus_sign: | :heavy_minus_sign: |
| 1152 | Towards Cross-Language Prosody Transfer for Dialog | :heavy_minus_sign: | :heavy_minus_sign: |
| 2506 | Strategies for Improving Low Resource Speech to Text Translation Relying on Pre-trained ASR Models | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2306.00208-b31b1b.svg)](https://arxiv.org/abs/2306.00208) |
 | 1980 | ITALIC: An Italian Intent Classification Dataset | [![GitHub](https://img.shields.io/github/stars/RiTA-nlp/ITALIC)](https://github.com/RiTA-nlp/ITALIC) <br /> [![Zenodo](https://img.shields.io/badge/Zenodo-dataset-FFD1BF.svg)](https://zenodo.org/record/8040649) | [![arXiv](https://img.shields.io/badge/arXiv-2306.08502-b31b1b.svg)](https://arxiv.org/abs/2306.08502) |
| 1778 | Perceptual and Task-Oriented Assessment of a Semantic Metric for ASR Evaluation | :heavy_minus_sign: | :heavy_minus_sign: |
| 1466 | How ChatGPT is Robust for Spoken Language Understanding? | :heavy_minus_sign: | :heavy_minus_sign: |
| 1233 | GigaST: A 10,000-hour Pseudo Speech Translation Corpus | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://st-benchmark.github.io/resources/GigaST.html) | [![arXiv](https://img.shields.io/badge/arXiv-2204.03939-b31b1b.svg)](https://arxiv.org/abs/2204.03939) |
| 1570 | Boosting Chinese ASR Error Correction with Dynamic Error Scaling Mechanism | :heavy_minus_sign: | :heavy_minus_sign: |
| 2473 | Crowdsource-based Validation of the Audio Cocktail as a Sound Browsing Tool | :heavy_minus_sign: | :heavy_minus_sign: |
| 1675 | PunCantonese: A Benchmark Corpus for Low-Resource Cantonese Punctuation Restoration from Speech Transcripts | :heavy_minus_sign: | :heavy_minus_sign: |
| 1358 | Speech-to-Face Conversion using Denoising Diffusion Probabilistic Models | :heavy_minus_sign: | :heavy_minus_sign: |
| 2255 | Inter-Connection: Effective Connection between Pre-trained Encoder and Decoder for Speech Translation | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2305.16897-b31b1b.svg)](https://arxiv.org/abs/2305.16897) |
| 1068 | How Does Pretraining Improve Discourse-aware Translation? | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2305.19847-b31b1b.svg)](https://arxiv.org/abs/2305.19847) |
| 1135 | PATCorrect: Non-Autoregressive Phoneme-Augmented Transformer for ASR Error Correction | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2302.05040-b31b1b.svg)](https://arxiv.org/abs/2302.05040) |
| 161 | Model-assisted Lexical Tone Evaluation of Three-Year-Old Chinese-Speaking Children by also Considering Segment Production | :heavy_minus_sign: | :heavy_minus_sign: |
| 1392 | Sentence Embedder Guided Utterance Encoder (SEGUE) for Spoken Language Understanding | [![GitHub](https://img.shields.io/github/stars/declare-lab/segue)](https://github.com/declare-lab/segue) | [![arXiv](https://img.shields.io/badge/arXiv-2305.12301-b31b1b.svg)](https://arxiv.org/abs/2305.12301) |
| 1582 | Joint Time and Frequency Transformer for Chinese Opera Classification | :heavy_minus_sign: | :heavy_minus_sign: |
| 116 | AdaMS: Deep Metric Learning with Adaptive Margin and Adaptive Scale for Acoustic Word Discrimination | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2210.14564-b31b1b.svg)](https://arxiv.org/abs/2210.14564) |
| 2252 | Investigating Reproducibility at Interspeech Conferences: A Longitudinal and Comparative Perspective | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2306.10033-b31b1b.svg)](https://arxiv.org/abs/2306.10033) |
| 2250 | Combining Heterogeneous Structures for Event Causality Identification | :heavy_minus_sign: | :heavy_minus_sign: |
| 1208 | An Efficient Approach for the Automated Segmentation and Transcription of the People's Speech Corpus | :heavy_minus_sign: | :heavy_minus_sign: |
| 1425 | Diverse Feature Mapping and Fusion via Multitask Learning for Multilingual Speech Emotion Recognition | :heavy_minus_sign: | :heavy_minus_sign: |
| 903 | Take the Hint: Improving Arabic Diacritization with Partially-Diacritized Text | [![GitHub](https://img.shields.io/github/stars/apptek/ArabicDiacritizationInterspeech2023)](https://github.com/apptek/ArabicDiacritizationInterspeech2023) | [![arXiv](https://img.shields.io/badge/arXiv-2306.03557-b31b1b.svg)](https://arxiv.org/abs/2306.03557) |
| 466 | Low-Resource Cross-Lingual Adaptive Training for Nigerian Pidgin | [![GitHub](https://img.shields.io/github/stars/muhammed-saeed/CLaT)](https://github.com/muhammed-saeed/CLaT) | [![arXiv](https://img.shields.io/badge/arXiv-2307.00382-b31b1b.svg)](https://arxiv.org/abs/2307.00382) |
| 1878 | Efficient Adaptation of Spoken Language Understanding based on End-to-End Automatic Speech Recognition | :heavy_minus_sign: | :heavy_minus_sign: |
| 597 | PhonMatchNet: Phoneme-Guided Zero-Shot Keyword Spotting for User-Defined Keywords | [![GitHub](https://img.shields.io/github/stars/ncsoft/PhonMatchNet)](https://github.com/ncsoft/PhonMatchNet) | :heavy_minus_sign: |
| 69 | Mix before Align: Towards Zero-Shot Cross-Lingual Sentiment Analysis via Soft-Mix and Multi-View Learning | :heavy_minus_sign: | :heavy_minus_sign: |
| 170 | AlignAtt: using Attention-based Audio-Translation Alignments as a Guide for Simultaneous Speech Translation | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2305.11408-b31b1b.svg)](https://arxiv.org/abs/2305.11408) |
| 2225 | Incremental Blockwise Beam Search for Simultaneous Speech Translation with Controllable Quality-Latency Tradeoff | :heavy_minus_sign: | :heavy_minus_sign: |
| 1979 | Zambezi Voice: A Multilingual Speech Corpus for Zambian Languages | [![GitHub](https://img.shields.io/github/stars/unza-speech-lab/zambezi-voice)](https://github.com/unza-speech-lab/zambezi-voice) | [![arXiv](https://img.shields.io/badge/arXiv-2306.04428-b31b1b.svg)](https://arxiv.org/abs/2306.04428) |

### Speech, Voice, and Hearing Disorders

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

| :id: | **Title** | **Repo** | **Paper** |
|------|-----------|:--------:|:---------:|
| 2421 | Debiased Automatic Speech Recognition for Dysarthric Speech via Sample Reweighting with Sample Affinity Test | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2305.13108-b31b1b.svg)](https://arxiv.org/abs/2305.13108) |
| 2198 | Multimodal Locally Enhanced Transformer for Continuous Sign Language Recognition | :heavy_minus_sign: | :heavy_minus_sign: |
| 1759 | Towards Supporting an Early Diagnosis of Multiple Sclerosis using Vocal Features | :heavy_minus_sign: | :heavy_minus_sign: |
| 1891 | Whisper Features for Dysarthric Severity-Level Classification | :heavy_minus_sign: | :heavy_minus_sign: |
| 2191 | A New Benchmark of Aphasia Speech Recognition and Detection based on E-Branchformer and Multi-task Learning | [![GitHub](https://img.shields.io/github/stars/espnet/espnet)](https://github.com/espnet/espnet/tree/master/egs2/aphasiabank/asr1) [![GitHub](https://img.shields.io/github/stars/alibaba-damo-academy/FunASR)](https://github.com/alibaba-damo-academy/FunASR) | [![arXiv](https://img.shields.io/badge/arXiv-2305.13331-b31b1b.svg)](https://arxiv.org/abs/2305.13331) |
| 222 | Dysarthric Speech Recognition, Detection and Classification using Raw Phase and Magnitude Spectra | :heavy_minus_sign: | :heavy_minus_sign: |
| 2026 | A Stutter Seldom Comes Alone - Cross-Corpus Stuttering Detection as a Multi-label Problem | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2305.19255-b31b1b.svg)](https://arxiv.org/abs/2305.19255) |
| 1542 | Transfer Learning to Aid Dysarthria Severity Classification for Patients with Amyotrophic Lateral Sclerosis | :heavy_minus_sign: | :heavy_minus_sign: |
| 2203 | DuTa-VC: A Duration-aware Typical-to-Atypical Voice Conversion Approach with Diffusion Probabilistic Model | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2306.10588-b31b1b.svg)](https://arxiv.org/abs/2306.10588) |
| 201 | CNVVE: Dataset and Benchmark for Classifying Non-verbal Voice | [![GitHub](https://img.shields.io/github/stars/hedeshy/CNVVE)](https://github.com/hedeshy/CNVVE) | [![University of Southampton](https://img.shields.io/badge/soton-ac-015C84.svg)](https://eprints.soton.ac.uk/478344/) |
| 1541 | Arabic Dysarthric Speech Recognition using Adversarial and Signal-based Augmentation | [![GitHub](https://img.shields.io/github/stars/massabaali7/AR_Dysarthric)](https://github.com/massabaali7/AR_Dysarthric) | [![arXiv](https://img.shields.io/badge/arXiv-2306.04368-b31b1b.svg)](https://arxiv.org/abs/2306.04368) |
| 1887 | Weakly-Supervised Forced Alignment of Disfluent Speech using Phoneme-level Modeling | [![GitHub](https://img.shields.io/github/stars/zelaki/WSFA)](https://github.com/zelaki/WSFA) | [![arXiv](https://img.shields.io/badge/arXiv-2306.00996-b31b1b.svg)](https://arxiv.org/abs/2306.00996) |
| 1998 | Glottal Source Analysis of Voice Deficits in Basal Ganglia Dysfunction: Evidence from de novo Parkinson's Disease and Huntington's Disease | :heavy_minus_sign: | :heavy_minus_sign: |
| 2478 | An Analysis of Glottal Features of Chronic Kidney Disease Speech and its Application to CKD Detection | :heavy_minus_sign: | :heavy_minus_sign: |
| 983 | Weakly Supervised Glottis Segmentation in High-Speed Video Endoscopy using Bounding Box Labels | :heavy_minus_sign: | :heavy_minus_sign: |
| 1669 | Investigating the Dynamics of Hand and Lips in French Cued Speech using Attention Mechanisms and CTC-based Decoding | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2306.08290-b31b1b.svg)](https://arxiv.org/abs/2306.08290) |
| 670 | Hearing Loss Affects Emotion Perception in Older Adults: Evidence from a Prosody-Semantics Stroop Task | :heavy_minus_sign: | :heavy_minus_sign: |
| 554 | Cochlear-Implant Listeners Listening to Cochlear-Implant Simulated Speech | :heavy_minus_sign: | :heavy_minus_sign: |
| 2168 | Validation of a Task-Independent Cepstral Peak Prominence Measure with Voice Activity Detection | :heavy_minus_sign: | :heavy_minus_sign: |
| 1679 | Score-balanced Loss for Multi-aspect Pronunciation Assessment | [![GitHub](https://img.shields.io/github/stars/doheejin/SB_loss_PA)](https://github.com/doheejin/SB_loss_PA) | [![arXiv](https://img.shields.io/badge/arXiv-2305.16664-b31b1b.svg)](https://arxiv.org/abs/2305.16664) |
| 2108 | Federated Learning for Secure Development of AI Models for Parkinson's Disease Detection using Speech from Different Languages | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2305.11284-b31b1b.svg)](https://arxiv.org/abs/2305.11284) |
| 652 | F0inTFS: A Lightweight Periodicity Enhancement Strategy for Cochlear Implants | :heavy_minus_sign: | :heavy_minus_sign: |
| 1678 | Differentiating Acoustic and Physiological Features in Speech for Hypoxia Detection | [![GitHub](https://img.shields.io/github/stars/doheejin/SB_loss_PA)](https://github.com/doheejin/SB_loss_PA) | [![HAL Science](https://img.shields.io/badge/hal-science-04154914.svg)](https://hal.science/hal-04154914) |
| 786 | Mandarin Electrolaryngeal Speech Voice Conversion using Cross-Domain Features | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2306.06653-b31b1b.svg)](https://arxiv.org/abs/2306.06653) |
| 866 | Audio-Visual Mandarin Electrolaryngeal Speech Voice Conversion | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2306.06652-b31b1b.svg)](https://arxiv.org/abs/2306.06652) |
| 1744 | Which Aspects of Motor Speech Disorder are Captured by Mel Frequency Cepstral Coefficients? Evidence from the Change in STN-DBS Conditions in Parkinson's Disease | :heavy_minus_sign: | :heavy_minus_sign: |
| 1096 | Detecting Manifest Huntington's Disease using Vocal Data | :heavy_minus_sign: | :heavy_minus_sign: |
| 1623 | Exploring Multi-Task Learning and Data Augmentation in Dementia Detection with Self-Supervised Pre-trained Models | :heavy_minus_sign: | :heavy_minus_sign: |

### Spoken Term Detection and Voice Search

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

| :id: | **Title** | **Repo** | **Paper** |
|------|-----------|:--------:|:---------:|
| 478 | Matching Latent Encoding for Audio-Text based Keyword Spotting | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2306.05245-b31b1b.svg)](https://arxiv.org/abs/2306.05245) |
| 1215 | Self-Paced Pattern Augmentation for Spoken Term Detection in Zero-Resource | :heavy_minus_sign: | :heavy_minus_sign: |
| 2362 | On-Device Constrained Self-Supervised Speech Representation Learning for Keyword Spotting via Knowledge Distillation | :heavy_minus_sign: | [![Amazon Science](https://img.shields.io/badge/amazon-science-FE9901.svg)](https://www.amazon.science/publications/on-device-constrained-self-supervised-speech-representation-learning-for-keyword-spotting-via-knowledge-distillation) |
| 90 | Online Continual Learning in Keyword Spotting for Low-Resource Devices via Pooling High-Order Temporal Statistics | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/michieli23_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.12660-b31b1b.svg)](https://arxiv.org/abs/2307.12660) |
| 689 | Improving Small Footprint Few-Shot Keyword Spotting with Supervision on Auxiliary Data | :heavy_minus_sign: | :heavy_minus_sign: |
| 2222 | Robust Keyword Spotting for Noisy Environments by Leveraging Speech Enhancement and Speech Presence Probability | :heavy_minus_sign: | :heavy_minus_sign: |

### Models for Streaming ASR

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

| :id: | **Title** | **Repo** | **Paper** |
|------|-----------|:--------:|:---------:|
|  831 | Enhancing the Unified Streaming and Non-Streaming Model with Contrastive Learning | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2306.00755-b31b1b.svg)](https://arxiv.org/abs/2306.00755) |
| 1497 | ZeroPrompt: Streaming Acoustic Encoders are Zero-Shot Masked LMs | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2305.10649-b31b1b.svg)](https://arxiv.org/abs/2305.10649) |
| 361 | Improved Training for End-to-End Streaming Automatic Speech Recognition Model with Punctuation | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2306.01296-b31b1b.svg)](https://arxiv.org/abs/2306.01296) |
| 1129 | DCTX-Conformer: Dynamic Context Carry-over for Low Latency Unified Streaming and Non-Streaming Conformer | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2306.08175-b31b1b.svg)](https://arxiv.org/abs/2306.08175) |
| 1121 | Knowledge Distillation from Non-Streaming to Streaming ASR Encoder using Auxiliary Non-Streaming Layer | :heavy_minus_sign: | :heavy_minus_sign: |
| 884 | Adaptive Contextual Biasing for Transducer based Streaming Speech Recognition | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2306.00804-b31b1b.svg)](https://arxiv.org/abs/2306.00804) |

### Source Separation

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

| :id: | **Title** | **Repo** | **Paper** |
|------|-----------|:--------:|:---------:|
| 1753 | Audio-Visual Speech Separation in Noisy Environments with a Lightweight Iterative Model | [![GitHub](https://img.shields.io/github/stars/hmartelb/avlit)](https://github.com/hmartelb/avlit) | [![arXiv](https://img.shields.io/badge/arXiv-2306.00160-b31b1b.svg)](https://arxiv.org/abs/2306.00160) |
| 1389 | Remixing-based Unsupervised Source Separation from Scratch | :heavy_minus_sign: | :heavy_minus_sign: |
| 577 | CAPTDURE: Captioned Sound Dataset of Single Sources | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2305.17758-b31b1b.svg)](https://arxiv.org/abs/2305.17758) |
| 488 | Recursive Sound Source Separation with Deep Learning-based Beamforming for Unknown Number of Sources | :heavy_minus_sign: | :heavy_minus_sign: |
| 2537 | Multi-Channel Speech Separation with Cross-Attention and Beamforming | :heavy_minus_sign: | :heavy_minus_sign: |
| 185 | Background-Sound Controllable Voice Source Separation | :heavy_minus_sign: | :heavy_minus_sign: |

### Speech Perception

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

| :id: | **Title** | **Repo** | **Paper** |
|------|-----------|:--------:|:---------:|
| 1922 | A Neural Architecture for Selective Attention to Speech Features | :heavy_minus_sign: | :heavy_minus_sign: |
| 1122 | Quantifying Informational Masking due to Masker Intelligibility in Same-Talker Speech-in-Speech Perception | :heavy_minus_sign: | :heavy_minus_sign: |
| 1476 | On the Benefits of Self-Supervised Learned Speech Representations for Predicting Human Phonetic Misperceptions | :heavy_minus_sign: | :heavy_minus_sign: |
| 2154 | Predicting Perceptual Centers Located at Vowel Onset in German Speech using Long Short-Term Memory Networks | :heavy_minus_sign: | :heavy_minus_sign: |
| 63 | Exploring the Mutual Intelligibility Breakdown Caused by Sculpting Speech from a Competing Speech Signal | :heavy_minus_sign: | :heavy_minus_sign: |
| 2103 | Perception of Incomplete Voicing Neutralization of Obstruents in Tohoku Japanese | :heavy_minus_sign: | :heavy_minus_sign: |

### Phonetics and Phonology: Languages and Varieties

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

| :id: | **Title** | **Repo** | **Paper** |
|------|-----------|:--------:|:---------:|
| 1879 | The Emergence of Obstruent-Intrinsic f0 and VOT as Cues to the Fortis/Lenis Contrast in West Central Bavarian | :heavy_minus_sign: | :heavy_minus_sign: |
| 431 | 〈'〉 in Tsimane': A Preliminary Investigation | :heavy_minus_sign: | :heavy_minus_sign: |
| 2200 | Segmental Features of Brazilian (Santa Catarina) Hunsrik | :heavy_minus_sign: | :heavy_minus_sign: |
| 2337 | Opening or Closing? An Electroglottographic Analysis of Voiceless Coda Consonants in Australian English | :heavy_minus_sign: | :heavy_minus_sign: |
| 295 | Increasing Aspiration of Word-Medial Fortis Plosives in Swiss Standard German | :heavy_minus_sign: | :heavy_minus_sign: |
| 1456 | Lexical Stress and Velar Palatalization in Italian: A Spatio-Temporal Interaction | :heavy_minus_sign: | :heavy_minus_sign: |

### Speaker and Language Identification

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

| :id: | **Title** | **Repo** | **Paper** |
|------|-----------|:--------:|:---------:|
| 1989 | Vietnam-Celeb: A Large-Scale Dataset for Vietnamese Speaker Recognition | :heavy_minus_sign: | :heavy_minus_sign: |
| 2254 | What Can an Accent Identifier Learn? Probing Phonetic and Prosodic Information in a Wav2vec2-based Accent Identification Model | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://is23-2254.github.io/) | [![arXiv](https://img.shields.io/badge/arXiv-2306.06524-b31b1b.svg)](https://arxiv.org/abs/2306.06524) |
| 241 | The 2022 NIST Language Recognition Evaluation | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2302.14624-b31b1b.svg)](https://arxiv.org/abs/2302.14624) |
| 1725 | ACA-Net: Towards Lightweight Speaker Verification using Asymmetric Cross Attention | [![GitHub](https://img.shields.io/github/stars/Yip-Jia-Qi/ACA-Net)](https://github.com/Yip-Jia-Qi/ACA-Net) | [![arXiv](https://img.shields.io/badge/arXiv-2305.12121-b31b1b.svg)](https://arxiv.org/abs/2305.12121) |
| 402 | Branch-ECAPA-TDNN: A Parallel Branch Architecture to Capture Local and Global Features for Speaker Verification | :heavy_minus_sign: | :heavy_minus_sign: |
| 2052 | Speaker Verification Across Ages: Investigating Deep Speaker Embedding Sensitivity to Age Mismatch in Enrollment and Test Speech | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2306.07501-b31b1b.svg)](https://arxiv.org/abs/2306.07501)|
| 2569 | Wavelet Scattering Transform for Improving Generalization in Low-Resourced Spoken Language Identification | :heavy_minus_sign: | :heavy_minus_sign: |
| 1407 | A Parameter-Efficient Learning Approach to Arabic Dialect Identification with Pre-trained General Purpose Speech Model | [![GitHub](https://img.shields.io/github/stars/Srijith-rkr/KAUST-Whisper-Adapter)](https://github.com/Srijith-rkr/KAUST-Whisper-Adapter) | [![arXiv](https://img.shields.io/badge/arXiv-2305.11244-b31b1b.svg)](https://arxiv.org/abs/2305.11244)|
| 2272 | HABLA: A Dataset of Latin American Spanish Accents for Voice Anti-Spoofing | :heavy_minus_sign: | :heavy_minus_sign: |
| 1702 | Self-Supervised Learning Representation based Accent Recognition with Persistent Accent Memory | :heavy_minus_sign: | :heavy_minus_sign: |
| 800 | Extremely Low Bit Quantization for Mobile Speaker Verification Systems Under 1MB Memory | :heavy_minus_sign: | :heavy_minus_sign: |
| 1974 | Unsupervised Out-of-Distribution Dialect Detection with Mahalanobis Distance | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2302.10326-b31b1b.svg)](https://arxiv.org/abs/2302.10326) |
| 105 | Pyannote.Audio 2.1 Speaker Diarization Pipeline: Principle, Benchmark and Recipe | [![GitHub](https://img.shields.io/github/stars/pyannote/pyannote-audio)](https://github.com/pyannote/pyannote-audio) | [![Pdf](https://img.shields.io/badge/pdf-version-003B10.svg)](https://huggingface.co/bhuvanesh25/pyannote-diar-copy/resolve/main/technical_report_2.1.pdf) |
| 1524 | Model Compression for DNN-based Speaker Verification using Weight Quantization | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2210.17326-b31b1b.svg)](https://arxiv.org/abs/2210.17326) |
| 1354 | Multi-Resolution Approach to Identification of Spoken Languages and to Improve Overall Language Diarization System using Whisper Model | :heavy_minus_sign: | :heavy_minus_sign: |
| 125 | Improving Generalization Ability of Countermeasures for New Mismatch Scenario by Combining Multiple Advanced Regularization Terms | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2305.10940-b31b1b.svg)](https://arxiv.org/abs/2305.10940) |
| 849 | Dynamic Fully-Connected Layer for Large-Scale Speaker Verification | :heavy_minus_sign: | :heavy_minus_sign: |
| 1314 | Mutual Information-based Embedding Decoupling for Generalizable Speaker Verification | :heavy_minus_sign: | :heavy_minus_sign: |
| 1206 | TO-Rawnet: Improving RawNet with TCN and Orthogonal Regularization for Fake Audio Detection | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2305.13701-b31b1b.svg)](https://arxiv.org/abs/2305.13701) |
| 777 | ECAPA++: Fine-grained Deep Embedding Learning for TDNN based Speaker Verification | :heavy_minus_sign: | :heavy_minus_sign: |
| 100 | Fooling Speaker Identification Systems with Adversarial Background Music | :heavy_minus_sign: | :heavy_minus_sign: |
| 574 | Target Active Speaker Detection with Audio-Visual Cues | [![GitHub](https://img.shields.io/github/stars/Jiang-Yidi/TS-TalkNet)](https://github.com/Jiang-Yidi/TS-TalkNet) | [![arXiv](https://img.shields.io/badge/arXiv-2305.12831-b31b1b.svg)](https://arxiv.org/abs/2305.12831) |
| 2401 | Improving End-to-End Neural Diarization using Conversational Summary Representations | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2306.13863-b31b1b.svg)](https://arxiv.org/abs/2306.13863) |
| 2039 | Phase Perturbation Improves Channel Robustness for Speech Spoofing Countermeasures | [![GitHub](https://img.shields.io/github/stars/yongyizang/PhaseAntispoofing_INTERSPEECH)](https://github.com/yongyizang/PhaseAntispoofing_INTERSPEECH) | [![arXiv](https://img.shields.io/badge/arXiv-2306.03389-b31b1b.svg)](https://arxiv.org/abs/2306.03389) |
| 210 | Improving Training Datasets for Resource-constrained Speaker Recognition Neural Networks | :heavy_minus_sign: | :heavy_minus_sign: |
| 1498 | Instance-based Temporal Normalization for Speaker Verification | :heavy_minus_sign: | :heavy_minus_sign: |
| 881 | On the Robustness of Wav2Vec 2.0 based Speaker Recognition Systems | :heavy_minus_sign: | :heavy_minus_sign: |
| 697 | P-Vectors: A Parallel-coupled TDNN/Transformer Network for Speaker Verification | [![GitHub](https://img.shields.io/github/stars/xyw7/pvector)](https://github.com/xyw7/pvector) | [![arXiv](https://img.shields.io/badge/arXiv-2305.14778-b31b1b.svg)](https://arxiv.org/abs/2305.14778) |
| 844 | Reversible Neural Networks for Memory-Efficient Speaker Verification | :heavy_minus_sign: | :heavy_minus_sign: |
| 452 | Robust Training for Speaker Verification against Noisy Labels | [![GitHub](https://img.shields.io/github/stars/PunkMale/OR-Gate)](https://github.com/PunkMale/OR-Gate) | [![arXiv](https://img.shields.io/badge/arXiv-2211.12080-b31b1b.svg)](https://arxiv.org/abs/2211.12080) |
| 1404 | Self-Distillation into Self-Attention Heads for Improving Transformer-based End-to-End Neural Speaker Diarization | :heavy_minus_sign: | :heavy_minus_sign: |
| 1217 | Build a SRE Challenge System: Lessons from VoxSRC 2022 and CNSRC 2022 | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2211.00815-b31b1b.svg)](https://arxiv.org/abs/2211.00815) |
| 1648 | Describing the Phonetics in the Underlying Speech Attributes for Deep and Interpretable Speaker Recognition | [![GitHub](https://img.shields.io/github/stars/LIAvignon/BA-LR)](https://github.com/LIAvignon/BA-LR) | :heavy_minus_sign: |
| 1214 | Range-based Equal Error Rate for Spoof Localization | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2305.17739-b31b1b.svg)](https://arxiv.org/abs/2305.17739) |
| 1888 | Exploring the English Accent-Independent Features for Speech Emotion Recognition using Filter and Wrapper-based Methods for Feature Selection | :heavy_minus_sign: | :heavy_minus_sign: |
| 205 | Powerset Multi-Class Cross Entropy Loss for Neural Speaker Diarization | [![GitHub](https://img.shields.io/github/stars/FrenchKrab/IS2023-powerset-diarization)](https://github.com/FrenchKrab/IS2023-powerset-diarization) | :heavy_minus_sign: |
| 394 | A Method of Audio-Visual Person Verification by Mining Connections between Time Series | :heavy_minus_sign: | :heavy_minus_sign: |
| 1249 | Group GMM-ResNet for Detection of Synthetic Speech Attacks | :heavy_minus_sign: | :heavy_minus_sign: |
| 605 | One-Step Knowledge Distillation and Fine-Tuning in using Large Pre-trained Self-Supervised Learning Models for Speaker Verification | [![GitHub](https://img.shields.io/github/stars/jungwoo4021/OS-KDFT)](https://github.com/jungwoo4021/OS-KDFT) | [![arXiv](https://img.shields.io/badge/arXiv-2305.17394-b31b1b.svg)](https://arxiv.org/abs/2305.17394) |
| 409 | Defense Against Adversarial Attacks on Audio DeepFake Detection | [![GitHub](https://img.shields.io/github/stars/piotrkawa/audio-deepfake-adversarial-attacks)](https://github.com/piotrkawa/audio-deepfake-adversarial-attacks) | [![arXiv](https://img.shields.io/badge/arXiv-2212.14597-b31b1b.svg)](https://arxiv.org/abs/2212.14597) |
| 1820 | A Conformer-based Classifier for Variable-Length Utterance Processing in Anti-Spoofing | :heavy_minus_sign: | :heavy_minus_sign: |
| 1557 | Conformer-based Language Embedding with Self-Knowledge Distillation for Spoken Language Identification | :heavy_minus_sign: | :heavy_minus_sign: |
| 2419 | CommonAccent: Exploring Large Acoustic Pre-trained Models for Accent Classification based on Common Voice | :heavy_minus_sign: | [![ResearchGate](https://img.shields.io/badge/Research-Gate-D7E7F5.svg)](https://www.researchgate.net/publication/371136510_CommonAccent_Exploring_Large_Acoustic_Pretrained_Models_for_Accent_Classification_Based_on_Common_Voice) |
| 266 | From Adaptive Score Normalization to Adaptive Data Normalization for Speaker Verification Systems | :heavy_minus_sign: | :heavy_minus_sign: |
| 1513 | CAM++: A Fast and Efficient Network for Speaker Verification using Context-aware Masking | [![GitHub](https://img.shields.io/github/stars/alibaba-damo-academy/3D-Speaker)](https://github.com/alibaba-damo-academy/3D-Speaker) | [![arXiv](https://img.shields.io/badge/arXiv-2303.00332-b31b1b.svg)](https://arxiv.org/abs/2303.00332) |
| 1928 | North Sámi Dialect Identification with Self-Supervised Speech Models | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2305.11864-b31b1b.svg)](https://arxiv.org/abs/2305.11864) |
| 2289 | Encoder-Decoder Multimodal Speaker Change Detection | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2306.00680-b31b1b.svg)](https://arxiv.org/abs/2306.00680) |
| 1603 | Disentangled Representation Learning for Multilingual Speaker Recognition | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://mm.kaist.ac.kr/projects/voxceleb1-b/) | [![arXiv](https://img.shields.io/badge/arXiv-2211.00437-b31b1b.svg)](https://arxiv.org/abs/2211.00437) |
| 2310 | A Compact End-to-End Model with Local and Global Context for Spoken Language Identification | [![GitHub](https://img.shields.io/github/stars/NVIDIA/NeMo)](https://github.com/NVIDIA/NeMo) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/jia23b_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2210.15781-b31b1b.svg)](https://arxiv.org/abs/2210.15781) |
| 1005 | On the Robustness of Arabic Speech Dialect Identification | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2306.03789-b31b1b.svg)](https://arxiv.org/abs/2306.03789) |
| 927 | Adaptive Neural Network Quantization for Lightweight Speaker Verification | :heavy_minus_sign: | :heavy_minus_sign: |
| 1205 | Adversarial Diffusion Probability Model For Cross-Domain Speaker Verification Integrating Contrastive Loss | :heavy_minus_sign: | :heavy_minus_sign: |
| 1554 | Chinese Dialect Recognition based on Transfer Learning | :heavy_minus_sign: | :heavy_minus_sign: |
| 270 | Spoofing Attacker also Benefits from Self-Supervised Pretrained Model | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2305.15518-b31b1b.svg)](https://arxiv.org/abs/2305.15518) |
| 854 | Label aware Speech Representation Learning for Language Identification | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2306.04374-b31b1b.svg)](https://arxiv.org/abs/2306.04374) |
| 1761 | Exploring the Impact of Back-end Network on Wav2vec 2.0 for Dialect Identification | :heavy_minus_sign: | :heavy_minus_sign: |
| 453 | Improving Speaker Verification with Self-pretrained Transformer Models | [![GitHub](https://img.shields.io/github/stars/JunyiPeng00/Interspeech23_SelfPretraining)](https://github.com/JunyiPeng00/Interspeech23_SelfPretraining) | [![arXiv](https://img.shields.io/badge/arXiv-2305.10517-b31b1b.svg)](https://arxiv.org/abs/2305.10517) |
| 155 | Description and Analysis of the KPT system for NIST Language Recognition Evaluation 2022 | :heavy_minus_sign: | :heavy_minus_sign: |
| 372 | Handling the Alignment for Wake Word Detection: A Comparison Between Alignment-based, Alignment-Free and Hybrid Approaches | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2302.08950-b31b1b.svg)](https://arxiv.org/abs/2302.08950) |

### Speech Synthesis and Voice Conversion

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

| :id: | **Title** | **Repo** | **Paper** |
|------|-----------|:--------:|:---------:|
| 2336 | Mitigating the Exposure Bias in Sentence-Level Grapheme-to-Phoneme (G2P) Transduction | :heavy_minus_sign: | :heavy_minus_sign: |
| 160 | Streaming Parrotron for On-Device Speech-to-Speech Conversion | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2210.13761-b31b1b.svg)](https://arxiv.org/abs/2210.13761) |
| 2407 | Exploiting Emotion Information in Speaker Embeddings for Expressive Text-to-Speech | :heavy_minus_sign: | :heavy_minus_sign: |
| 2518 | E2E-S2S-VC: End-to-End Sequence-to-Sequence Voice Conversion | :heavy_minus_sign: | :heavy_minus_sign: |
| 2403 | DC CoMix TTS: An End-to-End Expressive TTS with Discrete Code Collaborated with Mixer | [![GitHub](https://img.shields.io/github/stars/lakahaga/dc-comix-tts)](https://github.com/lakahaga/dc-comix-tts) | [![arXiv](https://img.shields.io/badge/arXiv-2305.19567-b31b1b.svg)](https://arxiv.org/abs/2305.19567) |
| 419 | Voice Conversion with Just Nearest Neighbors | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://bshall.github.io/knn-vc/) <br /> [![GitHub](https://img.shields.io/github/stars/bshall/knn-vc)](https://github.com/bshall/knn-vc) | [![arXiv](https://img.shields.io/badge/arXiv-2305.18975-b31b1b.svg)](https://arxiv.org/abs/2305.18975) |
| 1193 | CFVC: Conditional Filtering for Controllable Voice Conversion | :heavy_minus_sign: | :heavy_minus_sign: |
| 1157 | DualVC: Dual-mode Voice Conversion using Intra-Model Knowledge Distillation and Hybrid Predictive Coding | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://dualvc.github.io/) | [![arXiv](https://img.shields.io/badge/arXiv-2305.12425-b31b1b.svg)](https://arxiv.org/abs/2305.12425) |
| 39 | Attention-based Interactive Disentangling Network for Instance-Level Emotional Voice Conversion | :heavy_minus_sign: | :heavy_minus_sign: |
| 836 | ALO-VC: Any-to-Any Low-Latency One-Shot Voice Conversion | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://bohan7.github.io/ALO-VC-demo/) | [![arXiv](https://img.shields.io/badge/arXiv-2306.01100-b31b1b.svg)](https://arxiv.org/abs/2306.01100) |
| 1978 | Evaluating and Reducing the Distance between Synthetic and Real Speech Distributions | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2211.16049-b31b1b.svg)](https://arxiv.org/abs/2211.16049) |
| 2202 | Decoupling Segmental and Prosodic cues of Non-Native Speech through Vector Quantization | :heavy_minus_sign: | :heavy_minus_sign: |
| 2383 | VC-T: Streaming Voice Conversion based on Neural Transducer | :heavy_minus_sign: | :heavy_minus_sign: |
| 191 | Emo-StarGAN: A Semi-Supervised Any-to-Many Non-Parallel Emotion Preserving Voice Conversion | :heavy_minus_sign: | :heavy_minus_sign: |
| 1788 | ControlVC: Zero-Shot Voice Conversion with Time-Varying Controls on Pitch and Speed | [![GitHub](https://img.shields.io/github/stars/MelissaChen15/control-vc)](https://github.com/MelissaChen15/control-vc) | [![arXiv](https://img.shields.io/badge/arXiv-2209.11866-b31b1b.svg)](https://arxiv.org/abs/2209.11866) |
| 1356 | Reverberation-Controllable Voice Conversion using Reverberation Time Estimator | :heavy_minus_sign: | :heavy_minus_sign: |
| 2558 | Cross-Utterance Conditioned Coherent Speech Editing | :heavy_minus_sign: | :heavy_minus_sign: |

### Speech and Language in Health: from Remote Monitoring to Medical Conversations

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

| :id: | **Title** | **Repo** | **Paper** |
|------|-----------|:--------:|:---------:|
| 2287 | An Automatic Multimodal Approach to Analyze Linguistic and Acoustic Cues on Parkinson's Disease Patients | :heavy_minus_sign: | :heavy_minus_sign: |
| 1332 | Personalization for Robust Voice Pathology Detection in Sound Waves | [![GitHub](https://img.shields.io/github/stars/Fsoft-AIC/RoPADet)](https://github.com/Fsoft-AIC/RoPADet) | :heavy_minus_sign: |
| 2249 | Integrated and Enhanced Pipeline System to Support Spoken Language Analytics for Screening Neurocognitive Disorders | :heavy_minus_sign: | :heavy_minus_sign: |
| 1990 | Capturing Mismatch between Textual and Acoustic Emotion Expressions for Mood Identification in Bipolar Disorder | :heavy_minus_sign: | [![Pdf](https://img.shields.io/badge/pdf-version-003B10.svg)](http://emp.engin.umich.edu/wp-content/uploads/sites/67/2023/06/Capturing_Mismatch_between_Textual_and_Acoustic_Emotion_Expressions_for_Mood_Identification_in_Bipolar_Disorder-3.pdf) |
| 296 | FTA-Net: A Frequency and Time Attention Network for Speech Depression Detection | :heavy_minus_sign: | :heavy_minus_sign: |
| 1709 | Bayesian Networks for the Robust and Unbiased Prediction of Depression and its Symptoms Utilizing Speech and Multimodal Data | :heavy_minus_sign: | [![Pdf](https://img.shields.io/badge/pdf-version-003B10.svg)](https://readpaper.com/paper/4770892998779076609) |
| 1263 | Hyper-Parameter Adaptation of Conformer ASR Systems for Elderly and Dysarthric Speech Recognition | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2306.15265-b31b1b.svg)](https://arxiv.org/abs/2306.15265) |
| 1721 | Classifying Depression Symptom Severity: Assessment of Speech Representations in Personalized and Generalized Machine Learning Models | :heavy_minus_sign: | :heavy_minus_sign: |
| 1946 | Active Learning for Abnormal Lung Sound Data Curation and Detection in Asthma | :heavy_minus_sign: | :heavy_minus_sign: |
| 2079 | Automatic Assessment of Alzheimer's across Three Languages using Speech and Language Features | :heavy_minus_sign: | :heavy_minus_sign: |
| 301 | On-the-Fly Feature based Rapid Speaker Adaptation for Dysarthric and Elderly Speech Recognition | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2203.14593-b31b1b.svg)](https://arxiv.org/abs/2203.14593) |
| 1722 | Relationship between LTAS-based Spectral Moments and Acoustic Parameters of Hypokinetic Dysarthria in Parkinson's Disease | :heavy_minus_sign: | :heavy_minus_sign: |
| 963 | Respiratory Distress Estimation in Human-Robot Interaction Scenario | :heavy_minus_sign: | :heavy_minus_sign: |
| 1771 | Prediction of the Gender-based Violence Victim Condition using Speech: What do Machine Learning Models rely on? | :heavy_minus_sign: | :heavy_minus_sign: |
| 1916 | Whisper Encoder features for Infant Cry Classification | :heavy_minus_sign: | :heavy_minus_sign: |
| 1997 | Classifying Dementia in the Presence of Depression: A Cross-Corpus Study | :heavy_minus_sign: | :heavy_minus_sign: |
| 297 | Exploiting Cross-Domain and Cross-Lingual Ultrasound Tongue Imaging Features for Elderly and Dysarthric Speech Recognition | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2206.07327-b31b1b.svg)](https://arxiv.org/abs/2206.07327) |
| 464 | Multi-Class Detection of Pathological Speech with Latent Features: How does It Perform on Unseen Data? | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2210.15336-b31b1b.svg)](https://arxiv.org/abs/2210.15336) |
| 2002 | Responsiveness, Sensitivity and Clinical Utility of Timing-Related Speech Biomarkers for Remote Monitoring of ALS Disease Progression | :heavy_minus_sign: | [![Pdf](https://img.shields.io/badge/pdf-version-003B10.svg)](https://drive.google.com/file/d/1-W1buG48sqQnd9uld2c-z-Ls0NSS-bNn/view) |
| 322 | Use of Speech Impairment Severity for Dysarthric Speech Recognition | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2305.10659-b31b1b.svg)](https://arxiv.org/abs/2305.10659) |
| 721 | MMLung: Moving Closer to Practical Lung Health Estimation using Smartphones | :heavy_minus_sign: | [![Pdf](https://img.shields.io/badge/pdf-version-003B10.svg)](https://mobiuk.org/2023/abstract/S5_P1_Mosuily_MMLung.pdf) |
| 913 | Investigating the Utility of Synthetic Data for Doctor-Patient Conversation Summarization | :heavy_minus_sign: | :heavy_minus_sign: |
| 2101 | Non-Uniform Speaker Disentanglement for Depression Detection from Raw Speech Signals | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2306.01861-b31b1b.svg)](https://arxiv.org/abs/2306.01861) |
| 753 | PoCaPNet: A Novel Approach for Surgical Phase Recognition using Speech and X-Ray Images | [![GitHub](https://img.shields.io/github/stars/kubicndmr/PoCaPNet)](https://github.com/kubicndmr/PoCaPNet) | [![arXiv](https://img.shields.io/badge/arXiv-2305.15993-b31b1b.svg)](https://arxiv.org/abs/2305.15993) |
| 2100 | Combining Multiple Multimodal Speech Features into an Interpretable Index Score for Capturing Disease Progression in Amyotrophic Lateral Sclerosis | :heavy_minus_sign: | [![Pdf](https://img.shields.io/badge/pdf-version-003B10.svg)](https://drive.google.com/file/d/1FfcQifvTL9bTD7SBU7y_A3APgX8N_Vd0/view) |
| 1438 | The MASCFLICHT Corpus: Face Mask Type and Coverage Area Recognition from Speech | :heavy_minus_sign: | :heavy_minus_sign: |
| 1435 | Towards Reference Speech Characterization for Health Applications | :heavy_minus_sign: | :heavy_minus_sign: |
| 2146 | Automatic Classification of Hypokinetic and Hyperkinetic Dysarthria based on GMM-Supervectors | :heavy_minus_sign: | :heavy_minus_sign: |
| 947 | Towards Robust Paralinguistic Assessment for Real-World Mobile Health (mHealth) Monitoring: an Initial Study of Reverberation Effects on Speech | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2305.12514-b31b1b.svg)](https://arxiv.org/abs/2305.12514) |

### Novel Transformer Models for ASR

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

| :id: | **Title** | **Repo** | **Paper** |
|------|-----------|:--------:|:---------:|
| 2228 | Conmer: Streaming Conformer without Self-Attention for Interactive Voice Assistants | :heavy_minus_sign: | [![Amazon Science](https://img.shields.io/badge/amazon-science-FE9901.svg)](https://www.amazon.science/publications/conmer-streaming-conformer-without-self-attention-for-interactive-voice-assistants) |
| 1255 | Intra-Ensemble: A New Method for Combining Intermediate Outputs in Transformer-based Automatic Speech Recognition | :heavy_minus_sign: | :heavy_minus_sign: |
| 1194 | A Comparative Study on E-Branchformer vs Conformer in Speech Recognition, Translation, and Understanding Tasks | [![GitHub](https://img.shields.io/github/stars/espnet/espnet)](https://github.com/espnet/espnet) [![GitHub](https://img.shields.io/github/stars/alibaba-damo-academy/FunASR)](https://github.com/alibaba-damo-academy/FunASR) | [![arXiv](https://img.shields.io/badge/arXiv-2305.11073-b31b1b.svg)](https://arxiv.org/abs/2305.11073) |
| 1611 | HyperConformer: Multi-head HyperMixer for Efficient Speech Recognition | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2305.18281-b31b1b.svg)](https://arxiv.org/abs/2305.18281) |
| 893 | Memory-Augmented Conformer for Improved End-To-End Long-form ASR | :heavy_minus_sign: | :heavy_minus_sign: |
| 552 | Towards Effective and Compact Contextual Representation for Conformer Transducer Speech Recognition Systems | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2306.13307-b31b1b.svg)](https://arxiv.org/abs/2306.13307) |

### Speaker Recognition

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

| :id: | **Title** | **Repo** | **Paper** |
|------|-----------|:--------:|:---------:|
| 1294 | An Enhanced Res2Net with Local and Global Feature Fusion for Speaker Verification | [![GitHub](https://img.shields.io/github/stars/alibaba-damo-academy/3D-Speaker)](https://github.com/alibaba-damo-academy/3D-Speaker) | [![arXiv](https://img.shields.io/badge/arXiv-2305.12838-b31b1b.svg)](https://arxiv.org/abs/2305.12838) |
| 1286 | A Study on Visualization of Voiceprint Feature | :heavy_minus_sign: | :heavy_minus_sign: |
| 1083 | VoxTube: A Multilingual Speaker Recognition Dataset | :heavy_minus_sign: | :heavy_minus_sign: |
| 1298 | Visualizing Data Augmentation in Deep Speaker Recognition | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2305.16070-b31b1b.svg)](https://arxiv.org/abs/2305.16070) |
| 1565 | Ordered and Binary Speaker Embedding | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2305.16043-b31b1b.svg)](https://arxiv.org/abs/2305.16043) |
| 2031 | Self-FiLM: Conditioning GANs with Self-Supervised Representations for Bandwidth Extension based Speaker Recognition | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2303.03657-b31b1b.svg)](https://arxiv.org/abs/2303.03657) |
| 1202 | Curriculum Learning for Self-Supervised Speaker Verification | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2203.14525-b31b1b.svg)](https://arxiv.org/abs/2203.14525) |
| 1558 | Introducing Self-Supervised Phonetic Information for Text-Independent Speaker Verification | :heavy_minus_sign: | :heavy_minus_sign: |
| 1379 | A Teacher-Student Approach for Extracting Informative Speaker Embeddings from Speech Mixtures | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2306.00634-b31b1b.svg)](https://arxiv.org/abs/2306.00634) |
| 1479 | Experimenting with Additive Margins for Contrastive Self-Supervised Speaker Verification | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2306.03664-b31b1b.svg)](https://arxiv.org/abs/2306.03664) |

### Cross-lingual and Multilingual ASR

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

| :id: | **Title** | **Repo** | **Paper** |
|------|-----------|:--------:|:---------:|
| 1630 | Fast and Efficient Multilingual Self-Supervised Pre-training for Low-Resource Speech Recognition | :heavy_minus_sign: | :heavy_minus_sign: |
| 1338 | UniSplice: Universal Cross-Lingual Data Splicing for Low-Resource ASR | :heavy_minus_sign: | :heavy_minus_sign: |
| 772 | Allophant: Cross-Lingual Phoneme Recognition with Articulatory Attributes | [![GitHub](https://img.shields.io/github/stars/kgnlp/allophant)](https://github.com/kgnlp/allophant) [![GitHub](https://img.shields.io/github/stars/Aariciah/allophoible)](https://github.com/Aariciah/allophoible) | [![arXiv](https://img.shields.io/badge/arXiv-2306.04306-b31b1b.svg)](https://arxiv.org/abs/2306.04306) |
| 97 | Phonetic-assisted Multi-Target Units Modeling for Improving Conformer-Transducer ASR System | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2211.01571-b31b1b.svg)](https://arxiv.org/abs/2211.01571) |
| 1061 | Comparison of Multilingual Self-Supervised and Weakly-Supervised Speech Pre-training for Adaptation to Unseen Languages | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2305.12606-b31b1b.svg)](https://arxiv.org/abs/2305.12606) |
| 1444 | DistilXLSR: A Light Weight Cross-Lingual Speech Representation Model | [![GitHub](https://img.shields.io/github/stars/backspacetg/distilXLSR)](https://github.com/backspacetg/distilXLSR) | [![arXiv](https://img.shields.io/badge/arXiv-2306.01303-b31b1b.svg)](https://arxiv.org/abs/2306.01303) |

### Voice Conversion

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

| :id: | **Title** | **Repo** | **Paper** |
|------|-----------|:--------:|:---------:|
| 251 | Emotional Voice Conversion with Semi-Supervised Generative Modeling |:heavy_minus_sign:| :heavy_minus_sign: |
| 817 | Diff-HierVC: Diffusion-based Hierarchical Voice Conversion with Robust Pitch Generation and Masked Prior for Zero-Shot Speaker Adaptation | [![GitHub](https://img.shields.io/github/stars/hayeong0/Diff-HierVC)](https://github.com/hayeong0/Diff-HierVC) | :heavy_minus_sign: |
| 215 | S2CD-VC: Self-Heuristic Speaker Content Disentanglement for Any-to-Any Voice Conversion | :heavy_minus_sign: | :heavy_minus_sign: |
| 1508 | Flow-VAE VC: End-to-End Flow Framework with Contrastive Loss for Zero-Shot Voice Conversion | :heavy_minus_sign: | :heavy_minus_sign: |
| 1602 | Automatic Speech Disentanglement for Voice Conversion using Rank Module and Speech Augmentation | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://hhhuazi.github.io) | [![arXiv](https://img.shields.io/badge/arXiv-2306.12259-b31b1b.svg)](https://arxiv.org/abs/2306.12259) |
| 2298 | End-to-End Zero-Shot Voice Conversion with Location-Variable Convolutions | [![GitHub](https://img.shields.io/github/stars/wonjune-kang/lvc-vc)](https://github.com/wonjune-kang/lvc-vc) | [![arXiv](https://img.shields.io/badge/arXiv-2205.09784-b31b1b.svg)](https://arxiv.org/abs/2205.09784) |

### Pathological Speech Analysis

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

| :id: | **Title** | **Repo** | **Paper** |
|------|-----------|:--------:|:---------:|
| 2093 | Multimodal Assessment of Bulbar Amyotrophic Lateral Sclerosis (ALS) using a Novel Remote Speech Assessment App | :heavy_minus_sign: | :heavy_minus_sign: |
| 2181 | On the use of High Frequency Information for Voice Pathology Classification | :heavy_minus_sign: | :heavy_minus_sign: |
| 1784 | Do Phonatory Features Display Robustness to Characterize Parkinsonian Speech Across Corpora? | :heavy_minus_sign: | :heavy_minus_sign: |
| 2531 | Severity Classification of Parkinson's Disease from Speech using Single Frequency Filtering-based Features | :heavy_minus_sign: | :heavy_minus_sign: |
| 1915 | Comparison of Acoustic Measures of Dysphonia in Parkinson's Disease and Huntington's Disease: Effect of Sex and Speaking Task | :heavy_minus_sign: | :heavy_minus_sign: |
| 1734 | Alzheimer Disease Classification through ASR-based Transcriptions: Exploring the Impact of Punctuation and Pauses | [![GitHub](https://img.shields.io/github/stars/LuciaGomZa/INTERSPEECH2023_AlzheimersDisease)](https://github.com/LuciaGomZa/INTERSPEECH2023_AlzheimersDisease) | [![arXiv](https://img.shields.io/badge/arXiv-2306.03443-b31b1b.svg)](https://arxiv.org/abs/2306.03443) |
| 1574 | A Pipeline to Evaluate the Effects of Noise on Machine Learning Detection of Laryngeal Cancer | [![GitHub](https://img.shields.io/github/stars/mary-paterson/Interspeech2023-EvaluationPipeline)](https://github.com/mary-paterson/Interspeech2023-EvaluationPipeline) | :heavy_minus_sign: |
| 2474 | ReCLR: Reference-Enhanced Contrastive Learning of Audio Representation for Depression Detection | :heavy_minus_sign: | :heavy_minus_sign: |
| 234 | Automated Multiple Sclerosis Screening based on Encoded Speech Representations | :heavy_minus_sign: | :heavy_minus_sign: |
| 1934 | Cross-Lingual Features for Alzheimer's Dementia Detection from Speech | :heavy_minus_sign: | :heavy_minus_sign: |
| 1653 | Careful Whisper - Leveraging Advances in Automatic Speech Recognition for Robust and Interpretable Aphasia Subtype Classification | :heavy_minus_sign: | :heavy_minus_sign: |
| 1868 | Behavioral Analysis of Pathological Speaker Embeddings of Patients During Oncological Treatment of Oral Cancer | :heavy_minus_sign: | :heavy_minus_sign: |

### Multimodal Speech Emotion Recognition

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

| :id: | **Title** | **Repo** | **Paper** |
|------|-----------|:--------:|:---------:|
| 1832 | LanSER: Language-Model Supported Speech Emotion Recognition | :heavy_minus_sign: | :heavy_minus_sign: |
| 463 | Fine-tuned RoBERTa Model with a CNN-LSTM Network for Conversational Emotion Recognition | :heavy_minus_sign: | :heavy_minus_sign: |
| 1591 | Emotion Label Encoding using Word Embeddings for Speech Emotion Recognition | :heavy_minus_sign: | :heavy_minus_sign: |
| 2444 | Discrimination of the Different Intents Carried by the Same Text through Integrating Multimodal Information | :heavy_minus_sign: | :heavy_minus_sign: |
| 510 | Meta-Domain Adversarial Contrastive Learning for Alleviating Individual Bias in Self-Sentiment Predictions | :heavy_minus_sign: | :heavy_minus_sign: |
| 413 | SWRR: Feature Map Classifier based on Sliding Window Attention and High-Response Feature Reuse for Multimodal Emotion Recognition | :heavy_minus_sign: | :heavy_minus_sign: |

### Phonetics, Phonology, and Prosody

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

| :id: | **Title** | **Repo** | **Paper** |
|------|-----------|:--------:|:---------:|
| 1443 | Effects of Meter, Genre and Experience on Pausing, Lengthening and Prosodic Phrasing in German Poetry Reading | :heavy_minus_sign: | :heavy_minus_sign: |
| 1142 | Comparing First Spectral Moment of Australian English /s/ between Straight and Gay Voices using Three Analysis Window Sizes | :heavy_minus_sign: | :heavy_minus_sign: |
| 2584 | Universal Automatic Phonetic Transcription into the International Phonetic Alphabet | [![GitHub](https://img.shields.io/github/stars/ctaguchi/multipa)](https://github.com/ctaguchi/multipa) | :heavy_minus_sign: |
| 2134 | Voice Twins: Discovering Extremely Similar-Sounding, Unrelated Speakers | :heavy_minus_sign: | :heavy_minus_sign: |
| 1042 | Filling the Population Statistics Gap: Swiss German Reference Data on F0 and Speech Tempo for Forensic Contexts | :heavy_minus_sign: | :heavy_minus_sign: |
| 1619 | Investigating the Syntax-Discourse Interface in the Phonetic Implementation of Discourse Markers | :heavy_minus_sign: | :heavy_minus_sign: |
| 2214 | Evaluation of a Forensic Automatic Speaker Recognition System with Emotional Speech Recordings | :heavy_minus_sign: | :heavy_minus_sign: |
| 1052 | An Outlier Analysis of Vowel Formants from a Corpus Phonetics Pipeline | [![GitHub](https://img.shields.io/github/stars/emilyahn/outliers)](https://github.com/emilyahn/outliers) | [![Pdf](https://img.shields.io/badge/pdf-version-003B10.svg)](https://www.eleanorchodroff.com/articles/AhnLevowWrightChodroff_Outliers_Interspeech_2023.pdf) |
| 340 | The Hidden Dance of Phonemes and Visage: Unveiling the Enigmatic Link between Phonemes and Facial Features | :heavy_minus_sign: | :heavy_minus_sign: |
| 1880 | Beatboxing Kick Drum Kinematics | :heavy_minus_sign: | :heavy_minus_sign: |
| 536 | Effects of Hearing Loss and Amplification on Mandarin Consonant Perception | :heavy_minus_sign: | :heavy_minus_sign: |
| 2020 | An Acoustic Analysis of Fricative Variation in Three Accents of English | :heavy_minus_sign: | :heavy_minus_sign: |
| 109 | Acoustic Cues to Stress Perception in Spanish – a Mismatch Negativity Study | :heavy_minus_sign: | :heavy_minus_sign: |
| 976 | Bulgarian Unstressed Vowel Reduction: Received Views vs Corpus Findings | :heavy_minus_sign: | :heavy_minus_sign: |
| 1764 | An Investigation of Indian Native Language Phonemic Influences on L2 English Pronunciations | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2212.09284-b31b1b.svg)](https://arxiv.org/abs/2212.09284) |
| 498 | Identifying Stable Sections for Formant Frequency Extraction of French Nasal Vowels based on Difference Thresholds | :heavy_minus_sign: | :heavy_minus_sign: |
| 1903 | Evaluation of Delexicalization Methods for Research on Emotional Speech | :heavy_minus_sign: | :heavy_minus_sign: |
| 1772 | Nonbinary American English Speakers Encode Gender in Vowel Acoustics | :heavy_minus_sign: | :heavy_minus_sign: |
| 44 | Coarticulation of Sibe Vowels and Dorsal Fricatives in Spontaneous Speech: An Acoustic Study | :heavy_minus_sign: | :heavy_minus_sign: |
| 1013 | Using Speech Synthesis to Explain Automatic Speaker Recognition: A New Application of Synthetic Speech | :heavy_minus_sign: | :heavy_minus_sign: |
| 2534 | Same F0, Different Tones: A Multidimensional Investigation of Zhangzhou Tones | :heavy_minus_sign: | :heavy_minus_sign: |
| 1985 | Discovering Phonetic Feature Event Patterns in Transformer Embeddings | :heavy_minus_sign: | :heavy_minus_sign: |
| 2204 | A System for Generating Voice Source Signals that Implements the Transformed LF-Model Parameter Control | :heavy_minus_sign: | :heavy_minus_sign: |
| 2352 | Speaker-Independent Speech Inversion for Estimation of Nasalance | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2306.00203-b31b1b.svg)](https://arxiv.org/abs/2306.00203) |
| 1359 | Effects of Tonal Coarticulation and Prosodic Positions on Tonal Contours of Low Rising Tones: In the Case of Xiamen Dialect | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2306.02251-b31b1b.svg)](https://arxiv.org/abs/2306.02251) |
| 2187 | Durational and Non-Durational Correlates of Lexical and Derived Geminates in Arabic | :heavy_minus_sign: | :heavy_minus_sign: |
| 68 | Mapping Phonemes to Acoustic Symbols and Codes using Synchrony in Speech Modulation Vectors Estimated by the Travellingwave Filter Bank | :heavy_minus_sign: | :heavy_minus_sign: |
| 1480 | Rhythmic Characteristics of L2 German Speech by Advanced Chinese Learners | :heavy_minus_sign: | :heavy_minus_sign: |
| 1538 | (Dis)agreement and Preference Structure are Reflected in Matching Along Distinct Acoustic-Prosodic Features | :heavy_minus_sign: | :heavy_minus_sign: |
| 995 | Vowel Reduction by Greek-Speaking Children: The Effect of Stress and Word Length | :heavy_minus_sign: | :heavy_minus_sign: |
| 1822 | Pitch Distributions in a Very Large Corpus of Spontaneous Finnish Speech | :heavy_minus_sign: | :heavy_minus_sign: |
| 828 | Speech Enhancement Patterns in Human-Robot Interaction: A Cross-Linguistic Perspective | :heavy_minus_sign: | :heavy_minus_sign: |

### Speech Coding: Privacy

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

| :id: | **Title** | **Repo** | **Paper** |
|------|-----------|:--------:|:---------:|
| 1026 | Masking Kernel for Learning Energy-Efficient Representations for Speaker Recognition and Mobile Health | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2302.04161-b31b1b.svg)](https://arxiv.org/abs/2302.04161) |
| 727 | eSTImate: A Real-Time Speech Transmission Index Estimator with Speech Enhancement Auxiliary Task using Self-Attention Feature Pyramid Network | :heavy_minus_sign: | :heavy_minus_sign: |
| 815 | Efficient Encoder-Decoder and Dual-Path Conformer for Comprehensive Feature Learning in Speech Enhancement | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2306.05861-b31b1b.svg)](https://arxiv.org/abs/2306.05861) |
| 2138 | Privacy-Preserving Representation Learning for Speech Understanding | :heavy_minus_sign: | :heavy_minus_sign: |
| 448 | Vocoder Drift in X-Vector–based Speaker Anonymization | [![GitHub](https://img.shields.io/github/stars/eurecom-asp/vocoder-drift)](https://github.com/eurecom-asp/vocoder-drift) | [![arXiv](https://img.shields.io/badge/arXiv-2306.02892-b31b1b.svg)](https://arxiv.org/abs/2306.02892) |
| 703 | Malafide: A Novel Adversarial Convolutive Noise Attack Against Deepfake and Spoofing Detection Systems | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2306.07655-b31b1b.svg)](https://arxiv.org/abs/2306.07655) |

### Analysis of Neural Speech Representations

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

| :id: | **Title** | **Repo** | **Paper** |
|------|-----------|:--------:|:---------:|
| 1087 | Speech Self-Supervised Representation Bench-Marking: Are We Doing it Right? | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://github.com/salah-zaiem/speechbrain-2/tree/develop/recipes/SSL_benchmark) | [![arXiv](https://img.shields.io/badge/arXiv-2306.00452-b31b1b.svg)](https://arxiv.org/abs/2306.00452) |
| 383 | An Extension of Disentanglement Metrics and its Application to Voice | :heavy_minus_sign: | :heavy_minus_sign: |
| 2131 | An Information-Theoretic Analysis of Self-Supervised Discrete Representations of Speech | [![GitHub](https://img.shields.io/github/stars/uds-lsv/phone2unit)](https://github.com/uds-lsv/phone2unit) | [![arXiv](https://img.shields.io/badge/arXiv-2306.02405-b31b1b.svg)](https://arxiv.org/abs/2306.02405) |
| 1823 | SpeechGLUE: How Well Can Self-Supervised Speech Models Capture Linguistic Knowledge? | [![GitHub](https://img.shields.io/github/stars/ashi-ta/speechGLUE)](https://github.com/ashi-ta/speechGLUE) | [![arXiv](https://img.shields.io/badge/arXiv-2306.08374-b31b1b.svg)](https://arxiv.org/abs/2306.08374) |
| 1418 | Comparison of GIF- and SSL-based Features in Pathological Voice Detection | :heavy_minus_sign: | :heavy_minus_sign: |
| 1617 | What is Learnt by the LEArnable Front-end (LEAF)? Adapting Per-Channel Energy Normalisation (PCEN) to Noisy Conditions | :heavy_minus_sign:| :heavy_minus_sign: |

### End-to-end ASR

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

| :id: | **Title** | **Repo** | **Paper** |
|------|-----------|:--------:|:---------:|
| 1640 | End-to-End Joint Target and Non-Target Speakers ASR | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/masumura23_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.02273-b31b1b.svg)](https://arxiv.org/abs/2306.02273) |
| 144 | Improving Frame-Level Classifier for Word Timings with Non-peaky CTC in End-to-End Automatic Speech Recognition | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/chen23c_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.07949-b31b1b.svg)](https://arxiv.org/abs/2306.07949) |
| 564 | Joint Autoregressive Modeling of End-to-End Multi-Talker Overlapped Speech Recognition and Utterance-Level Timestamp Prediction | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/makishima23_interspeech.pdf) |
| 101 | Dual-Path Style Learning for End-to-End Noise-Robust Speech Recognition | [![GitHub](https://img.shields.io/github/stars/YUCHEN005/DPSL-ASR)](https://github.com/YUCHEN005/DPSL-ASR) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/hu23_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2203.14838-b31b1b.svg)](https://arxiv.org/abs/2203.14838) |
| 142 | Multi-Pass Training and Cross-Information Fusion for Low-Resource End-to-End Accented Speech Recognition | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/wang23_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.11309-b31b1b.svg)](https://arxiv.org/abs/2306.11309) |
| 906 | Text-Only Domain Adaptation for End-to-End ASR using Integrated Text-to-Mel-Spectrogram Generator | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/bataev23_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2302.14036-b31b1b.svg)](https://arxiv.org/abs/2302.14036) |

### Spoken Language Understanding, Summarization, and Information Retrieval

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

| :id: | **Title** | **Repo** | **Paper** |
|------|-----------|:--------:|:---------:|
| 461 | Leveraging Pretrained ASR Encoders for Effective and Efficient End-to-End Speech Intent Classification and Slot Filling | :heavy_minus_sign: |:heavy_minus_sign: |
| 277 | Relation-based Counterfactual Data Augmentation and Contrastive Learning for Robustifying Natural Language Inference Models | :heavy_minus_sign: |:heavy_minus_sign: |
| 1307 | Transfer Learning from Pre-trained Language Models Improves End-to-End Speech Summarization | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2306.04233-b31b1b.svg)](https://arxiv.org/abs/2306.04233) |
| 1136 | Audio Retrieval with WavText5K and CLAP Training | [![GitHub](https://img.shields.io/github/stars/microsoft/WavText5K)](https://github.com/microsoft/WavText5K) | [![arXiv](https://img.shields.io/badge/arXiv-2209.14275-b31b1b.svg)](https://arxiv.org/abs/2209.14275) |
| 242 | Sequence-Level Knowledge Distillation for Class-Incremental End-to-End Spoken Language Understanding | [![GitHub](https://img.shields.io/github/stars/umbertocappellazzo/slurp-seqkd)](https://github.com/umbertocappellazzo/slurp-seqkd) | [![arXiv](https://img.shields.io/badge/arXiv-2305.13899-b31b1b.svg)](https://arxiv.org/abs/2305.13899) |
| 1652 | Contrastive Disentangled Learning for Memory-Augmented Transformer | :heavy_minus_sign: | :heavy_minus_sign: |

### Invariant and Robust Pre-trained Acoustic Models

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

| :id: | **Title** | **Repo** | **Paper** |
|------|-----------|:--------:|:---------:|
| 438 | ProsAudit, a Prosodic Benchmark for Self-Supervised Speech Models | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2302.12057-b31b1b.svg)](https://arxiv.org/abs/2302.12057) |
| 871 | Self-Supervised Predictive Coding Models Encode Speaker and Phonetic Information in Orthogonal Subspaces | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2305.12464-b31b1b.svg)](https://arxiv.org/abs/2305.12464) |
| 1862 | Evaluating Context-Invariance in Unsupervised Speech Representations | [![GitHub](https://img.shields.io/github/stars/perceptimatic/irpam2023)](https://github.com/perceptimatic/irpam2023) | [![arXiv](https://img.shields.io/badge/arXiv-2210.15775-b31b1b.svg)](https://arxiv.org/abs/2210.15775) |
| 1390 | CoBERT: Self-Supervised Speech Representation Learning through Code Representation Learning | [![GitHub](https://img.shields.io/github/stars/mct10/CoBERT)](https://github.com/mct10/CoBERT) | [![arXiv](https://img.shields.io/badge/arXiv-2210.04062-b31b1b.svg)](https://arxiv.org/abs/2210.04062) |
| 847 | Self-Supervised Fine-tuning for Improved Content Representations by Speaker-Invariant Clustering | [![GitHub](https://img.shields.io/github/stars/vectominist/spin)](https://github.com/vectominist/spin) | [![arXiv](https://img.shields.io/badge/arXiv-2305.11072-b31b1b.svg)](https://arxiv.org/abs/2305.11072) |
| 359 | Self-Supervised Acoustic Word Embedding Learning via Correspondence Transformer Encoder | :heavy_minus_sign: | :heavy_minus_sign: |

### Speech Synthesis: Representation Learning

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

| :id: | **Title** | **Repo** | **Paper** |
|------|-----------|:--------:|:---------:|
| 1571 | Adversarial Learning of Intermediate Acoustic Feature for End-to-End Lightweight Text-to-Speech | :heavy_minus_sign: | :heavy_minus_sign: |
| 2313 | Adapter-based Extension of Multi-Speaker Text-To-Speech Model for New Speakers | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2211.00585-b31b1b.svg)](https://arxiv.org/abs/2211.00585) |
| 2574 | SALTTS: Leveraging Self-Supervised Speech Representations for improved Text-to-Speech Synthesis | :heavy_minus_sign: | :heavy_minus_sign: |
| 2326 | UnitSpeech: Speaker-Adaptive Speech Synthesis with Untranscribed Data | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://unitspeech.github.io) <br /> [![GitHub](https://img.shields.io/github/stars/gmltmd789/UnitSpeech)](https://github.com/gmltmd789/UnitSpeech) | [![arXiv](https://img.shields.io/badge/arXiv-2306.16083-b31b1b.svg)](https://arxiv.org/abs/2306.16083) |
| 677 | LightVoc: an Upsampling-Free GAN Vocoder based on Conformer and Inverse Short-time Fourier Transform | :heavy_minus_sign: | :heavy_minus_sign: |
| 1095 | ChatGPT-EDSS: Empathetic Dialogue Speech Synthesis Trained from ChatGPT-derived Context Word Embeddings | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://sarulab-speech.github.io/demo_ChatGPT_EDSS/) | [![arXiv](https://img.shields.io/badge/arXiv-2305.13724-b31b1b.svg)](https://arxiv.org/abs/2305.13724) |

### Speech Perception, Production, and Acquisition

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

| :id: | **Title** | **Repo** | **Paper** |
|------|-----------|:--------:|:---------:|
| 1330 | Human Transcription Quality Improvement | :heavy_minus_sign: | [![Amazon Science](https://img.shields.io/badge/amazon-science-FE9901.svg)](https://www.amazon.science/publications/human-transcription-quality-improvement) |
| 1604 | The Effect of Masking Noise on Listeners' Spectral Tilt Preferences | :heavy_minus_sign: | :heavy_minus_sign: |
| 1967 | The Effect of Whistled Vowels on Whistled Word Categorization for Naive Listeners | :heavy_minus_sign: | :heavy_minus_sign: |
| 1481 | Automatic Deep Neural Network-based Segmental Pronunciation Error Detection of L2 English Speech (L1 Bengali) | :heavy_minus_sign: | :heavy_minus_sign: |
| 1662 | The Effect of Stress on Mandarin Tonal Perception in Continuous Speech for Spanish-Speaking Learners | :heavy_minus_sign: | :heavy_minus_sign: |
| 1918 | Combining Acoustic and Aerodynamic Data Collection: A Perceptual Evaluation of Acoustic Distortions | :heavy_minus_sign: | :heavy_minus_sign: |
| 953 | Estimating Virtual Targets for Lingual Stop Consonants using General Tau Theory | :heavy_minus_sign: | :heavy_minus_sign: |
| 1931 | Using Random Forests to Classify Language as a Function of Syllable Timing in Two Groups: Children with Cochlear Implants and with Normal Hearing | :heavy_minus_sign: | :heavy_minus_sign: |
| 2256 | An Improved End-to-End Audio-Visual Speech Recognition Model | :heavy_minus_sign: | :heavy_minus_sign: |
| 1954 | What Influences the Foreign Accent Strength? Phonological and Grammatical Errors in the Perception of Accentedness | :heavy_minus_sign: | :heavy_minus_sign: |
| 2077 | Investigating the Perception Production Link through Perceptual Adaptation and Phonetic Convergence | :heavy_minus_sign: | :heavy_minus_sign: |
| 1385 | Emotion Prompting for Speech Emotion Recognition | :heavy_minus_sign: | :heavy_minus_sign: |
| 1196 | Speech-in-Speech Recognition is Modulated by Familiarity to Dialect | :heavy_minus_sign: | :heavy_minus_sign: |
| 673 | BASEN: Time-Domain Brain-Assisted Speech Enhancement Network with Convolutional Cross Attention in Multi-Talker Conditions | [![GitHub](https://img.shields.io/github/stars/jzhangU/Basen)](https://github.com/jzhangU/Basen) | [![arXiv](https://img.shields.io/badge/arXiv-2305.09994-b31b1b.svg)](https://arxiv.org/abs/2305.09994) |
| 2046 | Are Retroflex-to-Dental Sibilant Substitutions in Polish Children's Speech an Example of a Covert Contrast? A Preliminary Acoustic Study | :heavy_minus_sign: | :heavy_minus_sign: |
| 1123 | First Language Effects on Second Language Perception: Evidence from English Low-Vowel Nasal Sequences Perceived by L1 Mandarin Chinese Listeners | :heavy_minus_sign: | :heavy_minus_sign: |
| 2247 | Motor Control Similarity between Speakers Saying "a Souk" using Inverse Atlas Tongue Modeling | :heavy_minus_sign: | :heavy_minus_sign: |
| 910 | Assessing Phrase Break of ESL Speech with Pre-trained Language Models and Large Language Models | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2306.04980-b31b1b.svg)](https://arxiv.org/abs/2306.04980) |
| 317 | A Relationship between Vocal Fold Vibration and Droplet Production | :heavy_minus_sign: | :heavy_minus_sign: |
| 803 | Audio, Visual and Audiovisual Intelligibility of Vowels Produced in Noise | :heavy_minus_sign: | :heavy_minus_sign: |
| 593 | Computational Modeling of Auditory Brainstem Responses Derived from Modified Speech | :heavy_minus_sign: | :heavy_minus_sign: |
| 1732 | Leveraging Label Information for Multimodal Emotion Recognition | :heavy_minus_sign: | :heavy_minus_sign: |
| 1465 | Improving End-to-End Modeling for Mandarin-English Code-Switching using Lightweight Switch-Routing Mixture-of-Experts | :heavy_minus_sign: | :heavy_minus_sign: |
| 1803 | Frequency Patterns of Individual Speaker Characteristics at Higher and Lower Spectral Ranges | :heavy_minus_sign: | :heavy_minus_sign: |
| 1818 | Adaptation to Predictive Prosodic cues in Non-Native Standard Dialect | :heavy_minus_sign: | :heavy_minus_sign: |
| 1007 | Head Movements in Two- and Four-Person Inter-Active Conversational Tasks in Noisy and Moderately Reverberant Conditions | :heavy_minus_sign: | :heavy_minus_sign: |
| 334 | Second Language Identification of Vietnamese Tones by Native Mandarin Learners | :heavy_minus_sign: | :heavy_minus_sign: |
| 203 | Nasal Vowel Production and Grammatical Processing in French-Speaking Children with Cochlear Implants and Normal-Hearing Peers | :heavy_minus_sign: | :heavy_minus_sign: |
| 412 | Emotion Classification with EEG Responses Evoked by Emotional Prosody of Speech | :heavy_minus_sign: |:heavy_minus_sign: |
| 145 | L2-Mandarin Regional Accent Variability During Mandarin Tone-Word Training Facilitates English listeners' Subsequent tone Categorizations | :heavy_minus_sign: | :heavy_minus_sign: |
| 1680 | HumanDiffusion: Diffusion Model using Perceptual Gradients | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2306.12169-b31b1b.svg)](https://arxiv.org/abs/2306.12169) |
| 2087 | Queer Events, Relationships, and Sports: Does Topic Influence Speakers' Acoustic Expression of Sexual Orientation? | :heavy_minus_sign: | :heavy_minus_sign: |
| 172 | Optimal Control of Speech with Context-Dependent Articulatory Targets | :heavy_minus_sign: | :heavy_minus_sign: |

### Acoustic Model Adaptation for ASR

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

| :id: | **Title** | **Repo** | **Paper** |
|------|-----------|:--------:|:---------:|
| 583 | Factorised Speaker-Environment Adaptive Training of Conformer Speech Recognition Systems | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2306.14608-b31b1b.svg)](https://arxiv.org/abs/2306.14608) |
| 1349 | Text Only Domain Adaptation with Phoneme Guided Data Splicing for End-to-End Speech Recognition | [![GitHub](https://img.shields.io/github/stars/NVIDIA/NeMo)](https://github.com/NVIDIA/NeMo/) | [![arXiv](https://img.shields.io/badge/arXiv-2302.14036-b31b1b.svg)](https://arxiv.org/abs/2302.14036) |
 | 327 | Towards Cross-Lingual Cross-Age Adaptation for Low-Resource Elderly Speech Emotion Recognition | [![GitHub](https://img.shields.io/github/stars/hltchkust/elderly_ser)](https://github.com/hltchkust/elderly_ser) | [![arXiv](https://img.shields.io/badge/arXiv-2306.14517-b31b1b.svg)](https://arxiv.org/abs/2306.14517) |
 | 2215 | Modular Domain Adaptation for Conformer-based Streaming ASR | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2305.13408-b31b1b.svg)](https://arxiv.org/abs/2305.13408) |
 | 2192 | Don't Stop Self-Supervision: Accent Adaptation of Speech Representations via Residual Adapters | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2307.00453-b31b1b.svg)](https://arxiv.org/abs/2307.00453) |
| 1282 | SGEM: Test-Time Adaptation for Automatic Speech Recognition via Sequential-Level Generalized Entropy Minimization | [![GitHub](https://img.shields.io/github/stars/drumpt/SGEM)](https://github.com/drumpt/SGEM/) | [![arXiv](https://img.shields.io/badge/arXiv-2306.01981-b31b1b.svg)](https://arxiv.org/abs/2306.01981) |

### Speech Synthesis: Expressivity

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

| :id: | **Title** | **Repo** | **Paper** |
|------|-----------|:--------:|:---------:|
| 858 | Controllable Generation of Artificial Speaker Embeddings through Discovery of Principal Directions | [![GitHub](https://img.shields.io/github/stars/DigitalPhonetics/IMS-Toucan)](https://github.com/DigitalPhonetics/IMS-Toucan) | :heavy_minus_sign: |
| 2242 | Dual Audio Encoders based Mandarin Prosodic Boundary Prediction by using Multi-Granularity Prosodic Representations | :heavy_minus_sign: | :heavy_minus_sign: |
| 645 | NoreSpeech: Knowledge Distillation based Conditional Diffusion Model for Noise-robust Expressive TTS | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](http://anonymousdemo.fun/) | [![arXiv](https://img.shields.io/badge/arXiv-2211.02448-b31b1b.svg)](https://arxiv.org/abs/2211.02448) |
| 782 | MaskedSpeech: Context-aware Speech Synthesis with Masking Strategy | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://speech11.github.io/MaskedSpeech/) | [![arXiv](https://img.shields.io/badge/arXiv-2211.06170-b31b1b.svg)](https://arxiv.org/abs/2211.06170) |
| 2469 | Narrator or Character: Voice Modulation in an Expressive Multi-Speaker TTS | :heavy_minus_sign: | :heavy_minus_sign: |
| 843 | CASEIN: Cascading Explicit and Implicit Control for Fine-grained Emotion Intensity Regulation | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2307.00020-b31b1b.svg)](https://arxiv.org/abs/2307.00020) |
| 1405 | Semi-Supervised Learning for Continuous Emotional Intensity Controllable Speech Synthesis with Disentangled Representations | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2211.06160-b31b1b.svg)](https://arxiv.org/abs/2211.06160) |
| 1905 | Expresso: A Benchmark and Analysis of Discrete Expressive Speech Resynthesis | :heavy_minus_sign: | :heavy_minus_sign: |
| 1460 | ComedicSpeech: Adaptive Text to Speech For Stand-up Comedy in Low-Resource Scenario | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://xh621.github.io/stand-up-comedy-demo/) | [![arXiv](https://img.shields.io/badge/arXiv-2305.12200-b31b1b.svg)](https://arxiv.org/abs/2305.12200) |
| 1552 | Neural Speech Synthesis with Enriched Phrase Boundaries | :heavy_minus_sign: | :heavy_minus_sign: |
| 437 | Cross-Lingual Prosody Transfer for Expressive Machine Dubbing | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2306.11658-b31b1b.svg)](https://arxiv.org/abs/2306.11658) |
| 2178 | Synthesis after a couple PINTs: Investigating the Role of Pause-Internal Phonetic Particles in Speech Synthesis and Perception | [![GitHub](https://img.shields.io/github/stars/MikeyElmers/paper_interspeech23)](https://github.com/MikeyElmers/paper_interspeech23) | :heavy_minus_sign: |
| 433 | Accentor: An Explicit Lexical Stress Model for TTS Systems | :heavy_minus_sign: | [![Pdf](https://img.shields.io/badge/pdf-version-003B10.svg)](http://lml.bas.bg/~stoyan/interspeech2023.pdf) |
| 1032 | A Neural TTS System with Parallel Prosody Transfer from Unseen Speakers | :heavy_minus_sign: | :heavy_minus_sign: |
| 715 | Diverse and Expressive Speech Prosody Prediction with Denoising Diffusion Probabilistic Model | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://diffvar.github.io/DDPM-prosody-predictor/) | [![arXiv](https://img.shields.io/badge/arXiv-2305.16749-b31b1b.svg)](https://arxiv.org/abs/2305.16749) |
| 289 | Prosody Modeling with 3D Visual Information for Expressive Video Dubbing | :heavy_minus_sign: | :heavy_minus_sign: |
| 1528 | LightClone: Speaker-Guided Parallel Subnet Selection for Few-Shot Voice Cloning | :heavy_minus_sign: | :heavy_minus_sign: |
| 1671 | EE-TTS: Emphatic Expressive TTS with Linguistic Information | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://expressive-emphatic-ttsdemo.github.io/) | [![arXiv](https://img.shields.io/badge/arXiv-2305.12107-b31b1b.svg)](https://arxiv.org/abs/2305.12107) |
| 1673 | Stochastic Pitch Prediction Improves the Diversity and Naturalness of Speech in Glow-TTS | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2305.17724-b31b1b.svg)](https://arxiv.org/abs/2305.17724) |
| 122 | ContextSpeech: Expressive and Efficient Text-to-Speech for Paragraph Reading | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://contextspeech.github.io/demo/) | [![arXiv](https://img.shields.io/badge/arXiv-2307.00782-b31b1b.svg)](https://arxiv.org/abs/2307.00782) |
| 1779 | PromptStyle: Controllable Style Transfer for Text-to-Speech with Natural Language Descriptions | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://promptstyle.github.io/PromptStyle) | [![arXiv](https://img.shields.io/badge/arXiv-2305.19522-b31b1b.svg)](https://arxiv.org/abs/2305.19522)
| 1639 | Creating Personalized Synthetic Voices from Post-Glossectomy Speech with Guided Diffusion Models | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://diffcorrect.github.io/) | [![arXiv](https://img.shields.io/badge/arXiv-2305.17436-b31b1b.svg)](https://arxiv.org/abs/2305.17436) |
| 2453 | A Generative Framework for Conversational Laughter: Its "Language Model" and Laughter Sound Synthesis | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2306.03465-b31b1b.svg)](https://arxiv.org/abs/2306.03465) |
| 1754 | Towards Spontaneous Style Modeling with Semi-Supervised Pre-training for Conversational Text-to-Speech Synthesis | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://thuhcsi.github.io/interspeech2023-spontaneousTTS/) | :heavy_minus_sign: |
| 2072 | Beyond Style: Synthesizing Speech with Pragmatic Functions | :heavy_minus_sign: | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://www.speech.kth.se/tts-demos/beyond_style/) |
| 965 | eCat: An End-to-End Model for Multi-Speaker TTS & Many-to-Many Fine-Grained Prosody Transfer | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2306.11327-b31b1b.svg)](https://arxiv.org/abs/2306.11327) |

### Multi-modal Systems

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

| :id: | **Title** | **Repo** | **Paper** |
|------|-----------|:--------:|:---------:|
| 1146 | BeAts: Bengali Speech Acts Recognition using Multimodal Attention Fusion | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://soumitri2001.github.io/BeAts) | [![arXiv](https://img.shields.io/badge/arXiv-2306.02680-b31b1b.svg)](https://arxiv.org/abs/2306.02680) |
| 370 | Improving the Gap in Visual Speech Recognition Between Normal and Silent Speech based on Metric Learning | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2305.14203-b31b1b.svg)](https://arxiv.org/abs/2305.14203) |
| 989 | Whistle-to-Text: Automatic Recognition of the Silbo Gomero Whistled Language | :heavy_minus_sign: | :heavy_minus_sign: |
| 663 | A Novel Interpretable and Generalizable Re-Synchronization Model for Cued Speech based on a Multi-Cuer Corpus | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2306.02596-b31b1b.svg)](https://arxiv.org/abs/2306.02596) |
| 668 | Visually Grounded Few-Shot Word Acquisition with Fewer Shots | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2305.15937-b31b1b.svg)](https://arxiv.org/abs/2305.15937) |
| 183 | JAMFN: Joint Attention Multi-Scale Fusion Network for Depression Detection | :heavy_minus_sign: | :heavy_minus_sign: |

### Question Answering from Speech

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

| :id: | **Title** | **Repo** | **Paper** |
|------|-----------|:--------:|:---------:|
| 1485 | Prompt Guided Copy Mechanism for Conversational Question Answering | :heavy_minus_sign: | :heavy_minus_sign: |
| 1240 | Composing Spoken Hints for Follow-on Question Suggestion in Voice Assistants | :heavy_minus_sign: | :heavy_minus_sign: |
| 1391 | On Monotonic Aggregation for Open-Domain QA | [![GitHub](https://img.shields.io/github/stars/YeonseokJeong/Judge-Specialist)](https://github.com/YeonseokJeong/Judge-Specialist) | :heavy_minus_sign: |
| 2240 | Question-Context Alignment and Answer-Context Dependencies for Effective Answer Sentence Selection | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2306.02196-b31b1b.svg)](https://arxiv.org/abs/2306.02196) |
| 1606 | Multi-Scale Attention for Audio Question Answering | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2305.17993-b31b1b.svg)](https://arxiv.org/abs/2305.17993) |
| 539 | Enhancing Visual Question Answering via Deconstructing Questions and Explicating Answers | :heavy_minus_sign: | :heavy_minus_sign: |

### Multi-talker Methods in Speech Processing

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

| :id: | **Title** | **Repo** | **Paper** |
|------|-----------|:--------:|:---------:|
| 1749 | SEF-Net: Speaker Embedding Free Target Spekaer Extraction Network | :heavy_minus_sign: | :heavy_minus_sign: |
| 1530 | Overlap aware Continuous Speech Separation without Permutation Invariant Training Linfeng | :heavy_minus_sign: | :heavy_minus_sign: |
| 1952 | Cascaded Encoders for Fine-Tuning ASR Models on Overlapped Speech | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2306.16398-b31b1b.svg)](https://arxiv.org/abs/2306.16398) |
| 2069 | TokenSplit: using Discrete Speech Representations for Direct, Refined, and Transcript-Conditioned Speech Separation and Recognition | :heavy_minus_sign: | :heavy_minus_sign: |
| 1422 | Unified Modeling of Multi-Talker Overlapped Speech Recognition and Diarization with a Sidecar Separator | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2305.16263-b31b1b.svg)](https://arxiv.org/abs/2305.16263) |
| 2098 | Time-Domain Transformer-based Audiovisual Speaker Separation | :heavy_minus_sign: | :heavy_minus_sign: |
| 628 | Multi-Stream Extension of Variational Bayesian HMM Clustering (MS-VBx) for Combined End-to-End and Vector Clustering-based Diarization | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2305.13580-b31b1b.svg)](https://arxiv.org/abs/2305.13580) |
| 1502 | Unsupervised Adaptation with Quality-aware Masking to Improve Target-Speaker Voice Activity Detection for Speaker Diarization | :heavy_minus_sign: | :heavy_minus_sign: |
| 1521 | BA-SOT: Boundary-aware Serialized Output Training for Multi-Talker ASR | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2305.13716-b31b1b.svg)](https://arxiv.org/abs/2305.13716) |
| 1172 | Improving Label Assignments Learning by Dynamic Sample Dropout Combined with Layer-wise Optimization in Speech Separation | :heavy_minus_sign: | :heavy_minus_sign: |
| 975 | Joint Compensation of Multi-Talker Noise and Reverberation for Speech Enhancement with Cochlear Implants using One or More Microphones | :heavy_minus_sign: | :heavy_minus_sign: |
| 494 | Speaker Diarization for ASR Output with T-vectors: A Sequence Classification Approach | :heavy_minus_sign: | :heavy_minus_sign: |
| 42 | GPU-accelerated Guided Source Separation for Meeting Transcription | [![GitHub](https://img.shields.io/github/stars/desh2608/gss)](https://github.com/desh2608/gss) | [![arXiv](https://img.shields.io/badge/arXiv-2212.05271-b31b1b.svg)](https://arxiv.org/abs/2212.05271) |
| 1280 | Weakly-Supervised Speech Pre-training: A Case Study on Target Speech Recognition | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://github.com/Emrys365/fairseq/tree/wavlm/examples/tshubert) | [![arXiv](https://img.shields.io/badge/arXiv-2305.16286-b31b1b.svg)](https://arxiv.org/abs/2305.16286) |
| 2076 | Directional Speech Recognition for Speaker Disambiguation and Cross-talk Suppression | :heavy_minus_sign: | :heavy_minus_sign: |
| 1815 | Mixture Encoder for Joint Speech Separation and Recognition | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2306.12173-b31b1b.svg)](https://arxiv.org/abs/2306.12173) |

### Sociophonetics

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

| :id: | **Title** | **Repo** | **Paper** |
|------|-----------|:--------:|:---------:|
| 206 | Aberystwyth English Pre-Aspiration in Apparent Time |:heavy_minus_sign: | :heavy_minus_sign: |
| 1154 | Speech Entrainment in Chinese Story-Style Talk Shows: The Interaction Between Gender and Role | :heavy_minus_sign: | :heavy_minus_sign: |
| 1414 | Sociodemographic and Attitudinal Effects on Dialect Speakers' Articulation of the Standard Language: Evidence from German-Speaking Switzerland | :heavy_minus_sign: | :heavy_minus_sign: |
| 1704 | Vowel Normalisation in Latent Space for Sociolinguistics |:heavy_minus_sign: | :heavy_minus_sign: |

### Speaker and Language Diarization

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

| :id: | **Title** | **Repo** | **Paper** |
|------|-----------|:--------:|:---------:|
| 1228 | Attention-based Encoder-Decoder Network for End-to-End Neural Speaker Diarization with Target Speaker Attractor | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2305.10704-b31b1b.svg)](https://arxiv.org/abs/2305.10704) |
| 1447 | Robust Self Supervised Speech Embeddings for Child-Adult Classification in Interactions involving Children with Autism | :heavy_minus_sign: | :heavy_minus_sign: |
| 2367 | The DISPLACE Challenge 2023 - DIarization of SPeaker and LAnguage in Conversational Environments | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://displace2023.github.io/) | [![arXiv](https://img.shields.io/badge/arXiv-2303.00830-b31b1b.svg)](https://arxiv.org/abs/2303.00830) |
| 1982 | Lexical Speaker Error Correction: Leveraging Language Models for Speaker Diarization Error Correction | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2306.09313-b31b1b.svg)](https://arxiv.org/abs/2306.09313) |
| 1839 | The SpeeD-ZevoTech Submission at DISPLACE 2023 | :heavy_minus_sign: | :heavy_minus_sign: |
| 656 | End-to-End Neural Speaker Diarization with Absolute Speaker Loss | :heavy_minus_sign: | :heavy_minus_sign: |

### Anti-Spoofing for Speaker Verification

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

| :id: | **Title** | **Repo** | **Paper** |
|------|-----------|:--------:|:---------:|
| 1402 | Towards Single Integrated Spoofing-aware Speaker Verification Embeddings | [![GitHub](https://img.shields.io/github/stars/sasv-challenge/ASVSpoof5-SASVBaseline)](https://github.com/sasv-challenge/ASVSpoof5-SASVBaseline) | [![arXiv](https://img.shields.io/badge/arXiv-2305.19051-b31b1b.svg)](https://arxiv.org/abs/2305.19051) |
| 1352 | Pseudo-Siamese Network based Timbre-Reserved Black-Box Adversarial Attack in Speaker Identification | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2305.19020-b31b1b.svg)](https://arxiv.org/abs/2305.19020) |
| 2335 | Betray Oneself: A Novel Audio DeepFake Detection Model via Mono-to-Stereo Conversion | [![GitHub](https://img.shields.io/github/stars/ttslr/M2S-ADD)](https://github.com/ttslr/M2S-ADD) | [![arXiv](https://img.shields.io/badge/arXiv-2305.16353-b31b1b.svg)](https://arxiv.org/abs/2305.16353) |
| 1166 | Robust Audio Anti-Spoofing Countermeasure with Joint Training of Front-end and Back-end and Models | :heavy_minus_sign: | :heavy_minus_sign: |
| 1537 | Improved DeepFake Detection using Whisper Features | [![GitHub](https://img.shields.io/github/stars/piotrkawa/deepfake-whisper-features)](https://github.com/piotrkawa/deepfake-whisper-features) | [![arXiv](https://img.shields.io/badge/arXiv-2306.01428-b31b1b.svg)](https://arxiv.org/abs/2306.01428) |
| 371 | DoubleDeceiver: Deceiving the Speaker Verification System Protected by Spoofing Countermeasures | :heavy_minus_sign: | :heavy_minus_sign: |

### Speech Coding: Intelligibility

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

| :id: | **Title** | **Repo** | **Paper** |
|------|-----------|:--------:|:---------:|
| 2209 | On Training a Neural Residual Acoustic echo Suppressor for Improved ASR | :heavy_minus_sign: | :heavy_minus_sign: |
| 1429 | Extending DNN-based Multiplicative Masking to Deep Subband Filtering for Improved Dereverberation | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://jmlemercier.github.io/2023/05/30/interspeech2023.html) | [![arXiv](https://img.shields.io/badge/arXiv-2303.00529-b31b1b.svg)](https://arxiv.org/abs/2303.00529) |
| 378 | UnSE: Unsupervised Speech Enhancement using Optimal Transport | :heavy_minus_sign: | :heavy_minus_sign: |
| 1130 | MC-SpEx: Towards Effective Speaker Extraction with Multi-Scale Interfusion and Conditional Speaker Modulation | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://rookiejunchen.github.io/MC-SpEx_demo/) | [![arXiv](https://img.shields.io/badge/arXiv-2306.16250-b31b1b.svg)](https://arxiv.org/abs/2306.16250) |
| 2177 | Causal Signal-based DCCRN with Overlapped-Frame Prediction for Online Speech Enhancement | :heavy_minus_sign: | :heavy_minus_sign: |
| 1511 | Gesper: A Restoration-Enhancement Framework for General Speech Reconstruction | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2306.08454-b31b1b.svg)](https://arxiv.org/abs/2306.08454) |

### New Computational Strategies for ASR Training and Inference

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

| :id: | **Title** | **Repo** | **Paper** |
|------|-----------|:--------:|:---------:|
| 2183 | A Metric-Driven Approach to Conformer Layer Pruning for Efficient ASR Inference | :heavy_minus_sign: | :heavy_minus_sign: |
| 1981 | Distillation Strategies for Discriminative Speech Recognition Rescoring | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2306.09452-b31b1b.svg)](https://arxiv.org/abs/2306.09452) |
| 969 | Another Point of View on Visual Speech Recognition | :heavy_minus_sign: | :heavy_minus_sign: |
| 1062 | RASR2: The RWTH ASR Toolkit for Generic Sequence-to-Sequence Speech Recognition | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2305.17782-b31b1b.svg)](https://arxiv.org/abs/2305.17782) |
| 486 |  Streaming Speech-to-Confusion Network Speech Recognition | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2306.03778-b31b1b.svg)](https://arxiv.org/abs/2306.03778) |
| 809 | Accurate and Structured Pruning for Efficient Automatic Speech Recognition | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2305.19549-b31b1b.svg)](https://arxiv.org/abs/2305.19549) |

### MERLIon CCS Challenge: Multilingual Everyday Recordings - Language Identification On Code-Switched Child-Directed Speech

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

| :id: | **Title** | **Repo** | **Paper** |
|------|-----------|:--------:|:---------:|
| 1446 | MERLIon CCS Challenge: A English-Mandarin Code-Switching Child-directed Speech Corpus for Language Identification and Diarization | [![GitHub](https://img.shields.io/github/stars/MERLIon-Challenge/merlion-ccs-2023)](https://github.com/MERLIon-Challenge/merlion-ccs-2023) | [![arXiv](https://img.shields.io/badge/arXiv-2305.18881-b31b1b.svg)](https://arxiv.org/abs/2305.18881) |
| 1335 | Spoken Language Identification System for English-Mandarin Code-Switching Child-Directed Speech | [![GitHub](https://img.shields.io/github/stars/shashikg/LID-Code-Switching)](https://github.com/shashikg/LID-Code-Switching) | [![arXiv](https://img.shields.io/badge/arXiv-2306.00736-b31b1b.svg)](https://arxiv.org/abs/2306.00736) |
| 1707 | Investigating Model Performance in Language Identification: beyond Simple Error Statistics | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2305.18925-b31b1b.svg)](https://arxiv.org/abs/2305.18925) |
| 2533 | Improving Wav2vec2-based Spoken Language Identification by Learning Phonological Features | :heavy_minus_sign: | :heavy_minus_sign: |
| 2047 | Language Identification Networks for Multilingual Everyday Recordings | :heavy_minus_sign: |:heavy_minus_sign: |

### Health-Related Speech Analysis

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

| :id: | **Title** | **Repo** | **Paper** |
|------|-----------|:--------:|:---------:|
| 2038 | Classification of Vocal Intensity Category from Speech using the Wav2vec2 and Whisper Embeddings | :heavy_minus_sign: | :heavy_minus_sign: |
| 1668 | The Effect of Clinical Intervention on the Speech of Individuals with PTSD: Features and Recognition Performances | :heavy_minus_sign: | :heavy_minus_sign: |
| 470 | Analysis and Automatic Prediction of Exertion from Speech: Contrasting Objective and Subjective Measures Collected while Running | :heavy_minus_sign: | :heavy_minus_sign: |
| 894 | The Androids Corpus: A New Publicly Available Benchmark for Speech based Depression Detection | :heavy_minus_sign: | :heavy_minus_sign: |
| 658 | Comparing Hand-Crafted Features to Spectrograms for Autism Severity Estimation | :heavy_minus_sign: | :heavy_minus_sign: |
| 839 | Acoustic Characteristics of Depression in Older Adults' Speech: the Role of Covariates | :heavy_minus_sign: | :heavy_minus_sign: |

### Automatic Audio Classification and Audio Captioning

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

| :id: | **Title** | **Repo** | **Paper** |
|------|-----------|:--------:|:---------:|
| 943 | Dual Transformer Decoder based Features Fusion Network for Automated Audio Captioning | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2305.18753-b31b1b.svg)](https://arxiv.org/abs/2305.18753) |
| 1564 | Adapting a ConvNeXt Model to Audio Classification on AudioSet | [![GitHub](https://img.shields.io/github/stars/topel/audioset-convnext-inf)](https://github.com/topel/audioset-convnext-inf) | [![arXiv](https://img.shields.io/badge/arXiv-2306.00830-b31b1b.svg)](https://arxiv.org/abs/2306.00830) |
| 1610 | Few-Shot Class-Incremental Audio Classification using Stochastic Classifier | [![GitHub](https://img.shields.io/github/stars/vinceasvp/meta-sc)](https://github.com/vinceasvp/meta-sc) | [![arXiv](https://img.shields.io/badge/arXiv-2306.02053-b31b1b.svg)](https://arxiv.org/abs/2306.02053) |
| 1614 | Enhance Temporal Relations in Audio Captioning with Sound Event Detection | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2306.01533-b31b1b.svg)](https://arxiv.org/abs/2306.01533) |

### Speech Synthesis

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

| :id: | **Title** | **Repo** | **Paper** |
|------|-----------|:--------:|:---------:|
| 407 | Epoch-based Spectrum Estimation for Speech | :heavy_minus_sign: | :heavy_minus_sign: |
| 1996 | OverFlow: Putting Flows on Top of Neural Transducers for Better TTS | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://shivammehta25.github.io/OverFlow/) <br /> [![GitHub](https://img.shields.io/github/stars/shivammehta25/OverFlow)](https://github.com/shivammehta25/OverFlow) | [![arXiv](https://img.shields.io/badge/arXiv-2211.06892-b31b1b.svg)](https://arxiv.org/abs/2211.06892) |
| 1568 | AdapterMix: Exploring the Efficacy of Mixture of Adapters for Low-Resource TTS Adaptation | [![GitHub](https://img.shields.io/github/stars/declare-lab/adapter-mix)](https://github.com/declare-lab/adapter-mix) | [![arXiv](https://img.shields.io/badge/arXiv-2305.18028-b31b1b.svg)](https://arxiv.org/abs/2305.18028) |
| 506 | Prior-Free Guided TTS: An Improved and Efficient Diffusion-based Text-Guided Speech Synthesis | :heavy_minus_sign: | :heavy_minus_sign: |
| 367 | UnDiff: Unsupervised Voice Restoration with Unconditional Diffusion Model | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2306.00721-b31b1b.svg)](https://arxiv.org/abs/2306.00721) |
| 1301 | Pruning Self-Attention for Zero-Shot Multi-Speaker Text-to-Speech | :heavy_minus_sign: | :heavy_minus_sign: |
| 1151 | Interpretable Style Transfer for Text-to-Speech with ControlVAE and Diffusion Bridge | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://gwh22.github.io/) | [![arXiv](https://img.shields.io/badge/arXiv-2306.04301-b31b1b.svg)](https://arxiv.org/abs/2306.04301) |
| 879 | Towards Robust FastSpeech 2 by Modelling Residual Multimodality | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://sony.github.io/ai-research-code/tvcgmm/project_page/index.html) | [![arXiv](https://img.shields.io/badge/arXiv-2306.01442-b31b1b.svg)](https://arxiv.org/abs/2306.01442) |
| 1137 | Real Time Spectrogram Inversion on Mobile Phone | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://github.com/google-research/google-research/tree/master/specinvert) | [![arXiv](https://img.shields.io/badge/arXiv-2203.00756-b31b1b.svg)](https://arxiv.org/abs/2203.00756) |
| 58 | Automatic Tuning of Loss Trade-offs without Hyper-Parameter Search in End-to-End Zero-Shot Speech Synthesis | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://cnaigithub.github.io/Auto_Tuning_Zeroshot_TTS_and_VC/) <br /> [![GitHub](https://img.shields.io/github/stars/cnaigithub/Auto_Tuning_Zeroshot_TTS_and_VC)](https://github.com/cnaigithub/Auto_Tuning_Zeroshot_TTS_and_VC) | [![arXiv](https://img.shields.io/badge/arXiv-2305.16699-b31b1b.svg)](https://arxiv.org/abs/2305.16699) |
| 2056 | A Low-Resource Pipeline for Text-to-Speech from Found Data With Application to Scottish Gaelic | :heavy_minus_sign: | :heavy_minus_sign: |
| 2173 | Self-Supervised Solution to the Control Problem of Articulatory Synthesis | :heavy_minus_sign: | :heavy_minus_sign: |
| 1128 | Hierarchical Timbre-Cadence Speaker Encoder for Zero-Shot Speech Synthesis | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://srtts.github.io/tc-zstts/) | :heavy_minus_sign: |
| 754 | ZET-Speech: Zero-Shot adaptive Emotion-Controllable Text-to-Speech Synthesis with Diffusion and Style-based Models | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://zet-speech.github.io/ZET-Speech-Demo/) | [![arXiv](https://img.shields.io/badge/arXiv-2305.13831-b31b1b.svg)](https://arxiv.org/abs/2305.13831) |
| 690 | Improving WaveRNN with Heuristic Dynamic Blending for Fast and High-Quality GPU Vocoding | :heavy_minus_sign: | :heavy_minus_sign: |
| 194 | Intelligible Lip-to-Speech Synthesis with Speech Units | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2305.19603-b31b1b.svg)](https://arxiv.org/abs/2305.19603) |
| 1212 | Parameter-Efficient Learning for Text-to-Speech Accent Adaptation | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://tts-research.github.io) <br /> [![GitHub](https://img.shields.io/github/stars/TTS-Research/PEL-TTS)](https://github.com/TTS-Research/PEL-TTS) | [![arXiv](https://img.shields.io/badge/arXiv-2305.11320-b31b1b.svg)](https://arxiv.org/abs/2305.11320) |
| 820 | Controlling Formant Frequencies with Neural Text-to-Speech for the Manipulation of Perceived Speaker Age | :heavy_minus_sign: | :heavy_minus_sign: |
| 2379 | FastFit: Towards Real-Time Iterative Neural Vocoder by Replacing U-Net Encoder with Multiple STFTs | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://kallavinka8045.github.io/is2023/) | [![arXiv](https://img.shields.io/badge/arXiv-2305.10823-b31b1b.svg)](https://arxiv.org/abs/2305.10823) |
| 1726 | iSTFTNet2: Faster and more Lightweight iSTFT-based Neural Vocoder using 1D-2D CNN | :heavy_minus_sign: | :heavy_minus_sign: |
| 534 | VITS2: Improving Quality and Efficiency of Single Stage Text to Speech with Adversarial Learning and Architecture Design | :heavy_minus_sign: | :heavy_minus_sign: |
| 1175 | Controlling Multi-Class Human Vocalization Generation via a Simple Segment-based Labeling Scheme | :heavy_minus_sign: | :heavy_minus_sign: |

### Speech Synthesis: Controllability and Adaptation

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

| :id: | **Title** | **Repo** | **Paper** |
|------|-----------|:--------:|:---------:|
| 1608 | HierVST: Hierarchical Adaptive Zero-Shot Voice Style Transfer | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://hiervst.github.io) | :heavy_minus_sign: |
| 391 | VISinger2: High-Fidelity End-to-End Singing Voice Synthesis Enhanced by Digital Signal Processing Synthesizer | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://zhangyongmao.github.io/VISinger2/) <br /> [![GitHub](https://img.shields.io/github/stars/zhangyongmao/VISinger2)](https://github.com/zhangyongmao/VISinger2) | [![arXiv](https://img.shields.io/badge/arXiv-2211.02903-b31b1b.svg)](https://arxiv.org/abs/2211.02903) |
| 700 | EdenTTS: A Simple and Efficient Parallel Text-to-Speech Architecture with Collaborative Duration-Alignment Learning | :heavy_minus_sign: | :heavy_minus_sign: |
| 368 | Generalizable Zero-Shot Speaker Adaptive Speech Synthesis with Disentangled Representations | :heavy_minus_sign: | :heavy_minus_sign: |
| 1020 | Speech Inpainting: Context-based Speech Synthesis Guided by Video | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://ipcv.github.io/avsi/) | [![arXiv](https://img.shields.io/badge/arXiv-2306.00489-b31b1b.svg)](https://arxiv.org/abs/2306.00489) |
| 2243 | STEN-TTS: Improving Zero-Shot Cross-Lingual Transfer for Multi-Lingual TTS with Style-Enhanced Normalization Diffusion Framework | :heavy_minus_sign: | :heavy_minus_sign: |

### Search Methods and Decoding Algorithms for ASR

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

| :id: | **Title** | **Repo** | **Paper** |
|------|-----------|:--------:|:---------:|
| 933 | Average Token Delay: A Latency Metric for Simultaneous Translation | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2211.13173-b31b1b.svg)](https://arxiv.org/abs/2211.13173) |
| 1450 | Automatic Speech Recognition Transformer with Global Contextual Information Decoder | :heavy_minus_sign: | :heavy_minus_sign: |
| 1333 | Time-Synchronous One-Pass Beam Search for Parallel Online and Offline Transducers with Dynamic Block Training | :heavy_minus_sign: | :heavy_minus_sign: |
| 2065 | Prefix Search Decoding for RNN Transducers | :heavy_minus_sign: | :heavy_minus_sign: |
| 78 | WhisperX: Time-Accurate Speech Transcription of Long-Form Audio | [![GitHub](https://img.shields.io/github/stars/m-bain/whisperX)](https://github.com/m-bain/whisperX) | [![arXiv](https://img.shields.io/badge/arXiv-2303.00747-b31b1b.svg)](https://arxiv.org/abs/2303.00747) |
| 2449 | Implementing Contextual Biasing in GPU Decoder for Online ASR | [![GitHub](https://img.shields.io/github/stars/idiap/contextual-biasing-on-gpus)](https://github.com/idiap/contextual-biasing-on-gpus) | [![arXiv](https://img.shields.io/badge/arXiv-2306.15685-b31b1b.svg)](https://arxiv.org/abs/2306.15685) |

### Speech Signal Analysis

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

| :id: | **Title** | **Repo** | **Paper** |
|------|-----------|:--------:|:---------:|
| 2487 | MF-PAM: Accurate Pitch Estimation through Periodicity Analysis and Multi-Level Feature Fusion | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2306.09640-b31b1b.svg)](https://arxiv.org/abs/2306.09640) |
| 2211 | Enhancing Speech Articulation Analysis using A Geometric Transformation of the X-ray Microbeam Dataset | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2305.10775-b31b1b.svg)](https://arxiv.org/abs/2305.10775) |
| 1729 | Matching Acoustic and Perceptual Measures of Phonation Assessment in Disordered Speech - A Case Study | :heavy_minus_sign: | :heavy_minus_sign: |
| 283 | Improved Contextualized Speech Representations for Tonal Analysis | :heavy_minus_sign: | :heavy_minus_sign: |
| 1738 | A Study on the Importance of Formant Transitions for Stop-Consonant Classification in VCV Sequence | :heavy_minus_sign: | [![idiap](https://img.shields.io/badge/idiap.ch.5064-FF6A00.svg)](https://publications.idiap.ch/index.php/publications/show/5064) |
| 2229 | FusedF0: Improving DNN-based F0 Estimation by Fusion of Summary-Correlograms and Raw Waveform Representations of Speech Signals | :heavy_minus_sign:| [![Pdf](https://img.shields.io/badge/pdf-version-003B10.svg)](http://www.seas.ucla.edu/spapl/paper/Eray_IS_2023.pdf) |

### Connecting Speech-science and Speech-technology for Children's Speech

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

| :id: | **Title** | **Repo** | **Paper** |
|------|-----------|:--------:|:---------:|
| 928 | Using Commercial ASR Solutions to Assess Reading Skills in Children: A Case Report | :heavy_minus_sign: | :heavy_minus_sign: |
| 907 | Uncertainty Estimation for Connectionist Temporal Classification based Automatic Speech Recognition | :heavy_minus_sign: | [![Pdf](https://img.shields.io/badge/pdf-version-003B10.svg)](https://www.tnt.uni-hannover.de/papers/data/1678/2023-Rumberg-Uncertainty_Estimation_for_Connectionist_Temporal_Classification_Based_Speech_Recognition.pdf) |
| 2185 | Speech Breathing Behavior During Pauses in Children | :heavy_minus_sign: | :heavy_minus_sign: |
| 926 | Exploiting Diversity of Automatic Transcripts from Distinct Speech Recognition Techniques for Children's Speech | :heavy_minus_sign: | [![Pdf](https://img.shields.io/badge/pdf-version-003B10.svg)](https://www.tnt.uni-hannover.de/papers/data/1679/gebauer_interspeech23_childspeechdiversity.pdf) |
| 1924 | Acoustic-to-Articulatory Speech Inversion Features for Mispronunciation Detection of /r/ in Child Speech Sound Disorders | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2305.16085-b31b1b.svg)](https://arxiv.org/abs/2305.16085) |
| 978 | BabySLM: Language-Acquisition-Friendly Benchmark of Self-Supervised Spoken Language Models | [![GitHub](https://img.shields.io/github/stars/MarvinLvn/BabySLM)](https://github.com/MarvinLvn/BabySLM) | [![arXiv](https://img.shields.io/badge/arXiv-2306.01506-b31b1b.svg)](https://arxiv.org/abs/2306.01506) |
| 702 | Data Augmentation for Children ASR and Child-adult Speaker Classification using Voice Conversion Methods | :heavy_minus_sign: | :heavy_minus_sign: |
| 2236 | Developmental Articulatory and Acoustic Features for Six to Ten Year Old Children | :heavy_minus_sign: | :heavy_minus_sign: |
| 2251 | Automatically Predicting Perceived Conversation Quality in a Pediatric Sample Enriched for Autism | :heavy_minus_sign:  | :heavy_minus_sign: |
| 1257 | An Equitable Framework for Automatically Assessing Children's Oral Narrative Language Abilities | :heavy_minus_sign: | :heavy_minus_sign: |
| 743 | An Analysis of Goodness of Pronunciation for Child Speech | :heavy_minus_sign: | :heavy_minus_sign: |
| 1569 | Measuring Language Development from Child-centered Recordings | :heavy_minus_sign: | :heavy_minus_sign: |
| 2057 | Speaking Clearly, Understanding Better: Predicting the L2 Narrative Comprehension of Chinese Bilingual Kindergarten Children based on Speech Intelligibility using a Machine Learning Approach | :heavy_minus_sign: | :heavy_minus_sign: |
| 312 | Classifying Rhoticity of /r/ in Speech Sound Disorder using Age-and-Sex Normalized Formants | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2305.16111-b31b1b.svg)](https://arxiv.org/abs/2305.16111) |
| 1273 | Understanding Spoken Language Development of Children with ASD using Pre-trained Speech Embeddings | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2305.14117-b31b1b.svg)](https://arxiv.org/abs/2305.14117) |
| 2099 | Measuring Phonological Precision in Children with Cleft Lip and Palate | :heavy_minus_sign: | :heavy_minus_sign: |
| 937 | A Study on using Duration and Formant Features in Automatic Detection of Speech Sound Disorder in Children | :heavy_minus_sign: | :heavy_minus_sign: |
| 1873 | Influence of Utterance and Speaker Characteristics on the Classification of Children with Cleft Lip and Palate | :heavy_minus_sign: | :heavy_minus_sign: |
| 1882 | Prospective Validation of Motor-based Intervention with Automated Mispronunciation Detection of Rhotics in Residual Speech Sound Disorders | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2305.19090-b31b1b.svg)](https://arxiv.org/abs/2305.19090) |

### Dialog Management

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

| :id: | **Title** | **Repo** | **Paper** |
|------|-----------|:--------:|:---------:|
| 2238 | Parameter-Efficient Low-Resource Dialogue State Tracking by Prompt Tuning | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2301.10915-b31b1b.svg)](https://arxiv.org/abs/2301.10915) |
| 2525 | An Autoregressive Conversational Dynamics Model for Dialogue Systems | :heavy_minus_sign: | :heavy_minus_sign: |
| 1983 | Style-Transfer based Speech and Audio-Visual Scene Understanding for Robot Action Sequence Acquisition from Videos | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2306.15644-b31b1b.svg)](https://arxiv.org/abs/2306.15644) |
| 1037 | Speech aware Dialog System Technology Challenge (DSTC11) | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://dstc11.dstc.community/) | [![arXiv](https://img.shields.io/badge/arXiv-2212.08704-b31b1b.svg)](https://arxiv.org/abs/2212.08704) |
| 1397 | Knowledge-Retrieval Task-Oriented Dialog Systems with Semi-Supervision | [![GitHub](https://img.shields.io/github/stars/thu-spmi/JSA-KRTOD)](https://github.com/thu-spmi/JSA-KRTOD) | [![arXiv](https://img.shields.io/badge/arXiv-2305.13199-b31b1b.svg)](https://arxiv.org/abs/2305.13199) |
| 2513 | Tracking Must Go On: Dialogue State Tracking with Verified Self-Training | :heavy_minus_sign: | :heavy_minus_sign: |

### Speech Activity Detection and Modeling

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

| :id: | **Title** | **Repo** | **Paper** |
|------|-----------|:--------:|:---------:|
| 558 | GL-SSD: Global and Local Speech Style Disentanglement by Vector Quantization for Robust Sentence Boundary Detection in Speech Stream | :heavy_minus_sign: | :heavy_minus_sign: |
| 598 | Semantic VAD: Low-Latency Voice Activity Detection for Speech Interaction | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2305.12450-b31b1b.svg)](https://arxiv.org/abs/2305.12450) |
| 2466 | Dynamic Encoder RNN for Online Voice Activity Detection in Adverse Noise Conditions | :heavy_minus_sign: | :heavy_minus_sign: |
| 996 | Point to the Hidden: Exposing Speech Audio Splicing via Signal Pointer Nets | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2307.05641-b31b1b.svg)](https://arxiv.org/abs/2307.05641) |
| 716 | Real-Time Causal Spectro-Temporal Voice Activity Detection based on Convolutional Encoding and Residual Decoding | :heavy_minus_sign: | :heavy_minus_sign: |
| 2413 | SVVAD: Personal Voice Activity Detection for Speaker Verification | :heavy_minus_sign:  | [![arXiv](https://img.shields.io/badge/arXiv-2305.19581-b31b1b.svg)](https://arxiv.org/abs/2305.19581) |

### Multilingual Models for ASR

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

| :id: | **Title** | **Repo** | **Paper** |
|------|-----------|:--------:|:---------:|
| 1613 | Learning Cross-Lingual Mappings for Data Augmentation to Improve Low-Resource Speech Recognition | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2306.08577-b31b1b.svg)](https://arxiv.org/abs/2306.08577) |
| 2122 | AfriNames: Most ASR models "butcher" African Names | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2306.00253-b31b1b.svg)](https://arxiv.org/abs/2306.00253) |
| 2528 | Towards Dialect-Inclusive Recognition in a Low-Resource Language: are Balanced Corpora the Answer? | :heavy_minus_sign: | :heavy_minus_sign: |
| 2588 | Svarah: Evaluating English ASR Systems on Indian Accents | [![GitHub](https://img.shields.io/github/stars/AI4Bharat/Svarah)](https://github.com/AI4Bharat/Svarah) | [![arXiv](https://img.shields.io/badge/arXiv-2305.15760-b31b1b.svg)](https://arxiv.org/abs/2305.15760) |
| 1044 | N-Shot Benchmarking of Whisper on Diverse Arabic Speech Recognition | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2306.02902-b31b1b.svg)](https://arxiv.org/abs/2306.02902) |
| 1014 | The MALACH Corpus: Results with End-to-End Architectures and Pretraining | :heavy_minus_sign: | :heavy_minus_sign: |

### Speech Enhancement and Bandwidth Expansion

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

| :id: | **Title** | **Repo** | **Paper** |
|------|-----------|:--------:|:---------:|
| 232 | Unsupervised Speech Enhancement with Deep Dynamical Generative Speech and Noise Models | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2306.07820-b31b1b.svg)](https://arxiv.org/abs/2306.07820) |
| 857 | Noise-Robust Bandwidth Expansion for 8K Speech Recordings | :heavy_minus_sign: |:heavy_minus_sign: |
| 113 | mdctGAN: Taming Transformer-based GAN for Speech Super-Resolution with Modified DCT Spectra | [![GitHub](https://img.shields.io/github/stars/neoncloud/mdctgan)](https://github.com/neoncloud/mdctgan) | [![arXiv](https://img.shields.io/badge/arXiv-2305.11104-b31b1b.svg)](https://arxiv.org/abs/2305.11104) |
| 625 | Zoneformer: On-Device Neural Beamformer for In-Car Multi-Zone Speech Separation, Enhancement and echo Cancellation | :heavy_minus_sign: | :heavy_minus_sign: |
| 634 | Low-Complexity Broadband Beampattern Synthesis using Array Response Control | :heavy_minus_sign: | :heavy_minus_sign: |
| 904 | A GAN Speech Inpainting Model for Audio Editing Software | :heavy_minus_sign: | :heavy_minus_sign: |

### Articulation

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

| :id: | **Title** | **Repo** | **Paper** |
|------|-----------|:--------:|:---------:|
| 2316 | Deep Speech Synthesis from MRI-based Articulatory Representations | [![GitHub](https://img.shields.io/github/stars/articulatory/articulatory)](https://github.com/articulatory/articulatory) | [![arXiv](https://img.shields.io/badge/arXiv-2307.02471-b31b1b.svg)](https://arxiv.org/abs/2307.02471) |
| 562 | Learning to Compute the Articulatory Representations of Speech with the MIRRORNET | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://yashish92.github.io/MirrorNet-for-speech/) <br /> [![GitHub](https://img.shields.io/github/stars/Yashish92/MirrorNet-for-speech)](https://github.com/Yashish92/MirrorNet-for-speech)| [![arXiv](https://img.shields.io/badge/arXiv-2210.16454-b31b1b.svg)](https://arxiv.org/abs/2210.16454) |
| 804 | Generating High-Resolution 3D Real-Time MRI of the Vocal Tract | :heavy_minus_sign: | :heavy_minus_sign: |
| 1593 | Exploring a Classification Approach using Quantised Articulatory Movements for Acoustic to Articulatory Inversion | :heavy_minus_sign: | :heavy_minus_sign: |

### Neural Processing of Speech and Language: Encoding and Decoding the Diverse Auditory Brain

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

| :id: | **Title** | **Repo** | **Paper** |
|------|-----------|:--------:|:---------:|
| 633 | Coherence Estimation Tracks Auditory Attention in Listeners with Hearing Impairment | :heavy_minus_sign: | :heavy_minus_sign: |
| 2378 | Enhancing the EEG Speech Match Mismatch Tasks with Word Boundaries | [![GitHub](https://img.shields.io/github/stars/iiscleap/EEGspeech-MatchMismatch)](https://github.com/iiscleap/EEGspeech-MatchMismatch) | [![arXiv](https://img.shields.io/badge/arXiv-2307.00366-b31b1b.svg)](https://arxiv.org/abs/2307.00366) |
| 1347 | Similar Hierarchical Representation of Speech and Other Complex Sounds in the Brain and Deep Residual Networks: an MEG Study | :heavy_minus_sign: | :heavy_minus_sign: |
| 121 | Speech Taskonomy: Which Speech Tasks are the most Predictive of fMRI Brain Activity? | :heavy_minus_sign: | [![HAL Science](https://img.shields.io/badge/hal-science-040060.svg)](https://hal.science/hal-04131475) |
| 282 | MEG Encoding using Word Context Semantics in Listening Stories | :heavy_minus_sign: | [![HAL Science](https://img.shields.io/badge/hal-science-040060.svg)](https://hal.science/hal-04148324) |
| 1949 | Investigating the Cortical Tracking of Speech and Music with Sung Speech | :heavy_minus_sign: | :heavy_minus_sign: |
| 414 | Exploring Auditory Attention Decoding using Speaker Features | :heavy_minus_sign: | :heavy_minus_sign: |
| 1776 | Effects of Spectral Degradation on the Cortical Tracking of the Speech Envelope | :heavy_minus_sign: | :heavy_minus_sign: |
| 964 | Effects of Spectral and Temporal Modulation Degradation on Intelligibility and Cortical Tracking of Speech Signals | :heavy_minus_sign: | :heavy_minus_sign: |

### Perception of Paralinguistics

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

| :id: | **Title** | **Repo** | **Paper** |
|------|-----------|:--------:|:---------:|
| 2061 | Transfer Learning for Personality Perception via Speech Emotion Recognition | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2305.16076-b31b1b.svg)](https://arxiv.org/abs/2305.16076) |
| 1131 | A Stimulus-Organism-Response Model of Willingness to Buy from Advertising Speech using Voice Quality | :heavy_minus_sign: | :heavy_minus_sign: |
| 1835 | Voice Passing: A Non-Binary Voice Gender Prediction System for evaluating Transgender | :heavy_minus_sign: | :heavy_minus_sign: |
| 1139 | Influence of Personal Traits on Impressions of One's Own Voice | :heavy_minus_sign: | :heavy_minus_sign: |
| 887 | Pardon my Disfluency: The Impact of Disfluency Effects on the Perception of Speaker Competence and Confidence | :heavy_minus_sign: | :heavy_minus_sign: |
| 711 | Cross-Linguistic Emotion Perception in Human and TTS Voices | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://michelledcohn.com/2023/05/19/interspeech-2023-paper-on-cross-cultural-emotion-perception/) | :heavy_minus_sign: |

### Technologies for Child Speech Processing

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

| :id: | **Title** | **Repo** | **Paper** |
|------|-----------|:--------:|:---------:|
| 1302 | Joint Learning Feature and Model Adaptation for Unsupervised Acoustic Modelling of Child Speech | :heavy_minus_sign: | :heavy_minus_sign: |
| 1681 | Automatic Assessment of Oral Reading Accuracy for Reading Diagnostics | [![GitHub](https://img.shields.io/github/stars/bomolenaar/jasmin_data_prep)](https://github.com/bomolenaar/jasmin_data_prep) <br /> [![GitHub](https://img.shields.io/github/stars/cristiantg/kaldi_egs_CGN)](https://github.com/cristiantg/kaldi_egs_CGN/tree/onPonyLand) | [![arXiv](https://img.shields.io/badge/arXiv-2306.03444-b31b1b.svg)](https://arxiv.org/abs/2306.03444) |
| 2084 | An ASR-enabled Reading Tutor: Investigating Feedback to Optimize Interaction for Learning to Read | :heavy_minus_sign: | [![Pdf](https://img.shields.io/badge/pdf-version-003B10.svg)](https://aichildinteraction.github.io/preprint/AIAIC23_paper_7671.pdf) |
| 935 | Adaptation of Whisper Models to Child Speech Recognition | :heavy_minus_sign: | :heavy_minus_sign: |

### Speech Synthesis: Multilinguality; Evaluation

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

| :id: | **Title** | **Repo** | **Paper** |
|------|-----------|:--------:|:---------:|
| 2064 | Automatic Evaluation of Turn-Taking Cues in Conversational Speech Synthesis | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://erikekstedt.github.io/vap_tts/) | [![arXiv](https://img.shields.io/badge/arXiv-2305.17971-b31b1b.svg)](https://arxiv.org/abs/2305.17971) |
| 441 | Expressive Machine Dubbing through Phrase-Level Cross-Lingual Prosody Transfer | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2306.11662-b31b1b.svg)](https://arxiv.org/abs/2306.11662) |
| 1691 | Robust Feature Decoupling in Voice Conversion by using Locality-based Instance Normalization | [![GitHub](https://img.shields.io/github/stars/BrightGu/LoINVC)](https://github.com/BrightGu/LoINVC) | :heavy_minus_sign: |
| 612 | Zero-Shot Accent Conversion using Pseudo Siamese Disentanglement Network | :heavy_minus_sign: | :heavy_minus_sign: |
| 2148 | The Effects of Input Type and Pronunciation Dictionary Usage in Transfer Learning for Low-Resource Text-to-Speech | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://phat-do.github.io/nodict-IS23/) | [![arXiv](https://img.shields.io/badge/arXiv-2306.00535-b31b1b.svg)](https://arxiv.org/abs/2306.00535) |
| 1727 | GenerTTS: Pronunciation Disentanglement for Timbre and Style Generalization in Cross-Lingual Text-to-Speech | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://bytecong.github.io/GenerTTS/) | [![arXiv](https://img.shields.io/badge/arXiv-2306.15304-b31b1b.svg)](https://arxiv.org/abs/2306.15304) |
| 1285 | Analysis of Mean Opinion Scores in Subjective Evaluation of Synthetic Speech based on Tail Probabilities | [![GitHub](https://img.shields.io/github/stars/todalab/mos-analysis-interspeech2023)](https://github.com/todalab/mos-analysis-interspeech2023) | :heavy_minus_sign: |
| 1584 | LibriTTS-R: A Restored Multi-Speaker Text-to-Speech Corpus | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://google.github.io/df-conformer/) <br /> [![Openslr](https://img.shields.io/badge/OpenSLR-dataset-FFD1BF.svg)](http://www.openslr.org/141/) | [![arXiv](https://img.shields.io/badge/arXiv-2305.18802-b31b1b.svg)](https://arxiv.org/abs/2305.18802) |
| 1067 | UniFLG: Unified Facial Landmark Generator from Text or Speech | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://rinnakk.github.io/research/publications/UniFLG/) | [![arXiv](https://img.shields.io/badge/arXiv-2302.14337-b31b1b.svg)](https://arxiv.org/abs/2302.14337) |
| 444 | XPhoneBERT: A Pre-trained Multilingual Model for Phoneme Representations for Text-to-Speech | [![GitHub](https://img.shields.io/github/stars/VinAIResearch/XPhoneBERT)](https://github.com/VinAIResearch/XPhoneBERT) | [![arXiv](https://img.shields.io/badge/arXiv-2305.19709-b31b1b.svg)](https://arxiv.org/abs/2305.19709) |
| 2224 | ClArTTS: An Open-Source Classical Arabic Text-to-Speech Corpus | [![ClArTTS](https://img.shields.io/badge/ClArTTS-dataset-CBB2FF.svg)](https://www.clartts.com) | [![arXiv](https://img.shields.io/badge/arXiv-2303.00069-b31b1b.svg)](https://arxiv.org/abs/2303.00069) |
| 154 | Diffusion-based Accent Modelling in Speech Synthesis | :heavy_minus_sign: | :heavy_minus_sign: |
| 249 | Multilingual Text-to-Speech Synthesis for Turkic Languages using Transliteration | [![GitHub](https://img.shields.io/github/stars/IS2AI/TurkicTTS)](https://github.com/IS2AI/TurkicTTS) | [![arXiv](https://img.shields.io/badge/arXiv-2305.15749-b31b1b.svg)](https://arxiv.org/abs/2305.15749) |
| 553 | CVTE-Poly: A New Benchmark for Chinese Polyphone Disambiguation | :heavy_minus_sign: | :heavy_minus_sign: |
| 709 | Improve Bilingual TTS using Language and Phonology Embedding with Embedding Strength Modulator | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://fyyang1996.github.io/esm/) | [![arXiv](https://img.shields.io/badge/arXiv-2212.03435-b31b1b.svg)](https://arxiv.org/abs/2212.03435) |
| 2179 | High-Quality Automatic Voice Over with Accurate Alignment: Supervision through Self-Supervised Discrete Speech Units | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://ranacm.github.io/DSU-AVO/) <br /> [![GitHub](https://img.shields.io/github/stars/RanaCM/DSU-AVO)](https://github.com/RanaCM/DSU-AVO) | [![arXiv](https://img.shields.io/badge/arXiv-2306.17005-b31b1b.svg)](https://arxiv.org/abs/2306.17005) |
| 1097 | PronScribe: Highly Accurate Multimodal Phonemic Transcription From Speech and Text | :heavy_minus_sign: | :heavy_minus_sign: |
| 2158 | Resource-Efficient Fine-Tuning Strategies for Automatic MOS Prediction in Text-to-Speech for Low-Resource Languages | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2305.19396-b31b1b.svg)](https://arxiv.org/abs/2305.19396) |
| 416 | Why We Should Report the Details in Subjective Evaluation of TTS More Rigorously | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2306.02044-b31b1b.svg)](https://arxiv.org/abs/2306.02044) |
| 1622 | Speaker-Independent Neural Formant Synthesis | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://perezpoz.github.io/neuralformants) | [![arXiv](https://img.shields.io/badge/arXiv-2306.01957-b31b1b.svg)](https://arxiv.org/abs/2306.01957) |
| 1098 | CALLS: Japanese Empathetic Dialogue Speech Corpus of Complaint Handling and Attentive Listening in Customer Center | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://sython.org/Corpus/STUDIES-2/) | [![arXiv](https://img.shields.io/badge/arXiv-2305.13713-b31b1b.svg)](https://arxiv.org/abs/2305.13713) |
| 430 | SASPEECH: A Hebrew Single Speaker Dataset for Text to Speech and Voice Conversion | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://anonymous19283746.github.io/saspeech/) | :heavy_minus_sign: |

### Show and Tell: Health Applications and Emotion Recognition

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

| :id: | **Title** | **Repo** | **Paper** |
|------|-----------|:--------:|:---------:|
| 2618 | A Personalised Speech Communication Application for Dysarthric Speakers | :heavy_minus_sign: | :heavy_minus_sign: |
| 2624 | Video Multimodal Emotion Recognition System for Real World Applications | :heavy_minus_sign: | :heavy_minus_sign: |
| 2626 | Promoting Mental Self-Disclosure in a Spoken Dialogue System | :heavy_minus_sign: | :heavy_minus_sign: |
| 2632 | "Select Language, Modality or Put on a Mask!" Experiments with Multimodal Emotion Recognition | :heavy_minus_sign: | :heavy_minus_sign: |
| 2635 | My Vowels Matter: Formant Automation Tools for Diverse Child Speech | :heavy_minus_sign: | :heavy_minus_sign: |
| 2636 | NEMA: An Ecologically Valid Tool for Assessing Hearing Devices, Advanced Algorithms, and Communication in Diverse Listening Environments | :heavy_minus_sign: | :heavy_minus_sign: |
| 2644 | When Words Speak Just as Loudly as Actions: Virtual Agent based Remote Health Assessment Integrating What Patients Say with What They Do | :heavy_minus_sign: | [![Pdf](https://img.shields.io/badge/pdf-version-003B10.svg)](https://drive.google.com/file/d/1wxkBg7fqSi0yV6uLjNO4FyhT3cEKoDhF/view) |
| 2648 | Stuttering Detection Application | :heavy_minus_sign: | :heavy_minus_sign: |
| 2649 | Providing Interpretable Insights for Neurological Speech and Cognitive Disorders from Interactive Serious Games | :heavy_minus_sign: | :heavy_minus_sign: |
| 2651 | Automated Neural Nursing Assistant (ANNA): An Over-the-Phone System for Cognitive Monitoring | :heavy_minus_sign: | :heavy_minus_sign: |
| 2656 | 5G-IoT Cloud based Demonstration of Real-Time Audio-Visual Speech Enhancement for Multimodal Hearing-aids | :heavy_minus_sign: | :heavy_minus_sign: |
| 2671 | Towards Two-Point Neuron-Inspired Energy-Efficient Multimodal Open Master Hearing aid | :heavy_minus_sign: | :heavy_minus_sign: |

### Show and Tell: Speech Tools, Speech Enhancement, Speech Synthesis

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

| :id: | **Title** | **Repo** | **Paper** |
|------|-----------|:--------:|:---------:|
| 2614 | DeepFilterNet: Perceptually Motivated Real-Time Speech Enhancement | [![GitHub](https://img.shields.io/github/stars/Rikorose/DeepFilterNet)](https://github.com/Rikorose/DeepFilterNet) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/schroter23b_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.08227-b31b1b.svg)](https://arxiv.org/abs/2305.08227) |
| 2615 | Nkululeko: Machine Learning Experiments on Speaker Characteristics without Programming |  [![GitHub](https://img.shields.io/github/stars/felixbur/nkululeko)](https://github.com/felixbur/nkululeko) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/burkhardt23_interspeech.pdf) |
| 2625 | Sp1NY: A Quick and Flexible Python Speech Visualization Tool | :heavy_minus_sign: |  [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/lemaguer23_interspeech.pdf) |
| 2629 | Intonation Control for Neural Text-to-Speech Synthesis with Polynomial Models of F0 | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/corkey23_interspeech.pdf) |
| 2634 | So-to-Speak: an Exploratory Platform for Investigating the Interplay between Style and Prosody in TTS | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/szekely23b_interspeech.pdf) |
| 2638 | Comparing /b/ and /d/ with a Single Physical Model of the Human Vocal Tract to Visualize Droplets Produced while Speaking | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/arai23_interspeech.pdf) |
| 2640 | Show & Tell: Voice Activity Projection and Turn-taking | [![GitHub](https://img.shields.io/github/stars/ErikEkstedt/VoiceActivityProjection)](https://github.com/ErikEkstedt/VoiceActivityProjection) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/ekstedt23b_interspeech.pdf)|
| 2652 | Real-Time Detection of Soft Voice for Speech Enhancement | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/cordourier23_interspeech.pdf) |
| 2655 | Data Augmentation for Diverse Voice Conversion in Noisy Environments | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/tanna23_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.10684-b31b1b.svg)](https://arxiv.org/abs/2305.10684) |
| 2667 | Application for Real-Time Audio-Visual Speech Enhancement | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/gogate23_interspeech.pdf)|

### Show and Tell: Language Learning and Educational Resources

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

| :id: | **Title** | **Repo** | **Paper** |
|------|-----------|:--------:|:---------:|
| 2623 | A Unified Framework to Improve Learners' Skills of Perception and Production based on Speech Shadowing and Overlapping | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/minematsu23_interspeech.pdf) |
| 2633 | Speak & Improve: L2 English Speaking Practice Tool | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/nicholls23_interspeech.pdf) |
| 2641 | Measuring Prosody in Child Speech using SoapBox Fluency API | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/nicolao23_interspeech.pdf) |
| 2650 | Teaching Non-native Sound Contrasts using Visual Biofeedback | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/nissen23_interspeech.pdf) |
| 2654 | Large-Scale Automatic Audiobook Creation | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/walsh23_interspeech.pdf) |
| 2658 | QVoice: Arabic Speech Pronunciation Learning Application | :heavy_minus_sign: |  [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/elkheir23_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.07445-b31b1b.svg)](https://arxiv.org/abs/2305.07445) |
| 2659 | Asking Questions: an Innovative Way to Interact with Oral History Archives | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/svec23_interspeech.pdf) |
| 2660 | DisfluencyFixer: A Tool to Enhance Language Learning through Speech to Speech Disfluency Correction | [![React](https://img.shields.io/badge/react-%2320232a.svg?style=for-the-badge&logo=react&logoColor=%2361DAFB)](https://www.cfilt.iitb.ac.in/speech2text/) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/bhat23_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.16957-b31b1b.svg)](https://arxiv.org/abs/2305.16957) |
| 2661 | Technology Pipeline for Large Scale Cross-Lingual Dubbing of Lecture Videos into Multiple Indian Languages | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/prakash23_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.01338-b31b1b.svg)](https://arxiv.org/abs/2211.01338) |
| 2668 | MyVoice: Arabic Speech Resource Collaboration Platform | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/elshahawy23_interspeech.pdf)|
| 2669 | Personal Primer Prototype 1: Invitation to Make Your Own Embooked Speech-based Educational Artifact | [![GitHub](https://img.shields.io/github/stars/hromi/lesen-mikroserver)](https://github.com/hromi/lesen-mikroserver) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/hromada23_interspeech.pdf) <br /> [![ResearchGate](https://img.shields.io/badge/Research-Gate-D7E7F5.svg)](https://www.researchgate.net/publication/371491906_Personal_Primer_Prototype_1_Invitation_to_Make_Your_Own_Embooked_Speech-Based_Educational_Artifact) |

### Show and Tell: Media and Commercial Applications

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

| :id: | **Title** | **Repo** | **Paper** |
|------|-----------|:--------:|:---------:|
| 2621 | Let's Give a Voice to Conversational Agents in Virtual Reality | [![GitHub](https://img.shields.io/github/stars/sislab-unitn/Let-s-Give-a-Voice-to-Conversational-Agents-in-VR)](https://github.com/sislab-unitn/Let-s-Give-a-Voice-to-Conversational-Agents-in-VR) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/yin23b_interspeech.pdf) |
| 2622 | FOOCTTS: Generating Arabic Speech with Acoustic Environment for Football Commentator | :heavy_minus_sign: |[![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/baali23b_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.07936-b31b1b.svg)](https://arxiv.org/abs/2306.07936) |
| 2637 | Video Summarization Leveraging Multimodal Information for Presentations | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/liu23x_interspeech.pdf) |
| 2645 | What Questions are My Customers Asking?: Towards Actionable Insights from Customer Questions in Contact Center Calls | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/nathan23_interspeech.pdf) |
| 2646 | COnVoy: A Contact Center Operated Pipeline for Voice of Customer Discovery | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/tripathi23_interspeech.pdf) |
| 2653 | NeMo Forced Aligner and its Application to Word Alignment for Subtitle Generation | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/rastorgueva23_interspeech.pdf) |
| 2662 | CauSE: Causal Search Engine for Understanding Contact-Center Conversations | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/pattnaik23_interspeech.pdf)|
| 2663 | Tailored Real-Time Call Summarization System for Contact Centers | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/sachdeva23_interspeech.pdf) |
| 2647 | Federated Learning Toolkit with Voice-based User Verification Demo | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/mandke23_interspeech.pdf) |
| 2657 | Learning when to Speak: Latency and Quality Trade-offs for Simultaneous Speech-to-Speech Translation with Offline Models | [![GitHub](https://img.shields.io/github/stars/liamdugan/speech-to-speech)](https://github.com/liamdugan/speech-to-speech) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/dugan23_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.01201-b31b1b.svg)](https://arxiv.org/abs/2306.01201) |
| 2628 | Fast Enrollable Streaming Keyword Spotting System: Training and Inference using a Web Browser | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/cho23b_interspeech.pdf) |
| 2665 | Cross-Lingual/Cross-Channel Intent Detection in Contact-Center Conversations | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-speech.org/archive/pdfs/interspeech_2023/agrawal23b_interspeech.pdf) |

---

## Star History

<p align="center">
    <a href="https://star-history.com/#Dmitryryumin/Interspeech-2023-papers&Date" target="_blank">
        <img width="500" src="https://api.star-history.com/svg?repos=Dmitryryumin/Interspeech-2023-papers&type=Date" alt="Star History Chart">
    </a>
<p>
