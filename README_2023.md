<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&height=115&color=2C2A2E&text=INTERSPEECH-2023-24-Papers&section=header&reversal=false&textBg=false&fontAlign=50&fontSize=36&fontColor=FFFFFF&animation=scaleIn&fontAlignY=18" alt="INTERSPEECH-2023-24-Papers">
</p>

<table align="center">
  <tr>
    <td><strong>General Information</strong></td>
    <td>
      <a href="https://github.com/sindresorhus/awesome">
        <img src="https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg" alt="Awesome">
      </a>
      <a href="https://interspeech2023.org/">
        <img src="http://img.shields.io/badge/INTERSPEECH-2023-0C1C43.svg" alt="Conference">
      </a>
      <img src="https://img.shields.io/badge/version-v1.0.0-4FC528" alt="Version">
      <img src="https://img.shields.io/badge/License-MIT-green.svg" alt="License: MIT">
    </td>
  </tr>
  <tr>
    <td><strong>Repository Size and Activity</strong></td>
    <td>
      <img src="https://img.shields.io/github/repo-size/DmitryRyumin/INTERSPEECH-2023-24-Papers" alt="GitHub repo size">
      <img src="https://img.shields.io/github/commit-activity/t/dmitryryumin/INTERSPEECH-2023-24-Papers" alt="GitHub commit activity (branch)">
    </td>
  </tr>
  <tr>
    <td><strong>Contribution Statistics</strong></td>
    <td>
      <img src="https://img.shields.io/github/contributors/dmitryryumin/INTERSPEECH-2023-24-Papers" alt="GitHub contributors">
      <img src="https://img.shields.io/github/issues-closed/DmitryRyumin/INTERSPEECH-2023-24-Papers" alt="GitHub closed issues">
      <img src="https://img.shields.io/github/issues/DmitryRyumin/INTERSPEECH-2023-24-Papers" alt="GitHub issues">
      <img src="https://img.shields.io/github/issues-pr-closed/DmitryRyumin/INTERSPEECH-2023-24-Papers" alt="GitHub closed pull requests">
      <img src="https://img.shields.io/github/issues-pr/dmitryryumin/INTERSPEECH-2023-24-Papers" alt="GitHub pull requests">
    </td>
  </tr>
  <tr>
    <td><strong>Other Metrics</strong></td>
    <td>
      <img src="https://img.shields.io/github/last-commit/DmitryRyumin/INTERSPEECH-2023-24-Papers" alt="GitHub last commit">
      <img src="https://img.shields.io/github/watchers/dmitryryumin/INTERSPEECH-2023-24-Papers?style=flat" alt="GitHub watchers">
      <img src="https://img.shields.io/github/forks/dmitryryumin/INTERSPEECH-2023-24-Papers?style=flat" alt="GitHub forks">
      <img src="https://img.shields.io/github/stars/dmitryryumin/INTERSPEECH-2023-24-Papers?style=flat" alt="GitHub Repo stars">
      <img src="https://api.visitorbadge.io/api/combined?path=https%3A%2F%2Fgithub.com%2FDmitryRyumin%2FINTERSPEECH-2023-Papers&label=Visitors&countColor=%23263759&style=flat" alt="Visitors">
    </td>
  </tr>
  <tr>
    <td><strong>Application</strong></td>
    <td>
      <a href="https://huggingface.co/spaces/DmitryRyumin/NewEraAI-Papers" style="float:left;">
        <img src="https://img.shields.io/badge/ðŸ¤—-NewEraAI--Papers-FFD21F.svg" alt="App" />
      </a>
    </td>
  </tr>
  <tr>
    <td colspan="2" align="center"><strong>Progress Status</strong></td>
  </tr>
  <tr>
    <td><strong>Main</strong></td>
    <td>
      <div style="float:left;">
        <img src="https://geps.dev/progress/100?successColor=006600" alt="" />
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/completed_checkmark_done.svg" width="25" alt="" />
      </div>
    </td>
  </tr>
</table>

---

INTERSPEECH 2023 Papers: A complete collection of influential and exciting research papers from the [*INTERSPEECH 2023*](https://interspeech2023.org/) conference. Explore the latest advances in speech and language processing. Code included. :star: the repository to support the advancement of speech technology!

<p align="center">
    <a href="https://interspeech2023.org/" target="_blank">
        <img width="600" src="https://cdn.jsdelivr.net/gh/DmitryRyumin/INTERSPEECH-2023-24-Papers@main/images/Interspeech2023-Stacked-Colour_v2.png" alt="INTERSPEECH 2023">
    </a>
<p>

<div style="float:left;">
    <strong>Main</strong>
    <br />
    <img src="https://img.shields.io/badge/Total%20Papers-1142-42BA16" alt="Total Papers" />
    <img src="https://img.shields.io/badge/Preprint%20Papers-503%20(44.05%25)-b31b1b" alt="Preprint Papers" />
    <img src="https://img.shields.io/badge/Papers%20with%20Open%20Code-248%20(21.72%25)%20*-1D7FBF" alt="Papers with Open Code" />
</div>

> :point_right: `*` This count includes repositories on GitHub, GitLab, Hugging Face, and distributions on PyPI, while excluding Web Page or GitHub Page links.

---

> [!TIP]
[*The PDF version of the INTERSPEECH 2023 Conference Programme*](https://drive.google.com/file/d/1xnYB2tQdhSNQwa3txhxFJ3OyUnLpuOCT/view), comprises a list of all accepted full papers, their presentation order, as well as the designated presentation times.

---

<a href="https://github.com/DmitryRyumin/NewEraAI-Papers" style="float:left;">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/arrow_click_cursor_pointer.png" width="25" alt="" />
  Other collections of the best AI conferences
</a>

<br />
<br />

> [!important]
> Conference table will be up to date all the time.

<table>
    <tr>
        <td rowspan="2" align="center"><strong>Conference</strong></td>
        <td colspan="2" align="center"><strong>Year</strong></td>
    </tr>
    <tr>
        <td colspan="1" align="center"><i>2023</i></td>
        <td colspan="1" align="center"><i>2024</i></td>
    </tr>
    <tr>
        <td colspan="3" align="center"><i>Computer Vision (CV)</i></td>
    </tr>
    <tr>
        <td>CVPR</td>
        <td colspan="2" align="center"><a href="https://github.com/DmitryRyumin/CVPR-2023-24-Papers" target="_blank"><img src="https://img.shields.io/github/stars/DmitryRyumin/CVPR-2023-24-Papers?style=flat" alt="" /></a></td>
    </tr>
    <tr>
        <td>ICCV</td>
        <td align="center"><a href="https://github.com/DmitryRyumin/ICCV-2023-Papers" target="_blank"><img src="https://img.shields.io/github/stars/DmitryRyumin/ICCV-2023-Papers?style=flat" alt="" />&nbsp;<img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/done.svg" width="20" alt="" /></a></td>
        <td align="center"><img src="https://img.shields.io/badge/Not%20Scheduled-CC5540" alt=""/></td>
    </tr>
    <tr>
        <td>ECCV</td>
        <td align="center"><img src="https://img.shields.io/badge/Not%20Scheduled-CC5540" alt=""/></td>
        <td align="center"><img src="https://img.shields.io/badge/October-white?logo=github&labelColor=b31b1b" alt="" /></td>
    </tr>
    <tr>
        <td>WACV</td>
        <td align="center">:heavy_minus_sign:</td>
        <td align="center"><a href="https://github.com/DmitryRyumin/WACV-2024-Papers" target="_blank"><img src="https://img.shields.io/github/stars/DmitryRyumin/WACV-2024-Papers?style=flat" alt="" />&nbsp;<img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/done.svg" width="20" alt="" /></a></td>
    </tr>
    <tr>
        <td>FG</td>
        <td align="center">:heavy_minus_sign:</td>
        <td align="center"><a href="https://github.com/DmitryRyumin/FG-2024-Papers" target="_blank"><img src="https://img.shields.io/github/stars/DmitryRyumin/FG-2024-Papers?style=flat" alt="" /></a></td>
    </tr>
    <tr>
        <td colspan="3" align="center"><i>Speech/Signal Processing (SP/SigProc)</i></td>
    </tr>
    <tr>
        <td>ICASSP</td>
        <td colspan="2" align="center"><a href="https://github.com/DmitryRyumin/ICASSP-2023-24-Papers" target="_blank"><img src="https://img.shields.io/github/stars/DmitryRyumin/ICASSP-2023-24-Papers?style=flat" alt="" /></a></td>
    </tr>
    <tr>
        <td>INTERSPEECH</td>
        <td colspan="2" align="center"><a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers" target="_blank"><img src="https://img.shields.io/github/stars/DmitryRyumin/INTERSPEECH-2023-24-Papers?style=flat" alt="" /></a></td>
    </tr>
    <tr>
        <td>ISMIR</td>
        <td align="center"><a href="https://github.com/yamathcy/ISMIR-2023-Papers" target="_blank"><img src="https://img.shields.io/github/stars/yamathcy/ISMIR-2023-Papers?style=flat" alt="" />&nbsp;<img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/done.svg" width="20" alt="" /></a></td>
        <td align="center">:heavy_minus_sign:</td>
    </tr>
    <tr>
        <td colspan="3" align="center"><i>Natural Language Processing (NLP)</i></td>
    </tr>
    <tr>
        <td>EMNLP</td>
        <td align="center"><a href="https://github.com/DmitryRyumin/EMNLP-2023-Papers" target="_blank"><img src="https://img.shields.io/github/stars/DmitryRyumin/EMNLP-2023-Papers?style=flat" alt="" /></a></td>
        <td align="center"><img src="https://img.shields.io/badge/December-white?logo=github&labelColor=b31b1b" alt="" /></td>
    </tr>
    <tr>
        <td colspan="3" align="center"><i>Machine Learning (ML)</i></td>
    </tr>
    <tr>
        <td>AAAI</td>
        <td align="center">:heavy_minus_sign:</td>
        <td align="center"><a href="https://github.com/DmitryRyumin/AAAI-2024-Papers" target="_blank"><img src="https://img.shields.io/github/stars/DmitryRyumin/AAAI-2024-Papers?style=flat" alt="" /></a></td>
    </tr>
    <tr>
        <td>ICLR</td>
        <td align="center">:heavy_minus_sign:</td>
        <td align="center"><img src="https://img.shields.io/badge/May-white?logo=github&labelColor=b31b1b" alt="" /></td>
    </tr>
    <tr>
        <td>ICML</td>
        <td align="center">:heavy_minus_sign:</td>
        <td align="center"><img src="https://img.shields.io/badge/July-white?logo=github&labelColor=b31b1b" alt="" /></td>
    </tr>
    <tr>
        <td>NeurIPS</td>
        <td align="center">:heavy_minus_sign:</td>
        <td align="center"><img src="https://img.shields.io/badge/December-white?logo=github&labelColor=b31b1b" alt="" /></td>
    </tr>
</table>

---

## Contributors

<p align="center">
  <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/graphs/contributors">
    <img src="http://contributors.nn.ci/api?repo=DmitryRyumin/INTERSPEECH-2023-24-Papers" alt="" />
  </a>
</p>

<br />
<br />

> [!NOTE]
> Contributions to improve the completeness of this list are greatly appreciated. If you come across any overlooked papers, please **feel free to [*create pull requests*](https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/pulls), [*open issues*](https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/issues) or contact me via [*email*](mailto:neweraairesearch@gmail.com)**. Your participation is crucial to making this repository even better.

---

## [Papers](https://www.isca-speech.org/archive/interspeech_2023/) <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/ai.svg" width="30" alt="" />

<a href="https://huggingface.co/spaces/DmitryRyumin/NewEraAI-Papers" style="float:left;">
  <img src="https://img.shields.io/badge/ðŸ¤—-NewEraAI--Papers-FFD21F.svg" alt="App" />
</a>

<a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers">
  <img src="http://img.shields.io/badge/INTERSPEECH-2024-0C1C43.svg" alt="Conference">
</a>

<table>
    <thead>
        <tr>
            <th scope="col">Section</th>
            <th scope="col">Papers</th>
            <th scope="col"><img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/arxiv-logo.svg" width="45" alt="" /></th>
            <th scope="col"><img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/github_code_developer.svg" width="27" alt="" /></th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/resources-for-spoken-language-processing.md">Resources for Spoken Language Processing</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/resources-for-spoken-language-processing.md"><img src="https://img.shields.io/badge/6-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/resources-for-spoken-language-processing.md"><img src="https://img.shields.io/badge/3-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/resources-for-spoken-language-processing.md"><img src="https://img.shields.io/badge/2-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speech-synthesis-prosody-and-emotion.md">Speech Synthesis: Prosody and Emotion</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speech-synthesis-prosody-and-emotion.md"><img src="https://img.shields.io/badge/6-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speech-synthesis-prosody-and-emotion.md"><img src="https://img.shields.io/badge/4-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speech-synthesis-prosody-and-emotion.md"><img src="https://img.shields.io/badge/2-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/statistical-machine-translation.md">Statistical Machine Translation</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/statistical-machine-translation.md"><img src="https://img.shields.io/badge/6-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/statistical-machine-translation.md"><img src="https://img.shields.io/badge/4-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/statistical-machine-translation.md"><img src="https://img.shields.io/badge/1-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/self-supervised-learning-in-asr.md">Self-Supervised Learning in ASR</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/self-supervised-learning-in-asr.md"><img src="https://img.shields.io/badge/6-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/self-supervised-learning-in-asr.md"><img src="https://img.shields.io/badge/4-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/self-supervised-learning-in-asr.md"><img src="https://img.shields.io/badge/3-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/prosody.md">Prosody</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/prosody.md"><img src="https://img.shields.io/badge/6-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/prosody.md"><img src="https://img.shields.io/badge/0-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/prosody.md"><img src="https://img.shields.io/badge/0-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speech-production.md">Speech Production</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speech-production.md"><img src="https://img.shields.io/badge/4-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speech-production.md"><img src="https://img.shields.io/badge/1-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speech-production.md"><img src="https://img.shields.io/badge/1-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/dysarthric-speech-assessment.md">Dysarthric Speech Assessment</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/dysarthric-speech-assessment.md"><img src="https://img.shields.io/badge/6-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/dysarthric-speech-assessment.md"><img src="https://img.shields.io/badge/4-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/dysarthric-speech-assessment.md"><img src="https://img.shields.io/badge/1-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speech-coding-transmission.md">Speech Coding: Transmission</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speech-coding-transmission.md"><img src="https://img.shields.io/badge/4-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speech-coding-transmission.md"><img src="https://img.shields.io/badge/0-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speech-coding-transmission.md"><img src="https://img.shields.io/badge/2-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speech-recognition-signal-processing-acoustic-modeling-robustness-adaptation.md">Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speech-recognition-signal-processing-acoustic-modeling-robustness-adaptation.md"><img src="https://img.shields.io/badge/75-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speech-recognition-signal-processing-acoustic-modeling-robustness-adaptation.md"><img src="https://img.shields.io/badge/48-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speech-recognition-signal-processing-acoustic-modeling-robustness-adaptation.md"><img src="https://img.shields.io/badge/17-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/analysis-of-speech-and-audio-signals.md">Analysis of Speech and Audio Signals</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/analysis-of-speech-and-audio-signals.md"><img src="https://img.shields.io/badge/85-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/analysis-of-speech-and-audio-signals.md"><img src="https://img.shields.io/badge/32-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/analysis-of-speech-and-audio-signals.md"><img src="https://img.shields.io/badge/27-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speech-recognition-architecture-search-and-linguistic-components.md">Speech Recognition: Architecture, Search, and Linguistic Components</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speech-recognition-architecture-search-and-linguistic-components.md"><img src="https://img.shields.io/badge/50-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speech-recognition-architecture-search-and-linguistic-components.md"><img src="https://img.shields.io/badge/26-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speech-recognition-architecture-search-and-linguistic-components.md"><img src="https://img.shields.io/badge/6-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speech-recognition-technologies-and-systems-for-new-applications.md">Speech Recognition: Technologies and Systems for New Applications</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speech-recognition-technologies-and-systems-for-new-applications.md"><img src="https://img.shields.io/badge/35-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speech-recognition-technologies-and-systems-for-new-applications.md"><img src="https://img.shields.io/badge/20-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speech-recognition-technologies-and-systems-for-new-applications.md"><img src="https://img.shields.io/badge/8-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/lexical-and-language-modeling-for-asr.md">Lexical and Language Modeling for ASR</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/lexical-and-language-modeling-for-asr.md"><img src="https://img.shields.io/badge/6-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/lexical-and-language-modeling-for-asr.md"><img src="https://img.shields.io/badge/5-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/lexical-and-language-modeling-for-asr.md"><img src="https://img.shields.io/badge/1-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/language-identification-and-diarization.md">Language Identification and Diarization</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/language-identification-and-diarization.md"><img src="https://img.shields.io/badge/6-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/language-identification-and-diarization.md"><img src="https://img.shields.io/badge/1-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/language-identification-and-diarization.md"><img src="https://img.shields.io/badge/1-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speech-quality-assessment.md">Speech Quality Assessment</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speech-quality-assessment.md"><img src="https://img.shields.io/badge/6-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speech-quality-assessment.md"><img src="https://img.shields.io/badge/0-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speech-quality-assessment.md"><img src="https://img.shields.io/badge/1-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/feature-modeling-for-asr.md">Feature Modeling for ASR</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/feature-modeling-for-asr.md"><img src="https://img.shields.io/badge/6-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/feature-modeling-for-asr.md"><img src="https://img.shields.io/badge/3-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/feature-modeling-for-asr.md"><img src="https://img.shields.io/badge/1-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/interfacing-speech-technology-and-phonetics.md">Interfacing Speech Technology and Phonetics</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/interfacing-speech-technology-and-phonetics.md"><img src="https://img.shields.io/badge/4-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/interfacing-speech-technology-and-phonetics.md"><img src="https://img.shields.io/badge/0-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/interfacing-speech-technology-and-phonetics.md"><img src="https://img.shields.io/badge/0-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speech-synthesis-multilinguality.md">Speech Synthesis: Multilinguality</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speech-synthesis-multilinguality.md"><img src="https://img.shields.io/badge/6-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speech-synthesis-multilinguality.md"><img src="https://img.shields.io/badge/5-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speech-synthesis-multilinguality.md"><img src="https://img.shields.io/badge/0-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speech-emotion-recognition.md">Speech Emotion Recognition</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speech-emotion-recognition.md"><img src="https://img.shields.io/badge/29-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speech-emotion-recognition.md"><img src="https://img.shields.io/badge/6-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speech-emotion-recognition.md"><img src="https://img.shields.io/badge/5-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/spoken-dialog-systems-and-conversational-analysis.md">Spoken Dialog Systems and Conversational Analysis</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/spoken-dialog-systems-and-conversational-analysis.md"><img src="https://img.shields.io/badge/37-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/spoken-dialog-systems-and-conversational-analysis.md"><img src="https://img.shields.io/badge/8-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/spoken-dialog-systems-and-conversational-analysis.md"><img src="https://img.shields.io/badge/4-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speech-coding-and-enhancement.md">Speech Coding and Enhancement</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speech-coding-and-enhancement.md"><img src="https://img.shields.io/badge/58-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speech-coding-and-enhancement.md"><img src="https://img.shields.io/badge/33-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speech-coding-and-enhancement.md"><img src="https://img.shields.io/badge/14-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/paralinguistics.md">Paralinguistics</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/paralinguistics.md"><img src="https://img.shields.io/badge/22-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/paralinguistics.md"><img src="https://img.shields.io/badge/8-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/paralinguistics.md"><img src="https://img.shields.io/badge/1-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speech-enhancement-and-denoising.md">Speech Enhancement and Denoising</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speech-enhancement-and-denoising.md"><img src="https://img.shields.io/badge/6-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speech-enhancement-and-denoising.md"><img src="https://img.shields.io/badge/3-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speech-enhancement-and-denoising.md"><img src="https://img.shields.io/badge/0-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speech-synthesis-evaluation.md">Speech Synthesis: Evaluation</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speech-synthesis-evaluation.md"><img src="https://img.shields.io/badge/6-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speech-synthesis-evaluation.md"><img src="https://img.shields.io/badge/2-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speech-synthesis-evaluation.md"><img src="https://img.shields.io/badge/0-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/end-to-end-spoken-dialog-systems.md">End-to-End Spoken Dialog Systems</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/end-to-end-spoken-dialog-systems.md"><img src="https://img.shields.io/badge/6-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/end-to-end-spoken-dialog-systems.md"><img src="https://img.shields.io/badge/2-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/end-to-end-spoken-dialog-systems.md"><img src="https://img.shields.io/badge/1-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/biosignal-enabled-spoken-communication.md">Biosignal-enabled Spoken Communication</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/biosignal-enabled-spoken-communication.md"><img src="https://img.shields.io/badge/10-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/biosignal-enabled-spoken-communication.md"><img src="https://img.shields.io/badge/2-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/biosignal-enabled-spoken-communication.md"><img src="https://img.shields.io/badge/3-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/neural-based-speech-and-acoustic-analysis.md">Neural-based Speech and Acoustic Analysis</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/neural-based-speech-and-acoustic-analysis.md"><img src="https://img.shields.io/badge/6-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/neural-based-speech-and-acoustic-analysis.md"><img src="https://img.shields.io/badge/5-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/neural-based-speech-and-acoustic-analysis.md"><img src="https://img.shields.io/badge/3-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/digo---dialog-for-good-speech-and-language-technology-for-social-good.md">DiGo - Dialog for Good: Speech and Language Technology for Social Good</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/digo---dialog-for-good-speech-and-language-technology-for-social-good.md"><img src="https://img.shields.io/badge/5-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/digo---dialog-for-good-speech-and-language-technology-for-social-good.md"><img src="https://img.shields.io/badge/3-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/digo---dialog-for-good-speech-and-language-technology-for-social-good.md"><img src="https://img.shields.io/badge/2-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/spoken-language-processing-translation-information-retrieval-summarization-resources-and-evaluation.md">Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources, and Evaluation</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/spoken-language-processing-translation-information-retrieval-summarization-resources-and-evaluation.md"><img src="https://img.shields.io/badge/48-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/spoken-language-processing-translation-information-retrieval-summarization-resources-and-evaluation.md"><img src="https://img.shields.io/badge/21-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/spoken-language-processing-translation-information-retrieval-summarization-resources-and-evaluation.md"><img src="https://img.shields.io/badge/13-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speech-voice-and-hearing-disorders.md">Speech, Voice, and Hearing Disorders</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speech-voice-and-hearing-disorders.md"><img src="https://img.shields.io/badge/28-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speech-voice-and-hearing-disorders.md"><img src="https://img.shields.io/badge/13-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speech-voice-and-hearing-disorders.md"><img src="https://img.shields.io/badge/6-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/spoken-term-detection-and-voice-search.md">Spoken Term Detection and Voice Search</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/spoken-term-detection-and-voice-search.md"><img src="https://img.shields.io/badge/6-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/spoken-term-detection-and-voice-search.md"><img src="https://img.shields.io/badge/3-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/spoken-term-detection-and-voice-search.md"><img src="https://img.shields.io/badge/0-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/models-for-streaming-asr.md">Models for Streaming ASR</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/models-for-streaming-asr.md"><img src="https://img.shields.io/badge/6-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/models-for-streaming-asr.md"><img src="https://img.shields.io/badge/5-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/models-for-streaming-asr.md"><img src="https://img.shields.io/badge/0-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/source-separation.md">Source Separation</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/source-separation.md"><img src="https://img.shields.io/badge/6-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/source-separation.md"><img src="https://img.shields.io/badge/2-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/source-separation.md"><img src="https://img.shields.io/badge/1-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speech-perception.md">Speech Perception</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speech-perception.md"><img src="https://img.shields.io/badge/6-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speech-perception.md"><img src="https://img.shields.io/badge/0-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speech-perception.md"><img src="https://img.shields.io/badge/0-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/phonetics-and-phonology-languages-and-varieties.md">Phonetics and Phonology: Languages and Varieties</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/phonetics-and-phonology-languages-and-varieties.md"><img src="https://img.shields.io/badge/6-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/phonetics-and-phonology-languages-and-varieties.md"><img src="https://img.shields.io/badge/0-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/phonetics-and-phonology-languages-and-varieties.md"><img src="https://img.shields.io/badge/1-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speaker-and-language-identification.md">Speaker and Language Identification</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speaker-and-language-identification.md"><img src="https://img.shields.io/badge/59-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speaker-and-language-identification.md"><img src="https://img.shields.io/badge/30-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speaker-and-language-identification.md"><img src="https://img.shields.io/badge/16-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speech-synthesis-and-voice-conversion.md">Speech Synthesis and Voice Conversion</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speech-synthesis-and-voice-conversion.md"><img src="https://img.shields.io/badge/17-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speech-synthesis-and-voice-conversion.md"><img src="https://img.shields.io/badge/7-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speech-synthesis-and-voice-conversion.md"><img src="https://img.shields.io/badge/4-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speech-and-language-in-health-from-remote-monitoring-to-medical-conversations.md">Speech and Language in Health: from Remote Monitoring to Medical Conversations</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speech-and-language-in-health-from-remote-monitoring-to-medical-conversations.md"><img src="https://img.shields.io/badge/29-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speech-and-language-in-health-from-remote-monitoring-to-medical-conversations.md"><img src="https://img.shields.io/badge/13-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speech-and-language-in-health-from-remote-monitoring-to-medical-conversations.md"><img src="https://img.shields.io/badge/6-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/novel-transformer-models-for-asr.md">Novel Transformer Models for ASR</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/novel-transformer-models-for-asr.md"><img src="https://img.shields.io/badge/6-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/novel-transformer-models-for-asr.md"><img src="https://img.shields.io/badge/4-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/novel-transformer-models-for-asr.md"><img src="https://img.shields.io/badge/2-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speaker-recognition.md">Speaker Recognition</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speaker-recognition.md"><img src="https://img.shields.io/badge/10-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speaker-recognition.md"><img src="https://img.shields.io/badge/7-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speaker-recognition.md"><img src="https://img.shields.io/badge/2-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/cross-lingual-and-multilingual-asr.md">Cross-lingual and Multilingual ASR</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/cross-lingual-and-multilingual-asr.md"><img src="https://img.shields.io/badge/6-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/cross-lingual-and-multilingual-asr.md"><img src="https://img.shields.io/badge/4-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/cross-lingual-and-multilingual-asr.md"><img src="https://img.shields.io/badge/2-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/voice-conversion.md">Voice Conversion</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/voice-conversion.md"><img src="https://img.shields.io/badge/6-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/voice-conversion.md"><img src="https://img.shields.io/badge/2-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/voice-conversion.md"><img src="https://img.shields.io/badge/3-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/pathological-speech-analysis.md">Pathological Speech Analysis</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/pathological-speech-analysis.md"><img src="https://img.shields.io/badge/12-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/pathological-speech-analysis.md"><img src="https://img.shields.io/badge/1-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/pathological-speech-analysis.md"><img src="https://img.shields.io/badge/2-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/multimodal-speech-emotion-recognition.md">Multimodal Speech Emotion Recognition</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/multimodal-speech-emotion-recognition.md"><img src="https://img.shields.io/badge/6-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/multimodal-speech-emotion-recognition.md"><img src="https://img.shields.io/badge/0-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/multimodal-speech-emotion-recognition.md"><img src="https://img.shields.io/badge/0-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/phonetics-phonology-and-prosody.md">Phonetics, Phonology, and Prosody</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/phonetics-phonology-and-prosody.md"><img src="https://img.shields.io/badge/32-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/phonetics-phonology-and-prosody.md"><img src="https://img.shields.io/badge/4-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/phonetics-phonology-and-prosody.md"><img src="https://img.shields.io/badge/3-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speech-coding-privacy.md">Speech Coding: Privacy</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speech-coding-privacy.md"><img src="https://img.shields.io/badge/6-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speech-coding-privacy.md"><img src="https://img.shields.io/badge/4-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speech-coding-privacy.md"><img src="https://img.shields.io/badge/2-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/analysis-of-neural-speech-representations.md">Analysis of Neural Speech Representations</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/analysis-of-neural-speech-representations.md"><img src="https://img.shields.io/badge/6-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/analysis-of-neural-speech-representations.md"><img src="https://img.shields.io/badge/3-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/analysis-of-neural-speech-representations.md"><img src="https://img.shields.io/badge/3-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/end-to-end-asr.md">End-to-end ASR</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/end-to-end-asr.md"><img src="https://img.shields.io/badge/6-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/end-to-end-asr.md"><img src="https://img.shields.io/badge/5-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/end-to-end-asr.md"><img src="https://img.shields.io/badge/1-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/spoken-language-understanding-summarization-and-information-retrieval.md">Spoken Language Understanding, Summarization, and Information Retrieval</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/spoken-language-understanding-summarization-and-information-retrieval.md"><img src="https://img.shields.io/badge/6-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/spoken-language-understanding-summarization-and-information-retrieval.md"><img src="https://img.shields.io/badge/3-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/spoken-language-understanding-summarization-and-information-retrieval.md"><img src="https://img.shields.io/badge/4-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/invariant-and-robust-pre-trained-acoustic-models.md">Invariant and Robust Pre-trained Acoustic Models</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/invariant-and-robust-pre-trained-acoustic-models.md"><img src="https://img.shields.io/badge/6-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/invariant-and-robust-pre-trained-acoustic-models.md"><img src="https://img.shields.io/badge/5-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/invariant-and-robust-pre-trained-acoustic-models.md"><img src="https://img.shields.io/badge/3-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speech-synthesis-representation-learning.md">Speech Synthesis: Representation Learning</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speech-synthesis-representation-learning.md"><img src="https://img.shields.io/badge/6-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speech-synthesis-representation-learning.md"><img src="https://img.shields.io/badge/3-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speech-synthesis-representation-learning.md"><img src="https://img.shields.io/badge/1-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speech-perception-production-and-acquisition.md">Speech Perception, Production, and Acquisition</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speech-perception-production-and-acquisition.md"><img src="https://img.shields.io/badge/33-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speech-perception-production-and-acquisition.md"><img src="https://img.shields.io/badge/4-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speech-perception-production-and-acquisition.md"><img src="https://img.shields.io/badge/3-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/acoustic-model-adaptation-for-asr.md">Acoustic Model Adaptation for ASR</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/acoustic-model-adaptation-for-asr.md"><img src="https://img.shields.io/badge/6-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/acoustic-model-adaptation-for-asr.md"><img src="https://img.shields.io/badge/6-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/acoustic-model-adaptation-for-asr.md"><img src="https://img.shields.io/badge/3-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speech-synthesis-expressivity.md">Speech Synthesis: Expressivity</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speech-synthesis-expressivity.md"><img src="https://img.shields.io/badge/26-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speech-synthesis-expressivity.md"><img src="https://img.shields.io/badge/15-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speech-synthesis-expressivity.md"><img src="https://img.shields.io/badge/4-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/multi-modal-systems.md">Multi-modal Systems</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/multi-modal-systems.md"><img src="https://img.shields.io/badge/6-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/multi-modal-systems.md"><img src="https://img.shields.io/badge/4-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/multi-modal-systems.md"><img src="https://img.shields.io/badge/1-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/question-answering-from-speech.md">Question Answering from Speech</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/question-answering-from-speech.md"><img src="https://img.shields.io/badge/6-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/question-answering-from-speech.md"><img src="https://img.shields.io/badge/2-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/question-answering-from-speech.md"><img src="https://img.shields.io/badge/2-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/multi-talker-methods-in-speech-processing.md">Multi-talker Methods in Speech Processing</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/multi-talker-methods-in-speech-processing.md"><img src="https://img.shields.io/badge/16-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/multi-talker-methods-in-speech-processing.md"><img src="https://img.shields.io/badge/7-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/multi-talker-methods-in-speech-processing.md"><img src="https://img.shields.io/badge/1-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/sociophonetics.md">Sociophonetics</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/sociophonetics.md"><img src="https://img.shields.io/badge/4-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/sociophonetics.md"><img src="https://img.shields.io/badge/0-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/sociophonetics.md"><img src="https://img.shields.io/badge/0-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speaker-and-language-diarization.md">Speaker and Language Diarization</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speaker-and-language-diarization.md"><img src="https://img.shields.io/badge/6-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speaker-and-language-diarization.md"><img src="https://img.shields.io/badge/3-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speaker-and-language-diarization.md"><img src="https://img.shields.io/badge/0-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/anti-spoofing-for-speaker-verification.md">Anti-Spoofing for Speaker Verification</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/anti-spoofing-for-speaker-verification.md"><img src="https://img.shields.io/badge/6-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/anti-spoofing-for-speaker-verification.md"><img src="https://img.shields.io/badge/4-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/anti-spoofing-for-speaker-verification.md"><img src="https://img.shields.io/badge/3-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speech-coding-intelligibility.md">Speech Coding: Intelligibility</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speech-coding-intelligibility.md"><img src="https://img.shields.io/badge/6-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speech-coding-intelligibility.md"><img src="https://img.shields.io/badge/3-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speech-coding-intelligibility.md"><img src="https://img.shields.io/badge/0-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/new-computational-strategies-for-asr-training-and-inference.md">New Computational Strategies for ASR Training and Inference</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/new-computational-strategies-for-asr-training-and-inference.md"><img src="https://img.shields.io/badge/6-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/new-computational-strategies-for-asr-training-and-inference.md"><img src="https://img.shields.io/badge/4-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/new-computational-strategies-for-asr-training-and-inference.md"><img src="https://img.shields.io/badge/0-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/merlion-ccs-challenge-multilingual-everyday-recordings---language-identification-on-code-switched-child-directed-speech.md">MERLIon CCS Challenge: Multilingual Everyday Recordings - Language Identification On Code-Switched Child-Directed Speech</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/merlion-ccs-challenge-multilingual-everyday-recordings---language-identification-on-code-switched-child-directed-speech.md"><img src="https://img.shields.io/badge/5-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/merlion-ccs-challenge-multilingual-everyday-recordings---language-identification-on-code-switched-child-directed-speech.md"><img src="https://img.shields.io/badge/3-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/merlion-ccs-challenge-multilingual-everyday-recordings---language-identification-on-code-switched-child-directed-speech.md"><img src="https://img.shields.io/badge/2-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/health-related-speech-analysis.md">Health-Related Speech Analysis</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/health-related-speech-analysis.md"><img src="https://img.shields.io/badge/6-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/health-related-speech-analysis.md"><img src="https://img.shields.io/badge/0-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/health-related-speech-analysis.md"><img src="https://img.shields.io/badge/1-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/automatic-audio-classification-and-audio-captioning.md">Automatic Audio Classification and Audio Captioning</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/automatic-audio-classification-and-audio-captioning.md"><img src="https://img.shields.io/badge/4-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/automatic-audio-classification-and-audio-captioning.md"><img src="https://img.shields.io/badge/4-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/automatic-audio-classification-and-audio-captioning.md"><img src="https://img.shields.io/badge/2-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speech-synthesis.md">Speech Synthesis</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speech-synthesis.md"><img src="https://img.shields.io/badge/22-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speech-synthesis.md"><img src="https://img.shields.io/badge/11-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speech-synthesis.md"><img src="https://img.shields.io/badge/7-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speech-synthesis-controllability-and-adaptation.md">Speech Synthesis: Controllability and Adaptation</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speech-synthesis-controllability-and-adaptation.md"><img src="https://img.shields.io/badge/6-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speech-synthesis-controllability-and-adaptation.md"><img src="https://img.shields.io/badge/2-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speech-synthesis-controllability-and-adaptation.md"><img src="https://img.shields.io/badge/2-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/search-methods-and-decoding-algorithms-for-asr.md">Search Methods and Decoding Algorithms for ASR</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/search-methods-and-decoding-algorithms-for-asr.md"><img src="https://img.shields.io/badge/6-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/search-methods-and-decoding-algorithms-for-asr.md"><img src="https://img.shields.io/badge/3-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/search-methods-and-decoding-algorithms-for-asr.md"><img src="https://img.shields.io/badge/2-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speech-signal-analysis.md">Speech Signal Analysis</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speech-signal-analysis.md"><img src="https://img.shields.io/badge/6-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speech-signal-analysis.md"><img src="https://img.shields.io/badge/4-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speech-signal-analysis.md"><img src="https://img.shields.io/badge/1-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/connecting-speech-science-and-speech-technology-for-childrens-speech.md">Connecting Speech-science and Speech-technology for Children's Speech</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/connecting-speech-science-and-speech-technology-for-childrens-speech.md"><img src="https://img.shields.io/badge/6-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/connecting-speech-science-and-speech-technology-for-childrens-speech.md"><img src="https://img.shields.io/badge/4-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/connecting-speech-science-and-speech-technology-for-childrens-speech.md"><img src="https://img.shields.io/badge/1-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/dialog-management.md">Dialog Management</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/dialog-management.md"><img src="https://img.shields.io/badge/6-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/dialog-management.md"><img src="https://img.shields.io/badge/4-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/dialog-management.md"><img src="https://img.shields.io/badge/1-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speech-activity-detection-and-modeling.md">Speech Activity Detection and Modeling</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speech-activity-detection-and-modeling.md"><img src="https://img.shields.io/badge/6-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speech-activity-detection-and-modeling.md"><img src="https://img.shields.io/badge/3-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speech-activity-detection-and-modeling.md"><img src="https://img.shields.io/badge/0-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/multilingual-models-for-asr.md">Multilingual Models for ASR</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/multilingual-models-for-asr.md"><img src="https://img.shields.io/badge/6-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/multilingual-models-for-asr.md"><img src="https://img.shields.io/badge/4-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/multilingual-models-for-asr.md"><img src="https://img.shields.io/badge/2-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speech-enhancement-and-bandwidth-expansion.md">Speech Enhancement and Bandwidth Expansion</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speech-enhancement-and-bandwidth-expansion.md"><img src="https://img.shields.io/badge/6-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speech-enhancement-and-bandwidth-expansion.md"><img src="https://img.shields.io/badge/2-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speech-enhancement-and-bandwidth-expansion.md"><img src="https://img.shields.io/badge/2-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/articulation.md">Articulation</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/articulation.md"><img src="https://img.shields.io/badge/4-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/articulation.md"><img src="https://img.shields.io/badge/2-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/articulation.md"><img src="https://img.shields.io/badge/3-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/neural-processing-of-speech-and-language-encoding-and-decoding-the-diverse-auditory-brain.md">Neural Processing of Speech and Language: Encoding and Decoding the Diverse Auditory Brain</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/neural-processing-of-speech-and-language-encoding-and-decoding-the-diverse-auditory-brain.md"><img src="https://img.shields.io/badge/9-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/neural-processing-of-speech-and-language-encoding-and-decoding-the-diverse-auditory-brain.md"><img src="https://img.shields.io/badge/3-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/neural-processing-of-speech-and-language-encoding-and-decoding-the-diverse-auditory-brain.md"><img src="https://img.shields.io/badge/1-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/perception-of-paralinguistics.md">Perception of Paralinguistics</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/perception-of-paralinguistics.md"><img src="https://img.shields.io/badge/6-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/perception-of-paralinguistics.md"><img src="https://img.shields.io/badge/1-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/perception-of-paralinguistics.md"><img src="https://img.shields.io/badge/0-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/technologies-for-child-speech-processing.md">Technologies for Child Speech Processing</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/technologies-for-child-speech-processing.md"><img src="https://img.shields.io/badge/4-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/technologies-for-child-speech-processing.md"><img src="https://img.shields.io/badge/2-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/technologies-for-child-speech-processing.md"><img src="https://img.shields.io/badge/2-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
    </tbody>
</table>

<details open>
<summary>List of sections<a id="sections"></a></summary>

- [Speech Synthesis: Multilinguality; Evaluation](#speech-synthesis-multilinguality-evaluation)
- [Show and Tell: Health Applications and Emotion Recognition](#show-and-tell-health-applications-and-emotion-recognition)
- [Show and Tell: Speech Tools, Speech Enhancement, Speech Synthesis](#show-and-tell-speech-tools-speech-enhancement-speech-synthesis)
- [Show and Tell: Language Learning and Educational Resources](#show-and-tell-language-learning-and-educational-resources)
- [Show and Tell: Media and Commercial Applications](#show-and-tell-media-and-commercial-applications)

</details>

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" alt="" />
</a>

### Speech Synthesis: Multilinguality; Evaluation

![Section Papers](https://img.shields.io/badge/Section%20Papers-22-42BA16) ![Preprint Papers](https://img.shields.io/badge/Preprint%20Papers-15-b31b1b) ![Papers with Open Code](https://img.shields.io/badge/Papers%20with%20Open%20Code-7-1D7FBF)

| :id: | **Title** | **Repo** | **Paper** |
|------|-----------|:--------:|:---------:|
| 2064 | Automatic Evaluation of Turn-Taking Cues in Conversational Speech Synthesis | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://erikekstedt.github.io/vap_tts/) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-archive.org/interspeech_2023/ekstedt23_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.17971-b31b1b.svg)](https://arxiv.org/abs/2305.17971) |
| 441 | Expressive Machine Dubbing through Phrase-Level Cross-Lingual Prosody Transfer | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-archive.org/interspeech_2023/swiatkowski23b_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.11662-b31b1b.svg)](https://arxiv.org/abs/2306.11662) |
| 1691 | Robust Feature Decoupling in Voice Conversion by using Locality-based Instance Normalization | [![GitHub](https://img.shields.io/github/stars/BrightGu/LoINVC?style=flat)](https://github.com/BrightGu/LoINVC) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-archive.org/interspeech_2023/gu23b_interspeech.pdf) |
| 612 | Zero-Shot Accent Conversion using Pseudo Siamese Disentanglement Network | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-archive.org/interspeech_2023/jia23_interspeech.pdf) |
| 2148 | The Effects of Input Type and Pronunciation Dictionary Usage in Transfer Learning for Low-Resource Text-to-Speech | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://phat-do.github.io/nodict-IS23/) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-archive.org/interspeech_2023/do23c_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.00535-b31b1b.svg)](https://arxiv.org/abs/2306.00535) |
| 1727 | GenerTTS: Pronunciation Disentanglement for Timbre and Style Generalization in Cross-Lingual Text-to-Speech | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://bytecong.github.io/GenerTTS/) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-archive.org/interspeech_2023/cong23_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.15304-b31b1b.svg)](https://arxiv.org/abs/2306.15304) |
| 1285 | Analysis of Mean Opinion Scores in Subjective Evaluation of Synthetic Speech based on Tail Probabilities | [![GitHub](https://img.shields.io/github/stars/todalab/mos-analysis-interspeech2023?style=flat)](https://github.com/todalab/mos-analysis-interspeech2023) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-archive.org/interspeech_2023/yasuda23_interspeech.pdf) |
| 1584 | LibriTTS-R: A Restored Multi-Speaker Text-to-Speech Corpus | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://google.github.io/df-conformer/) <br /> [![Openslr](https://img.shields.io/badge/OpenSLR-dataset-FFD1BF.svg)](http://www.openslr.org/141/) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-archive.org/interspeech_2023/koizumi23_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.18802-b31b1b.svg)](https://arxiv.org/abs/2305.18802) |
| 1067 | UniFLG: Unified Facial Landmark Generator from Text or Speech | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://rinnakk.github.io/research/publications/UniFLG/) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-archive.org/interspeech_2023/mitsui23_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2302.14337-b31b1b.svg)](https://arxiv.org/abs/2302.14337) |
| 444 | XPhoneBERT: A Pre-trained Multilingual Model for Phoneme Representations for Text-to-Speech | [![GitHub](https://img.shields.io/github/stars/VinAIResearch/XPhoneBERT?style=flat)](https://github.com/VinAIResearch/XPhoneBERT) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-archive.org/interspeech_2023/thenguyen23_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.19709-b31b1b.svg)](https://arxiv.org/abs/2305.19709) |
| 2224 | ClArTTS: An Open-Source Classical Arabic Text-to-Speech Corpus | [![ClArTTS](https://img.shields.io/badge/ClArTTS-dataset-CBB2FF.svg)](https://www.clartts.com) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-archive.org/interspeech_2023/kulkarni23_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.00069-b31b1b.svg)](https://arxiv.org/abs/2303.00069) |
| 154 | Diffusion-based Accent Modelling in Speech Synthesis | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-archive.org/interspeech_2023/deja23_interspeech.pdf) |
| 249 | Multilingual Text-to-Speech Synthesis for Turkic Languages using Transliteration | [![GitHub](https://img.shields.io/github/stars/IS2AI/TurkicTTS?style=flat)](https://github.com/IS2AI/TurkicTTS) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-archive.org/interspeech_2023/yeshpanov23_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.15749-b31b1b.svg)](https://arxiv.org/abs/2305.15749) |
| 553 | CVTE-Poly: A New Benchmark for Chinese Polyphone Disambiguation | [![GitHub](https://img.shields.io/github/stars/NewZsh/polyphone?style=flat)](https://github.com/NewZsh/polyphone) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-archive.org/interspeech_2023/zhang23h_interspeech.pdf) |
| 709 | Improve Bilingual TTS using Language and Phonology Embedding with Embedding Strength Modulator | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://fyyang1996.github.io/esm/) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-archive.org/interspeech_2023/yang23k_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.03435-b31b1b.svg)](https://arxiv.org/abs/2212.03435) |
| 2179 | High-Quality Automatic Voice Over with Accurate Alignment: Supervision through Self-Supervised Discrete Speech Units | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://ranacm.github.io/DSU-AVO/) <br /> [![GitHub](https://img.shields.io/github/stars/RanaCM/DSU-AVO?style=flat)](https://github.com/RanaCM/DSU-AVO) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-archive.org/interspeech_2023/lu23f_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.17005-b31b1b.svg)](https://arxiv.org/abs/2306.17005) |
| 1097 | PronScribe: Highly Accurate Multimodal Phonemic Transcription From Speech and Text | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-archive.org/interspeech_2023/yu23_interspeech.pdf) |
| 2158 | Resource-Efficient Fine-Tuning Strategies for Automatic MOS Prediction in Text-to-Speech for Low-Resource Languages | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](phat-do.github.io/sigul22) |[![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-archive.org/interspeech_2023/do23d_interspeech.pdf) <br />[![arXiv](https://img.shields.io/badge/arXiv-2305.19396-b31b1b.svg)](https://arxiv.org/abs/2305.19396) |
| 416 | Why We Should Report the Details in Subjective Evaluation of TTS More Rigorously | [![GitHub](https://img.shields.io/github/stars/d223302/SubjectiveEvaluation?style=flat)](https://github.com/d223302/SubjectiveEvaluation) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-archive.org/interspeech_2023/chiang23_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.02044-b31b1b.svg)](https://arxiv.org/abs/2306.02044) |
| 1622 | Speaker-Independent Neural Formant Synthesis | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://perezpoz.github.io/neuralformants) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-archive.org/interspeech_2023/perezzarazaga23_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.01957-b31b1b.svg)](https://arxiv.org/abs/2306.01957) |
| 1098 | CALLS: Japanese Empathetic Dialogue Speech Corpus of Complaint Handling and Attentive Listening in Customer Center | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://sython.org/Corpus/STUDIES-2/) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-archive.org/interspeech_2023/saito23b_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.13713-b31b1b.svg)](https://arxiv.org/abs/2305.13713) |
| 430 | SASPEECH: A Hebrew Single Speaker Dataset for Text to Speech and Voice Conversion | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://anonymous19283746.github.io/saspeech/) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-archive.org/interspeech_2023/sharoni23_interspeech.pdf) |

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" alt="" />
</a>

### Show and Tell: Health Applications and Emotion Recognition

![Section Papers](https://img.shields.io/badge/Section%20Papers-12-42BA16) ![Preprint Papers](https://img.shields.io/badge/Preprint%20Papers-1-b31b1b) ![Papers with Open Code](https://img.shields.io/badge/Papers%20with%20Open%20Code-0-1D7FBF)

| :id: | **Title** | **Repo** | **Paper** |
|------|-----------|:--------:|:---------:|
| 2618 | A Personalised Speech Communication Application for Dysarthric Speakers | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-archive.org/interspeech_2023/gibson23b_interspeech.pdf) |
| 2624 | Video Multimodal Emotion Recognition System for Real World Applications | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-archive.org/interspeech_2023/lee23l_interspeech.pdf) |
| 2626 | Promoting Mental Self-Disclosure in a Spoken Dialogue System | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-archive.org/interspeech_2023/rohmatillah23_interspeech.pdf) |
| 2632 | "Select Language, Modality or Put on a Mask!" Experiments with Multimodal Emotion Recognition | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-archive.org/interspeech_2023/bujnowski23_interspeech.pdf) |
| 2635 | My Vowels Matter: Formant Automation Tools for Diverse Child Speech | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-archive.org/interspeech_2023/valentine23_interspeech.pdf) |
| 2636 | NEMA: An Ecologically Valid Tool for Assessing Hearing Devices, Advanced Algorithms, and Communication in Diverse Listening Environments | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-archive.org/interspeech_2023/chongwhite23_interspeech.pdf) |
| 2644 | When Words Speak Just as Loudly as Actions: Virtual Agent based Remote Health Assessment Integrating What Patients Say with What They Do | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-archive.org/interspeech_2023/ramanarayanan23_interspeech.pdf) <br />[![Pdf](https://img.shields.io/badge/pdf-version-003B10.svg)](https://drive.google.com/file/d/1wxkBg7fqSi0yV6uLjNO4FyhT3cEKoDhF/view) |
| 2648 | Stuttering Detection Application | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-archive.org/interspeech_2023/motepalli23_interspeech.pdf)|
| 2649 | Providing Interpretable Insights for Neurological Speech and Cognitive Disorders from Interactive Serious Games | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-archive.org/interspeech_2023/zusag23b_interspeech.pdf) |
| 2651 | Automated Neural Nursing Assistant (ANNA): An Over-the-Phone System for Cognitive Monitoring | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-archive.org/interspeech_2023/solinsky23_interspeech.pdf) |
| 2656 | 5G-IoT Cloud based Demonstration of Real-Time Audio-Visual Speech Enhancement for Multimodal Hearing-aids | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://cogmhear.org/index.html) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-archive.org/interspeech_2023/gupta23b_interspeech.pdf) |
| 2671 | Towards Two-Point Neuron-Inspired Energy-Efficient Multimodal Open Master Hearing aid | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-archive.org/interspeech_2023/raza23_interspeech.pdf) |

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" alt="" />
</a>

### Show and Tell: Speech Tools, Speech Enhancement, Speech Synthesis

![Section Papers](https://img.shields.io/badge/Section%20Papers-10-42BA16) ![Preprint Papers](https://img.shields.io/badge/Preprint%20Papers-2-b31b1b) ![Papers with Open Code](https://img.shields.io/badge/Papers%20with%20Open%20Code-4-1D7FBF)

| :id: | **Title** | **Repo** | **Paper** |
|------|-----------|:--------:|:---------:|
| 2614 | DeepFilterNet: Perceptually Motivated Real-Time Speech Enhancement | [![GitHub](https://img.shields.io/github/stars/Rikorose/DeepFilterNet?style=flat)](https://github.com/Rikorose/DeepFilterNet) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-archive.org/interspeech_2023/schroter23b_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.08227-b31b1b.svg)](https://arxiv.org/abs/2305.08227) |
| 2615 | Nkululeko: Machine Learning Experiments on Speaker Characteristics without Programming | [![GitHub](https://img.shields.io/github/stars/felixbur/nkululeko?style=flat)](https://github.com/felixbur/nkululeko) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-archive.org/interspeech_2023/burkhardt23_interspeech.pdf) |
| 2625 | Sp1NY: A Quick and Flexible Python Speech Visualization Tool | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-archive.org/interspeech_2023/lemaguer23_interspeech.pdf) |
| 2629 | Intonation Control for Neural Text-to-Speech Synthesis with Polynomial Models of F0 | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-archive.org/interspeech_2023/corkey23_interspeech.pdf) |
| 2634 | So-to-Speak: an Exploratory Platform for Investigating the Interplay between Style and Prosody in TTS | [![GitHub](https://img.shields.io/github/stars/evaszekely/So_To_Speak?style=flat)](https://github.com/evaszekely/So_To_Speak) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-archive.org/interspeech_2023/szekely23b_interspeech.pdf) |
| 2638 | Comparing /b/ and /d/ with a Single Physical Model of the Human Vocal Tract to Visualize Droplets Produced while Speaking | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-archive.org/interspeech_2023/arai23_interspeech.pdf) |
| 2640 | Show & Tell: Voice Activity Projection and Turn-taking | [![GitHub](https://img.shields.io/github/stars/ErikEkstedt/VoiceActivityProjection?style=flat)](https://github.com/ErikEkstedt/VoiceActivityProjection) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-archive.org/interspeech_2023/ekstedt23b_interspeech.pdf)|
| 2652 | Real-Time Detection of Soft Voice for Speech Enhancement | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-archive.org/interspeech_2023/cordourier23_interspeech.pdf) |
| 2655 | Data Augmentation for Diverse Voice Conversion in Noisy Environments | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-archive.org/interspeech_2023/tanna23_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.10684-b31b1b.svg)](https://arxiv.org/abs/2305.10684) |
| 2667 | Application for Real-Time Audio-Visual Speech Enhancement | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-archive.org/interspeech_2023/gogate23_interspeech.pdf)|

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" alt="" />
</a>

### Show and Tell: Language Learning and Educational Resources

![Section Papers](https://img.shields.io/badge/Section%20Papers-11-42BA16) ![Preprint Papers](https://img.shields.io/badge/Preprint%20Papers-4-b31b1b) ![Papers with Open Code](https://img.shields.io/badge/Papers%20with%20Open%20Code-1-1D7FBF)

| :id: | **Title** | **Repo** | **Paper** |
|------|-----------|:--------:|:---------:|
| 2623 | A Unified Framework to Improve Learners' Skills of Perception and Production based on Speech Shadowing and Overlapping | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-archive.org/interspeech_2023/minematsu23_interspeech.pdf) |
| 2633 | Speak & Improve: L2 English Speaking Practice Tool | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-archive.org/interspeech_2023/nicholls23_interspeech.pdf) |
| 2641 | Measuring Prosody in Child Speech using SoapBox Fluency API | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-archive.org/interspeech_2023/nicolao23_interspeech.pdf) |
| 2650 | Teaching Non-native Sound Contrasts using Visual Biofeedback | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-archive.org/interspeech_2023/nissen23_interspeech.pdf) |
| 2654 | Large-Scale Automatic Audiobook Creation | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-archive.org/interspeech_2023/walsh23_interspeech.pdf) |
| 2658 | QVoice: Arabic Speech Pronunciation Learning Application | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-archive.org/interspeech_2023/elkheir23_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.07445-b31b1b.svg)](https://arxiv.org/abs/2305.07445) |
| 2659 | Asking Questions: an Innovative Way to Interact with Oral History Archives | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-archive.org/interspeech_2023/svec23_interspeech.pdf) |
| 2660 | DisfluencyFixer: A Tool to Enhance Language Learning through Speech to Speech Disfluency Correction | [![React](https://img.shields.io/badge/react-%2320232a.svg?style=for-the-badge&logo=react&logoColor=%2361DAFB)](https://www.cfilt.iitb.ac.in/speech2text/) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-archive.org/interspeech_2023/bhat23_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.16957-b31b1b.svg)](https://arxiv.org/abs/2305.16957) |
| 2661 | Technology Pipeline for Large Scale Cross-Lingual Dubbing of Lecture Videos into Multiple Indian Languages | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-archive.org/interspeech_2023/prakash23_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.01338-b31b1b.svg)](https://arxiv.org/abs/2211.01338) |
| 2668 | MyVoice: Arabic Speech Resource Collaboration Platform | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-archive.org/interspeech_2023/elshahawy23_interspeech.pdf)|
| 2669 | Personal Primer Prototype 1: Invitation to Make Your Own Embooked Speech-based Educational Artifact | [![GitHub](https://img.shields.io/github/stars/hromi/lesen-mikroserver?style=flat)](https://github.com/hromi/lesen-mikroserver) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-archive.org/interspeech_2023/hromada23_interspeech.pdf) <br /> [![ResearchGate](https://img.shields.io/badge/Research-Gate-D7E7F5.svg)](https://www.researchgate.net/publication/371491906_Personal_Primer_Prototype_1_Invitation_to_Make_Your_Own_Embooked_Speech-Based_Educational_Artifact) |

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" alt="" />
</a>

### Show and Tell: Media and Commercial Applications

![Section Papers](https://img.shields.io/badge/Section%20Papers-12-42BA16) ![Preprint Papers](https://img.shields.io/badge/Preprint%20Papers-2-b31b1b) ![Papers with Open Code](https://img.shields.io/badge/Papers%20with%20Open%20Code-2-1D7FBF)

| :id: | **Title** | **Repo** | **Paper** |
|------|-----------|:--------:|:---------:|
| 2621 | Let's Give a Voice to Conversational Agents in Virtual Reality | [![GitHub](https://img.shields.io/github/stars/sislab-unitn/Let-s-Give-a-Voice-to-Conversational-Agents-in-VR?style=flat)](https://github.com/sislab-unitn/Let-s-Give-a-Voice-to-Conversational-Agents-in-VR) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-archive.org/interspeech_2023/yin23b_interspeech.pdf) |
| 2622 | FOOCTTS: Generating Arabic Speech with Acoustic Environment for Football Commentator | :heavy_minus_sign: |[![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-archive.org/interspeech_2023/baali23b_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.07936-b31b1b.svg)](https://arxiv.org/abs/2306.07936) |
| 2637 | Video Summarization Leveraging Multimodal Information for Presentations | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-archive.org/interspeech_2023/liu23x_interspeech.pdf) |
| 2645 | What Questions are My Customers Asking?: Towards Actionable Insights from Customer Questions in Contact Center Calls | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-archive.org/interspeech_2023/nathan23_interspeech.pdf) |
| 2646 | COnVoy: A Contact Center Operated Pipeline for Voice of Customer Discovery | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-archive.org/interspeech_2023/tripathi23_interspeech.pdf) |
| 2653 | NeMo Forced Aligner and its Application to Word Alignment for Subtitle Generation | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-archive.org/interspeech_2023/rastorgueva23_interspeech.pdf) |
| 2662 | CauSE: Causal Search Engine for Understanding Contact-Center Conversations | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-archive.org/interspeech_2023/pattnaik23_interspeech.pdf)|
| 2663 | Tailored Real-Time Call Summarization System for Contact Centers | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-archive.org/interspeech_2023/sachdeva23_interspeech.pdf) |
| 2647 | Federated Learning Toolkit with Voice-based User Verification Demo | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-archive.org/interspeech_2023/mandke23_interspeech.pdf) |
| 2657 | Learning when to Speak: Latency and Quality Trade-offs for Simultaneous Speech-to-Speech Translation with Offline Models | [![GitHub](https://img.shields.io/github/stars/liamdugan/speech-to-speech?style=flat)](https://github.com/liamdugan/speech-to-speech) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-archive.org/interspeech_2023/dugan23_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.01201-b31b1b.svg)](https://arxiv.org/abs/2306.01201) |
| 2628 | Fast Enrollable Streaming Keyword Spotting System: Training and Inference using a Web Browser | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-archive.org/interspeech_2023/cho23b_interspeech.pdf) |
| 2665 | Cross-Lingual/Cross-Channel Intent Detection in Contact-Center Conversations | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-archive.org/interspeech_2023/agrawal23b_interspeech.pdf) |

---

## Key Terms

<p align="center">
    <img width="500" src="https://cdn.jsdelivr.net/gh/DmitryRyumin/INTERSPEECH-2023-24-Papers@main/images/Keywords.png" alt="Key Terms">
<p>

---

## Star History

<p align="center">
    <a href="https://star-history.com/#Dmitryryumin/Interspeech-2023-papers&Date" target="_blank">
        <img width="500" src="https://api.star-history.com/svg?repos=Dmitryryumin/Interspeech-2023-papers&type=Date" alt="Star History Chart">
    </a>
<p>
