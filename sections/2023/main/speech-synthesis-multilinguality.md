# INTERSPEECH-2023-Papers

<table>
    <tr>
        <td><strong>Application</strong></td>
        <td>
            <a href="https://huggingface.co/spaces/DmitryRyumin/NewEraAI-Papers" style="float:left;">
                <img src="https://img.shields.io/badge/ðŸ¤—-NewEraAI--Papers-FFD21F.svg" alt="App" />
            </a>
        </td>
    </tr>
    <tr>
        <td><strong>New collections</strong></td>
        <td>
            <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/README.md">
                <img src="http://img.shields.io/badge/INTERSPEECH-2024-0C1C43.svg" alt="Conference">
            </a>
        </td>
    </tr>
</table>

<div align="center">
    <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/interfacing-speech-technology-and-phonetics.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/left.svg" width="40" alt="" />
    </a>
    <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/README_2023.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/home.svg" width="40" alt="" />
    </a>
    <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2023/main/speech-emotion-recognition.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/right.svg" width="40" alt="" />
    </a>
</div>

## Speech Synthesis: Multilinguality

![Section Papers](https://img.shields.io/badge/Section%20Papers-6-42BA16) ![Preprint Papers](https://img.shields.io/badge/Preprint%20Papers-5-b31b1b) ![Papers with Open Code](https://img.shields.io/badge/Papers%20with%20Open%20Code-0-1D7FBF)

| :id: | **Title** | **Repo** | **Paper** |
|------|-----------|:--------:|:---------:|
| 2303 | FACTSpeech: Speaking a Foreign Language Pronunciation using Only Your Native Characters | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://atozto9.github.io/demo/FACTSpeech/) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-archive.org/interspeech_2023/yang23x_interspeech.pdf) |
| 934 | Cross-Lingual Transfer Learning for Phrase Break Prediction with Multilingual Language Model | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-archive.org/interspeech_2023/lee23e_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.02579-b31b1b.svg)](https://arxiv.org/abs/2306.02579) |
| 363 | DSE-TTS: Dual Speaker Embedding for Cross-Lingual Text-to-Speech | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://goarsenal.github.io/DSE-TTS/) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-archive.org/interspeech_2023/liu23d_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.14145-b31b1b.svg)](https://arxiv.org/abs/2306.14145) |
| 1467 | Generating Multilingual Gender-Ambiguous Text-to-Speech Voices | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://innoetics.github.io/publications/gender-ambiguous/index.html) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-archive.org/interspeech_2023/markopoulos23_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.00375-b31b1b.svg)](https://arxiv.org/abs/2211.00375) |
| 2330 | RADMMM: Multilingual Multiaccented Multispeaker Text-to-Speech | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-archive.org/interspeech_2023/badlani23_interspeech.pdf) <br /> [![NVidia AI](https://img.shields.io/badge/NVidia-AI-78B900.svg)](https://research.nvidia.com/labs/adlr/projects/radmmm/) |
| 861 | Multilingual Context-based Pronunciation Learning for Text-to-Speech | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-archive.org/interspeech_2023/comini23_interspeech.pdf) <br /> [![Amazon Science](https://img.shields.io/badge/amazon-science-FE9901.svg)](https://www.amazon.science/publications/multilingual-context-based-pronunciation-learning-for-text-to-speech) |
