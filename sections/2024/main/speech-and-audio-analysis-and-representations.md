# INTERSPEECH-2024-Papers

<table>
    <tr>
        <td><strong>Application</strong></td>
        <td>
            <a href="https://huggingface.co/spaces/DmitryRyumin/NewEraAI-Papers" style="float:left;">
                <img src="https://img.shields.io/badge/ðŸ¤—-NewEraAI--Papers-FFD21F.svg" alt="App" />
            </a>
        </td>
    </tr>
    <tr>
        <td><strong>Previous Collections</strong></td>
        <td>
            <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/README_2023.md">
                <img src="http://img.shields.io/badge/INTERSPEECH-2023-0C1C43.svg" alt="Conference">
            </a>
        </td>
    </tr>
</table>

<div align="center">
    <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2024/main/speaker-diarization.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/left.svg" width="40" alt="" />
    </a>
    <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/README.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/home.svg" width="40" alt="" />
    </a>
    <a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-24-Papers/blob/main/sections/2024/main/acoustic-event-detection-segmentation-and-classification.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/right.svg" width="40" alt="" />
    </a>
</div>

## Speech and Audio Analysis and Representations

![Section Papers](https://img.shields.io/badge/Section%20Papers-7-42BA16) ![Preprint Papers](https://img.shields.io/badge/Preprint%20Papers-0-b31b1b) ![Papers with Open Code](https://img.shields.io/badge/Papers%20with%20Open%20Code-0-1D7FBF) ![Papers with Video](https://img.shields.io/badge/Papers%20with%20Video-0-FF0000)

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| [MINT: Boosting Audio-Language Model via Multi-Target Pre-Training and Instruction Tuning](https://www.isca-archive.org/interspeech_2024/zhao24h_interspeech.html) | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-archive.org/interspeech_2024/zhao24h_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2402.07485-b31b1b.svg)](https://arxiv.org/abs/2402.07485) | :heavy_minus_sign: |
| [M2D-CLAP: Masked Modeling Duo Meets CLAP for Learning General-Purpose Audio-Language Representation](https://www.isca-archive.org/interspeech_2024/niizumi24_interspeech.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://github.com/nttcslab/m2d/tree/master/clap) <br /> [![GitHub](https://img.shields.io/github/stars/nttcslab/m2d?style=flat)](https://github.com/nttcslab/m2d) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-archive.org/interspeech_2024/niizumi24_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2406.02032-b31b1b.svg)](https://arxiv.org/abs/2406.02032) | :heavy_minus_sign: |
| [Audio Fingerprinting with Holographic Reduced Representations](https://www.isca-archive.org/interspeech_2024/fujita24_interspeech.html) | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-archive.org/interspeech_2024/fujita24_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2406.13139-b31b1b.svg)](https://arxiv.org/abs/2406.13139) | :heavy_minus_sign: |
| [RAST: A Reference-Audio Synchronization Tool for Dubbed Content](https://www.isca-archive.org/interspeech_2024/meyer24b_interspeech.html) | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-archive.org/interspeech_2024/meyer24b_interspeech.pdf) | :heavy_minus_sign: |
| [YOLOPitch: A Time-Frequency Dual-Branch YOLO Model for Pitch Estimation](https://www.isca-archive.org/interspeech_2024/li24ja_interspeech.html) | [![GitHub](https://img.shields.io/github/stars/xjuspeech/YOLOPitch?style=flat)](https://github.com/xjuspeech/YOLOPitch) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-archive.org/interspeech_2024/li24ja_interspeech.pdf) | :heavy_minus_sign: |
| [Reduce, Reuse, Recycle: Is Perturbed Data Better than Other Language Augmentation for Low Resource Self-Supervised Speech Models](https://www.isca-archive.org/interspeech_2024/ullah24_interspeech.html) | :heavy_minus_sign: | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-archive.org/interspeech_2024/ullah24_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.12763-b31b1b.svg)](https://arxiv.org/abs/2309.12763) | :heavy_minus_sign: |
| [AlignNet: Learning Dataset Score Alignment Functions to Enable Better Training of Speech Quality Estimators](https://www.isca-archive.org/interspeech_2024/pieper24_interspeech.html) | [![GitHub](https://img.shields.io/github/stars/NTIA/alignnet?style=flat)](https://github.com/NTIA/alignnet) | [![ISCA](https://img.shields.io/badge/isca-version-355778.svg)](https://www.isca-archive.org/interspeech_2024/pieper24_interspeech.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2406.10205-b31b1b.svg)](https://arxiv.org/abs/2406.10205) | :heavy_minus_sign: |
